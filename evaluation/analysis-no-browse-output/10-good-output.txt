=== PHASE 1: ANALYSIS ===

Calling tool: createTodo
Todo List Created
Todo List (10 items):
- [ ] In `blib2to3/pgen2/driver.py`, verify that the whitespace handling now uses `elif char in ' \t': current_column += 1` so that both spaces and tabs increment `current_column` by 1. (ID: todo-1)
- [ ] Confirm that the previous branch `elif char == '\t': current_column += 4` has been fully removed from `blib2to3/pgen2/driver.py` and is not present elsewhere in the same function or class. (ID: todo-2)
- [ ] Search the repository (especially under `blib2to3/`) for any remaining logic that increments indent/column by 4 when encountering a tab (e.g., `current_column += 4` tied to `\t`) to ensure no conflicting tab-width assumptions remain for prefix/indent calculation. (ID: todo-3)
- [ ] Verify that the newline handling `elif char == '\n': current_column = 0` remains intact and appears after the whitespace branch, preserving the prior control flow order and semantics. (ID: todo-4)
- [ ] Check that the surrounding logic resetting state (e.g., `current_line = ''`, `current_column = 0`, `wait_for_nl = False`) remains unchanged aside from the tab/space handling and still precedes the whitespace branch. (ID: todo-5)
- [ ] Confirm that only space and tab characters advance `current_column` in this loop (i.e., no other whitespace characters like `\r` or `\f` are being incremented), aligning with the PR intent to match lib2to3's prefix consumption. (ID: todo-6)
- [ ] Locate the test `test_comment_indentation` and verify that its input uses tabs (not spaces) for indentation and includes a dedent followed by a comment line and then a code line at the same indentation level. (ID: todo-7)
- [ ] Verify that `test_comment_indentation` asserts that the comment's indentation matches the indentation of the subsequent code line, reflecting the PR's acceptance criteria. (ID: todo-8)
- [ ] Ensure `test_comment_indentation` executes the code path that computes comment/indent prefixes via `blib2to3/pgen2/driver.py` (e.g., by running the formatter/parser that relies on this driver) rather than bypassing it. (ID: todo-9)
- [ ] Check that no tests or code still assume a visual tab width of 4 in the context of prefix/indent calculation, to avoid inconsistency with the updated logic. (ID: todo-10)


=== PHASE 1 COMPLETE ===

=== PHASE 2: VALIDATION ===

Validating todo item 1 of 10
description: 'In `blib2to3/pgen2/driver.py`, verify that the whitespace handling now uses `elif char in ' \t': current_column += 1` so that both spaces and tabs increment `current_column` by 1.'
Calling tool: globTool
**/blib2to3/pgen2/driver.py
Matched files:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py

Calling tool: grepTool
elif char in ' \t':
No files found. Call grepTool again with a more generic pattern.

Calling tool: grepTool
current_column \+=
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:135:                current_column += 1

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char in ' \t':
135:                current_column += 1
136:            elif char == '\n':
137:                # unexpected empty line
138:                current_column = 0
139:            else:
140:                # indent is finished
141:                wait_for_nl = True
142:        return ''.join(lines), current_line
143:
144:
145:def _generate_pickle_name(gt, cache_dir=None):
146:    head, tail = os.path.splitext(gt)
147:    if tail == ".txt":
148:        tail = ""
149:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
150:    if cache_dir:
151:        return os.path.join(cache_dir, os.path.basename(name))
152:    else:
153:        return name
154:
155:
156:def load_grammar(gt="Grammar.txt", gp=None,
157:                 save=True, force=False, logger=None):
158:    """Load the grammar (maybe from a pickle)."""
159:    if logger is None:
160:        logger = logging.getLogger()
161:    gp = _generate_pickle_name(gt) if gp is None else gp
162:    if force or not _newer(gp, gt):
163:        logger.info("Generating grammar tables from %s", gt)
164:        g = pgen.generate_grammar(gt)
165:        if save:
166:            logger.info("Writing grammar tables to %s", gp)
167:            try:
168:                g.dump(gp)
169:            except OSError as e:

Calling tool: grepTool
\\t
Matched content:
134:            elif char in ' \t':

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: In blib2to3/pgen2/driver.py within _partially_consume_prefix, the whitespace handling now uses `elif char in ' \t': current_column += 1` (lines 134-135), ensuring both spaces and tabs increment current_column by 1. No other tab handling exists in the file.


Todo item 1 of 10 is correct
description: 'In `blib2to3/pgen2/driver.py`, verify that the whitespace handling now uses `elif char in ' \t': current_column += 1` so that both spaces and tabs increment `current_column` by 1.'
Validating todo item 2 of 10
description: 'Confirm that the previous branch `elif char == '\t': current_column += 4` has been fully removed from `blib2to3/pgen2/driver.py` and is not present elsewhere in the same function or class.'
Calling tool: grepTool
driver\.py
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/bugsinpy_patchfile.info:1:blib2to3/pgen2/driver.py;
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/parse.py:45:    See driver.py for how to get input tokens by tokenizing a file or
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black.py:2008:    in `pgen2/driver.py:Driver.parse_tokens()`.  This was a brilliant implementation

Calling tool: globTool
**/blib2to3/pgen2/driver.py
Matched files:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py

Calling tool: grepTool
char == '\\t'
No files found. Call grepTool again with a more generic pattern.

Calling tool: grepTool
current_column \+= 4
No files found. Call grepTool again with a more generic pattern.

Calling tool: grepTool
elif char in ' \\t':
Matched content:
134:            elif char in ' \t':

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char in ' \t':
135:                current_column += 1
136:            elif char == '\n':
137:                # unexpected empty line
138:                current_column = 0
139:            else:
140:                # indent is finished
141:                wait_for_nl = True
142:        return ''.join(lines), current_line
143:
144:
145:def _generate_pickle_name(gt, cache_dir=None):
146:    head, tail = os.path.splitext(gt)
147:    if tail == ".txt":
148:        tail = ""
149:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
150:    if cache_dir:
151:        return os.path.join(cache_dir, os.path.basename(name))
152:    else:
153:        return name
154:
155:
156:def load_grammar(gt="Grammar.txt", gp=None,
157:                 save=True, force=False, logger=None):
158:    """Load the grammar (maybe from a pickle)."""
159:    if logger is None:
160:        logger = logging.getLogger()
161:    gp = _generate_pickle_name(gt) if gp is None else gp
162:    if force or not _newer(gp, gt):
163:        logger.info("Generating grammar tables from %s", gt)
164:        g = pgen.generate_grammar(gt)
165:        if save:
166:            logger.info("Writing grammar tables to %s", gp)
167:            try:
168:                g.dump(gp)
169:            except OSError as e:
170:                logger.info("Writing failed: %s", e)
171:    else:
172:        g = grammar.Grammar()
173:        g.load(gp)
174:    return g
175:
176:
177:def _newer(a, b):
178:    """Inquire whether file a was written since file b."""
179:    if not os.path.exists(a):
180:        return False
181:    if not os.path.exists(b):
182:        return True
183:    return os.path.getmtime(a) >= os.path.getmtime(b)
184:
185:
186:def load_packaged_grammar(package, grammar_source, cache_dir=None):
187:    """Normally, loads a pickled grammar by doing
188:        pkgutil.get_data(package, pickled_grammar)
189:    where *pickled_grammar* is computed from *grammar_source* by adding the
190:    Python version and using a ``.pickle`` extension.
191:
192:    However, if *grammar_source* is an extant file, load_grammar(grammar_source)
193:    is called instead. This facilitates using a packaged grammar file when needed
194:    but preserves load_grammar's automatic regeneration behavior when possible.
195:
196:    """
197:    if os.path.isfile(grammar_source):
198:        gp = _generate_pickle_name(grammar_source, cache_dir) if cache_dir else None
199:        return load_grammar(grammar_source, gp=gp)

Calling tool: grepTool
current_column \+=
Matched content:
135:                current_column += 1

Calling tool: grepTool
elif char ==
Matched content:
136:            elif char == '\n':

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Modifications:
5:# Copyright 2006 Google, Inc. All Rights Reserved.
6:# Licensed to PSF under a Contributor Agreement.
7:
8:"""Parser driver.
9:
10:This provides a high-level interface to parse a file into a syntax tree.
11:
12:"""
13:
14:__author__ = "Guido van Rossum <guido@python.org>"
15:
16:__all__ = ["Driver", "load_grammar"]
17:
18:# Python imports
19:import codecs
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(self, grammar, convert=None, logger=None):
33:        self.grammar = grammar
34:        if logger is None:
35:            logger = logging.getLogger()
36:        self.logger = logger
37:        self.convert = convert
38:
39:    def parse_tokens(self, tokens, debug=False):
40:        """Parse a series of tokens and return the syntax tree."""
41:        # XXX Move the prefix computation into a wrapper around tokenize.
42:        p = parse.Parser(self.grammar, self.convert)
43:        p.setup()
44:        lineno = 1
45:        column = 0
46:        indent_columns = []
47:        type = value = start = end = line_text = None
48:        prefix = ""
49:        for quintuple in tokens:
50:            type, value, start, end, line_text = quintuple
51:            if start != (lineno, column):
52:                assert (lineno, column) <= start, ((lineno, column), start)
53:                s_lineno, s_column = start
54:                if lineno < s_lineno:
55:                    prefix += "\n" * (s_lineno - lineno)
56:                    lineno = s_lineno
57:                    column = 0
58:                if column < s_column:
59:                    prefix += line_text[column:s_column]
60:                    column = s_column
61:            if type in (tokenize.COMMENT, tokenize.NL):
62:                prefix += value
63:                lineno, column = end
64:                if value.endswith("\n"):
65:                    lineno += 1
66:                    column = 0
67:                continue
68:            if type == token.OP:
69:                type = grammar.opmap[value]
70:            if debug:
71:                self.logger.debug("%s %r (prefix=%r)",
72:                                  token.tok_name[type], value, prefix)
73:            if type == token.INDENT:
74:                indent_columns.append(len(value))
75:                _prefix = prefix + value
76:                prefix = ""
77:                value = ""
78:            elif type == token.DEDENT:
79:                _indent_col = indent_columns.pop()
80:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
81:            if p.addtoken(type, value, (prefix, start)):
82:                if debug:
83:                    self.logger.debug("Stop.")
84:                break
85:            prefix = ""
86:            if type in {token.INDENT, token.DEDENT}:
87:                prefix = _prefix
88:            lineno, column = end
89:            if value.endswith("\n"):
90:                lineno += 1
91:                column = 0
92:        else:
93:            # We never broke out -- EOF is too soon (how can this happen???)
94:            raise parse.ParseError("incomplete input",
95:                                   type, value, (prefix, start))
96:        return p.rootnode
97:
98:    def parse_stream_raw(self, stream, debug=False):
99:        """Parse a stream and return the syntax tree."""
100:        tokens = tokenize.generate_tokens(stream.readline)
101:        return self.parse_tokens(tokens, debug)
102:
103:    def parse_stream(self, stream, debug=False):
104:        """Parse a stream and return the syntax tree."""
105:        return self.parse_stream_raw(stream, debug)
106:
107:    def parse_file(self, filename, encoding=None, debug=False):
108:        """Parse a file and return the syntax tree."""
109:        with io.open(filename, "r", encoding=encoding) as stream:
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char in ' \t':
135:                current_column += 1
136:            elif char == '\n':
137:                # unexpected empty line
138:                current_column = 0
139:            else:
140:                # indent is finished

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
140:                # indent is finished
141:                wait_for_nl = True
142:        return ''.join(lines), current_line
143:
144:
145:def _generate_pickle_name(gt, cache_dir=None):
146:    head, tail = os.path.splitext(gt)
147:    if tail == ".txt":
148:        tail = ""
149:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
150:    if cache_dir:
151:        return os.path.join(cache_dir, os.path.basename(name))
152:    else:
153:        return name
154:
155:
156:def load_grammar(gt="Grammar.txt", gp=None,
157:                 save=True, force=False, logger=None):
158:    """Load the grammar (maybe from a pickle)."""
159:    if logger is None:
160:        logger = logging.getLogger()
161:    gp = _generate_pickle_name(gt) if gp is None else gp
162:    if force or not _newer(gp, gt):
163:        logger.info("Generating grammar tables from %s", gt)
164:        g = pgen.generate_grammar(gt)
165:        if save:
166:            logger.info("Writing grammar tables to %s", gp)
167:            try:
168:                g.dump(gp)
169:            except OSError as e:
170:                logger.info("Writing failed: %s", e)
171:    else:
172:        g = grammar.Grammar()
173:        g.load(gp)
174:    return g
175:
176:
177:def _newer(a, b):
178:    """Inquire whether file a was written since file b."""
179:    if not os.path.exists(a):

Calling tool: grepTool
\\t
Matched content:
134:            elif char in ' \t':

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: Searched `blib2to3/pgen2/driver.py` for the previous branch and related patterns. No occurrences of `elif char == '\t':` or `current_column += 4` remain. The tab handling is now unified under `elif char in ' \t': current_column += 1` within `_partially_consume_prefix`, and no other similar logic exists in the Driver class.


Todo item 2 of 10 is correct
description: 'Confirm that the previous branch `elif char == '\t': current_column += 4` has been fully removed from `blib2to3/pgen2/driver.py` and is not present elsewhere in the same function or class.'
Validating todo item 3 of 10
description: 'Search the repository (especially under `blib2to3/`) for any remaining logic that increments indent/column by 4 when encountering a tab (e.g., `current_column += 4` tied to `\t`) to ensure no conflicting tab-width assumptions remain for prefix/indent calculation.'
Calling tool: grepTool
current_column
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:120:        current_column = 0
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:126:                    if current_line.strip() and current_column < column:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:132:                    current_column = 0
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:135:                current_column += 1
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:138:                current_column = 0

Calling tool: grepTool
\+=\s*4
No files found. Call grepTool again with a more generic pattern.

Calling tool: grepTool
'\\t'
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:399:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize

Calling tool: grepTool
char\s*==\s*'\\t'
No files found. Call grepTool again with a more generic pattern.

Calling tool: grepTool
\\t
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:56:Whitespace = r'[ \f\t]*'
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:147:    print("%d,%d-%d,%d:\t%s\t%s" % \
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:229:cookie_re = re.compile(r'^[ \t\f]*#.*?coding[:=][ \t]*([-\w.]+)', re.ASCII)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:230:blank_re = re.compile(br'^[ \t\f]*(?:[#\r\n]|$)', re.ASCII)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:399:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:134:            elif char in ' \t':
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/data/function.py:59:        [\t ]*
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/data/function.py:62:            [^#\t\n]+?
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/data/function.py:69:        [\t ]*
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/data/function.py:203:        [\t ]*
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/data/function.py:206:            [^#\t\n]+?
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/data/function.py:213:        [\t ]*
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/conv.py:228:        assert line == "\tdfas,\n", (lineno, line)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/literals.py:13:                  "t": "\t",
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/data/fmtonoff.py:127:        [\t ]*
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/data/fmtonoff.py:130:            [^#\t\n]+?
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/data/fmtonoff.py:137:        [\t ]*
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/data/fmtonoff.py:345:        [\t ]*
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/data/fmtonoff.py:348:            [^#\t\n]+?
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/data/fmtonoff.py:355:        [\t ]*
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black.py:541:        src_name = f"{src}\t{then} +0000"
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black.py:542:        dst_name = f"{src}\t{now} +0000"
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black.py:592:            src_name = f"STDIN\t{then} +0000"
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black.py:593:            dst_name = f"STDOUT\t{now} +0000"
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:209:            rf"(STDIN|STDOUT)\t\d\d\d\d-\d\d-\d\d "
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:276:            rf"{re.escape(str(tmp_file))}\t\d\d\d\d-\d\d-\d\d "
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:513:        contents_tab = "if 1:\n\tif 2:\n\t\tpass\n\t# comment\n\tpass\n"
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:519:        contents_tab = "if 1:\n\tif 2:\n\t\tpass\n\t\t# comment\n\tpass\n"

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char in ' \t':
135:                current_column += 1
136:            elif char == '\n':
137:                # unexpected empty line
138:                current_column = 0
139:            else:
140:                # indent is finished
141:                wait_for_nl = True
142:        return ''.join(lines), current_line
143:
144:
145:def _generate_pickle_name(gt, cache_dir=None):
146:    head, tail = os.path.splitext(gt)
147:    if tail == ".txt":
148:        tail = ""
149:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
150:    if cache_dir:
151:        return os.path.join(cache_dir, os.path.basename(name))
152:    else:
153:        return name
154:
155:
156:def load_grammar(gt="Grammar.txt", gp=None,
157:                 save=True, force=False, logger=None):
158:    """Load the grammar (maybe from a pickle)."""
159:    if logger is None:
160:        logger = logging.getLogger()
161:    gp = _generate_pickle_name(gt) if gp is None else gp
162:    if force or not _newer(gp, gt):
163:        logger.info("Generating grammar tables from %s", gt)
164:        g = pgen.generate_grammar(gt)
165:        if save:
166:            logger.info("Writing grammar tables to %s", gp)
167:            try:
168:                g.dump(gp)
169:            except OSError as e:

Calling tool: grepTool
tabsize
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:138:tabsize = 8
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:399:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize

Calling tool: grepTool
==\s*'\\t'
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:399:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: Searched the repository for any instances of incrementing by 4 on encountering a tab. Only relevant code is in blib2to3/pgen2/driver.py where tabs now increment current_column by 1 (char in ' \t'). No other occurrences of "+= 4" or tab-specific increments were found; tokenize uses tabsize=8 but not for prefix/indent consumption.


Todo item 3 of 10 is correct
description: 'Search the repository (especially under `blib2to3/`) for any remaining logic that increments indent/column by 4 when encountering a tab (e.g., `current_column += 4` tied to `\t`) to ensure no conflicting tab-width assumptions remain for prefix/indent calculation.'
Validating todo item 4 of 10
description: 'Verify that the newline handling `elif char == '\n': current_column = 0` remains intact and appears after the whitespace branch, preserving the prior control flow order and semantics.'
Calling tool: grepTool
char == '\\n'
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:125:                if char == '\n':
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:136:            elif char == '\n':

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char in ' \t':
135:                current_column += 1
136:            elif char == '\n':
137:                # unexpected empty line
138:                current_column = 0
139:            else:
140:                # indent is finished
141:                wait_for_nl = True
142:        return ''.join(lines), current_line
143:
144:
145:def _generate_pickle_name(gt, cache_dir=None):
146:    head, tail = os.path.splitext(gt)
147:    if tail == ".txt":
148:        tail = ""
149:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
150:    if cache_dir:
151:        return os.path.join(cache_dir, os.path.basename(name))
152:    else:
153:        return name
154:
155:
156:def load_grammar(gt="Grammar.txt", gp=None,
157:                 save=True, force=False, logger=None):
158:    """Load the grammar (maybe from a pickle)."""
159:    if logger is None:
160:        logger = logging.getLogger()
161:    gp = _generate_pickle_name(gt) if gp is None else gp
162:    if force or not _newer(gp, gt):
163:        logger.info("Generating grammar tables from %s", gt)
164:        g = pgen.generate_grammar(gt)
165:        if save:
166:            logger.info("Writing grammar tables to %s", gp)
167:            try:
168:                g.dump(gp)
169:            except OSError as e:

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: In blib2to3/pgen2/driver.py within _partially_consume_prefix, the newline branch `elif char == '\n': current_column = 0` remains present and follows the whitespace branch `elif char in ' \t': current_column += 1` (lines 134-139), preserving the original control flow and semantics.


Todo item 4 of 10 is correct
description: 'Verify that the newline handling `elif char == '\n': current_column = 0` remains intact and appears after the whitespace branch, preserving the prior control flow order and semantics.'
Validating todo item 5 of 10
description: 'Check that the surrounding logic resetting state (e.g., `current_line = ''`, `current_column = 0`, `wait_for_nl = False`) remains unchanged aside from the tab/space handling and still precedes the whitespace branch.'
Calling tool: grepTool
current_line\s*=\s*""|current_line\s*=\s*''
Grep command failed with exit code 2: rg: /Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/{{FOLDER_PATH}}: IO error for operation on /Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/{{FOLDER_PATH}}: No such file or directory (os error 2)


Calling tool: grepTool
current_line\s*=\s*""|current_line\s*=\s*''
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:119:        current_line = ""
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:131:                    current_line = ""

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
100:        tokens = tokenize.generate_tokens(stream.readline)
101:        return self.parse_tokens(tokens, debug)
102:
103:    def parse_stream(self, stream, debug=False):
104:        """Parse a stream and return the syntax tree."""
105:        return self.parse_stream_raw(stream, debug)
106:
107:    def parse_file(self, filename, encoding=None, debug=False):
108:        """Parse a file and return the syntax tree."""
109:        with io.open(filename, "r", encoding=encoding) as stream:
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char in ' \t':
135:                current_column += 1
136:            elif char == '\n':
137:                # unexpected empty line
138:                current_column = 0
139:            else:
140:                # indent is finished
141:                wait_for_nl = True
142:        return ''.join(lines), current_line
143:
144:
145:def _generate_pickle_name(gt, cache_dir=None):
146:    head, tail = os.path.splitext(gt)
147:    if tail == ".txt":
148:        tail = ""
149:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
150:    if cache_dir:
151:        return os.path.join(cache_dir, os.path.basename(name))
152:    else:
153:        return name
154:
155:
156:def load_grammar(gt="Grammar.txt", gp=None,
157:                 save=True, force=False, logger=None):
158:    """Load the grammar (maybe from a pickle)."""
159:    if logger is None:
160:        logger = logging.getLogger()
161:    gp = _generate_pickle_name(gt) if gp is None else gp
162:    if force or not _newer(gp, gt):
163:        logger.info("Generating grammar tables from %s", gt)
164:        g = pgen.generate_grammar(gt)
165:        if save:
166:            logger.info("Writing grammar tables to %s", gp)
167:            try:
168:                g.dump(gp)
169:            except OSError as e:
170:                logger.info("Writing failed: %s", e)
171:    else:
172:        g = grammar.Grammar()
173:        g.load(gp)
174:    return g
175:
176:
177:def _newer(a, b):
178:    """Inquire whether file a was written since file b."""
179:    if not os.path.exists(a):

Calling tool: grepTool
wait_for_nl
Matched content:
121:        wait_for_nl = False
124:            if wait_for_nl:
133:                    wait_for_nl = False
141:                wait_for_nl = True

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: In blib2to3/pgen2/driver.py:_partially_consume_prefix(), the state reset (current_line="", current_column=0, wait_for_nl=False) remains intact at lines 131-133 and occurs within the wait_for_nl branch before the whitespace handling. Only the whitespace handling changed to `elif char in ' \t': current_column += 1`, removing the separate tab branch, matching the diff.


Todo item 5 of 10 is correct
description: 'Check that the surrounding logic resetting state (e.g., `current_line = ''`, `current_column = 0`, `wait_for_nl = False`) remains unchanged aside from the tab/space handling and still precedes the whitespace branch.'
Validating todo item 6 of 10
description: 'Confirm that only space and tab characters advance `current_column` in this loop (i.e., no other whitespace characters like `\r` or `\f` are being incremented), aligning with the PR intent to match lib2to3's prefix consumption.'
Calling tool: grepTool
class Driver\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:30:class Driver(object):

Calling tool: grepTool
current_column
Matched content:
120:        current_column = 0
126:                    if current_line.strip() and current_column < column:
132:                    current_column = 0
135:                current_column += 1
138:                current_column = 0

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
100:        tokens = tokenize.generate_tokens(stream.readline)
101:        return self.parse_tokens(tokens, debug)
102:
103:    def parse_stream(self, stream, debug=False):
104:        """Parse a stream and return the syntax tree."""
105:        return self.parse_stream_raw(stream, debug)
106:
107:    def parse_file(self, filename, encoding=None, debug=False):
108:        """Parse a file and return the syntax tree."""
109:        with io.open(filename, "r", encoding=encoding) as stream:
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char in ' \t':
135:                current_column += 1
136:            elif char == '\n':
137:                # unexpected empty line
138:                current_column = 0
139:            else:
140:                # indent is finished
141:                wait_for_nl = True
142:        return ''.join(lines), current_line
143:
144:
145:def _generate_pickle_name(gt, cache_dir=None):
146:    head, tail = os.path.splitext(gt)
147:    if tail == ".txt":
148:        tail = ""
149:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
150:    if cache_dir:
151:        return os.path.join(cache_dir, os.path.basename(name))
152:    else:
153:        return name
154:
155:
156:def load_grammar(gt="Grammar.txt", gp=None,
157:                 save=True, force=False, logger=None):
158:    """Load the grammar (maybe from a pickle)."""
159:    if logger is None:
160:        logger = logging.getLogger()
161:    gp = _generate_pickle_name(gt) if gp is None else gp
162:    if force or not _newer(gp, gt):
163:        logger.info("Generating grammar tables from %s", gt)
164:        g = pgen.generate_grammar(gt)
165:        if save:
166:            logger.info("Writing grammar tables to %s", gp)
167:            try:
168:                g.dump(gp)
169:            except OSError as e:
170:                logger.info("Writing failed: %s", e)
171:    else:
172:        g = grammar.Grammar()
173:        g.load(gp)
174:    return g
175:
176:
177:def _newer(a, b):
178:    """Inquire whether file a was written since file b."""
179:    if not os.path.exists(a):

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: In _partially_consume_prefix (driver.py lines 117-142), only the branch `elif char in ' \t': current_column += 1` advances the column. Newlines reset the column and all other characters (including \r, \f, etc.) trigger the indent-finished branch without incrementing, matching the PR intent.


Todo item 6 of 10 is correct
description: 'Confirm that only space and tab characters advance `current_column` in this loop (i.e., no other whitespace characters like `\r` or `\f` are being incremented), aligning with the PR intent to match lib2to3's prefix consumption.'
Validating todo item 7 of 10
description: 'Locate the test `test_comment_indentation` and verify that its input uses tabs (not spaces) for indentation and includes a dedent followed by a comment line and then a code line at the same indentation level.'
Calling tool: grepTool
test_comment_indentation
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/bugsinpy_run_test.sh:1:python -m unittest -q tests.test_black.BlackTestCase.test_comment_indentation
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:512:    def test_comment_indentation(self) -> None:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py
500:        self.assertFormatEqual(expected, actual)
501:        black.assert_equivalent(source, actual)
502:        black.assert_stable(source, actual, line_length=ll)
503:
504:    @patch("black.dump_to_file", dump_to_stderr)
505:    def test_bracket_match(self) -> None:
506:        source, expected = read_data("bracketmatch")
507:        actual = fs(source)
508:        self.assertFormatEqual(expected, actual)
509:        black.assert_equivalent(source, actual)
510:        black.assert_stable(source, actual, line_length=ll)
511:
512:    def test_comment_indentation(self) -> None:
513:        contents_tab = "if 1:\n\tif 2:\n\t\tpass\n\t# comment\n\tpass\n"
514:        contents_spc = "if 1:\n    if 2:\n        pass\n    # comment\n    pass\n"
515:
516:        self.assertFormatEqual(fs(contents_spc), contents_spc)
517:        self.assertFormatEqual(fs(contents_tab), contents_spc)
518:
519:        contents_tab = "if 1:\n\tif 2:\n\t\tpass\n\t\t# comment\n\tpass\n"
520:        contents_spc = "if 1:\n    if 2:\n        pass\n        # comment\n    pass\n"
521:
522:        self.assertFormatEqual(fs(contents_tab), contents_spc)
523:        self.assertFormatEqual(fs(contents_spc), contents_spc)
524:
525:    def test_report_verbose(self) -> None:
526:        report = black.Report(verbose=True)
527:        out_lines = []
528:        err_lines = []
529:
530:        def out(msg: str, **kwargs: Any) -> None:
531:            out_lines.append(msg)
532:
533:        def err(msg: str, **kwargs: Any) -> None:
534:            err_lines.append(msg)
535:
536:        with patch("black.out", out), patch("black.err", err):
537:            report.done(Path("f1"), black.Changed.NO)
538:            self.assertEqual(len(out_lines), 1)
539:            self.assertEqual(len(err_lines), 0)
540:            self.assertEqual(out_lines[-1], "f1 already well formatted, good job.")
541:            self.assertEqual(unstyle(str(report)), "1 file left unchanged.")
542:            self.assertEqual(report.return_code, 0)
543:            report.done(Path("f2"), black.Changed.YES)
544:            self.assertEqual(len(out_lines), 2)
545:            self.assertEqual(len(err_lines), 0)
546:            self.assertEqual(out_lines[-1], "reformatted f2")
547:            self.assertEqual(
548:                unstyle(str(report)), "1 file reformatted, 1 file left unchanged."
549:            )
550:            report.done(Path("f3"), black.Changed.CACHED)
551:            self.assertEqual(len(out_lines), 3)
552:            self.assertEqual(len(err_lines), 0)
553:            self.assertEqual(
554:                out_lines[-1], "f3 wasn't modified on disk since last run."
555:            )
556:            self.assertEqual(
557:                unstyle(str(report)), "1 file reformatted, 2 files left unchanged."
558:            )
559:            self.assertEqual(report.return_code, 0)
560:            report.check = True
561:            self.assertEqual(report.return_code, 1)
562:            report.check = False
563:            report.failed(Path("e1"), "boom")
564:            self.assertEqual(len(out_lines), 3)
565:            self.assertEqual(len(err_lines), 1)
566:            self.assertEqual(err_lines[-1], "error: cannot format e1: boom")
567:            self.assertEqual(
568:                unstyle(str(report)),
569:                "1 file reformatted, 2 files left unchanged, "
570:                "1 file failed to reformat.",
571:            )
572:            self.assertEqual(report.return_code, 123)
573:            report.done(Path("f3"), black.Changed.YES)
574:            self.assertEqual(len(out_lines), 4)
575:            self.assertEqual(len(err_lines), 1)
576:            self.assertEqual(out_lines[-1], "reformatted f3")
577:            self.assertEqual(
578:                unstyle(str(report)),
579:                "2 files reformatted, 2 files left unchanged, "

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: Located test in tests/test_black.py lines 512-523. The first contents_tab string uses literal tabs (\t) and shows a dedent from level 2 to 1 followed by a comment line and then a code line (pass) at the same indentation level.


Todo item 7 of 10 is correct
description: 'Locate the test `test_comment_indentation` and verify that its input uses tabs (not spaces) for indentation and includes a dedent followed by a comment line and then a code line at the same indentation level.'
Validating todo item 8 of 10
description: 'Verify that `test_comment_indentation` asserts that the comment's indentation matches the indentation of the subsequent code line, reflecting the PR's acceptance criteria.'
Calling tool: grepTool
test_comment_indentation
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/bugsinpy_run_test.sh:1:python -m unittest -q tests.test_black.BlackTestCase.test_comment_indentation
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:512:    def test_comment_indentation(self) -> None:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py
480:    @patch("black.dump_to_file", dump_to_stderr)
481:    def test_fmtonoff2(self) -> None:
482:        source, expected = read_data("fmtonoff2")
483:        actual = fs(source)
484:        self.assertFormatEqual(expected, actual)
485:        black.assert_equivalent(source, actual)
486:        black.assert_stable(source, actual, line_length=ll)
487:
488:    @patch("black.dump_to_file", dump_to_stderr)
489:    def test_remove_empty_parentheses_after_class(self) -> None:
490:        source, expected = read_data("class_blank_parentheses")
491:        actual = fs(source)
492:        self.assertFormatEqual(expected, actual)
493:        black.assert_equivalent(source, actual)
494:        black.assert_stable(source, actual, line_length=ll)
495:
496:    @patch("black.dump_to_file", dump_to_stderr)
497:    def test_new_line_between_class_and_code(self) -> None:
498:        source, expected = read_data("class_methods_new_line")
499:        actual = fs(source)
500:        self.assertFormatEqual(expected, actual)
501:        black.assert_equivalent(source, actual)
502:        black.assert_stable(source, actual, line_length=ll)
503:
504:    @patch("black.dump_to_file", dump_to_stderr)
505:    def test_bracket_match(self) -> None:
506:        source, expected = read_data("bracketmatch")
507:        actual = fs(source)
508:        self.assertFormatEqual(expected, actual)
509:        black.assert_equivalent(source, actual)
510:        black.assert_stable(source, actual, line_length=ll)
511:
512:    def test_comment_indentation(self) -> None:
513:        contents_tab = "if 1:\n\tif 2:\n\t\tpass\n\t# comment\n\tpass\n"
514:        contents_spc = "if 1:\n    if 2:\n        pass\n    # comment\n    pass\n"
515:
516:        self.assertFormatEqual(fs(contents_spc), contents_spc)
517:        self.assertFormatEqual(fs(contents_tab), contents_spc)
518:
519:        contents_tab = "if 1:\n\tif 2:\n\t\tpass\n\t\t# comment\n\tpass\n"
520:        contents_spc = "if 1:\n    if 2:\n        pass\n        # comment\n    pass\n"
521:
522:        self.assertFormatEqual(fs(contents_tab), contents_spc)
523:        self.assertFormatEqual(fs(contents_spc), contents_spc)
524:
525:    def test_report_verbose(self) -> None:
526:        report = black.Report(verbose=True)
527:        out_lines = []
528:        err_lines = []
529:
530:        def out(msg: str, **kwargs: Any) -> None:
531:            out_lines.append(msg)
532:
533:        def err(msg: str, **kwargs: Any) -> None:
534:            err_lines.append(msg)
535:
536:        with patch("black.out", out), patch("black.err", err):
537:            report.done(Path("f1"), black.Changed.NO)
538:            self.assertEqual(len(out_lines), 1)
539:            self.assertEqual(len(err_lines), 0)
540:            self.assertEqual(out_lines[-1], "f1 already well formatted, good job.")
541:            self.assertEqual(unstyle(str(report)), "1 file left unchanged.")
542:            self.assertEqual(report.return_code, 0)
543:            report.done(Path("f2"), black.Changed.YES)
544:            self.assertEqual(len(out_lines), 2)
545:            self.assertEqual(len(err_lines), 0)
546:            self.assertEqual(out_lines[-1], "reformatted f2")
547:            self.assertEqual(
548:                unstyle(str(report)), "1 file reformatted, 1 file left unchanged."
549:            )
550:            report.done(Path("f3"), black.Changed.CACHED)
551:            self.assertEqual(len(out_lines), 3)
552:            self.assertEqual(len(err_lines), 0)
553:            self.assertEqual(
554:                out_lines[-1], "f3 wasn't modified on disk since last run."
555:            )
556:            self.assertEqual(
557:                unstyle(str(report)), "1 file reformatted, 2 files left unchanged."
558:            )
559:            self.assertEqual(report.return_code, 0)
560:            report.check = True
561:            self.assertEqual(report.return_code, 1)
562:            report.check = False
563:            report.failed(Path("e1"), "boom")
564:            self.assertEqual(len(out_lines), 3)
565:            self.assertEqual(len(err_lines), 1)
566:            self.assertEqual(err_lines[-1], "error: cannot format e1: boom")
567:            self.assertEqual(
568:                unstyle(str(report)),
569:                "1 file reformatted, 2 files left unchanged, "
570:                "1 file failed to reformat.",
571:            )
572:            self.assertEqual(report.return_code, 123)
573:            report.done(Path("f3"), black.Changed.YES)
574:            self.assertEqual(len(out_lines), 4)
575:            self.assertEqual(len(err_lines), 1)
576:            self.assertEqual(out_lines[-1], "reformatted f3")
577:            self.assertEqual(
578:                unstyle(str(report)),
579:                "2 files reformatted, 2 files left unchanged, "
580:                "1 file failed to reformat.",
581:            )
582:            self.assertEqual(report.return_code, 123)
583:            report.failed(Path("e2"), "boom")
584:            self.assertEqual(len(out_lines), 4)
585:            self.assertEqual(len(err_lines), 2)
586:            self.assertEqual(err_lines[-1], "error: cannot format e2: boom")
587:            self.assertEqual(
588:                unstyle(str(report)),
589:                "2 files reformatted, 2 files left unchanged, "
590:                "2 files failed to reformat.",
591:            )
592:            self.assertEqual(report.return_code, 123)
593:            report.path_ignored(Path("wat"), "no match")
594:            self.assertEqual(len(out_lines), 5)
595:            self.assertEqual(len(err_lines), 2)
596:            self.assertEqual(out_lines[-1], "wat ignored: no match")
597:            self.assertEqual(
598:                unstyle(str(report)),
599:                "2 files reformatted, 2 files left unchanged, "

Calling tool: grepTool
class Driver\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:30:class Driver(object):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Modifications:
5:# Copyright 2006 Google, Inc. All Rights Reserved.
6:# Licensed to PSF under a Contributor Agreement.
7:
8:"""Parser driver.
9:
10:This provides a high-level interface to parse a file into a syntax tree.
11:
12:"""
13:
14:__author__ = "Guido van Rossum <guido@python.org>"
15:
16:__all__ = ["Driver", "load_grammar"]
17:
18:# Python imports
19:import codecs
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(self, grammar, convert=None, logger=None):
33:        self.grammar = grammar
34:        if logger is None:
35:            logger = logging.getLogger()
36:        self.logger = logger
37:        self.convert = convert
38:
39:    def parse_tokens(self, tokens, debug=False):
40:        """Parse a series of tokens and return the syntax tree."""
41:        # XXX Move the prefix computation into a wrapper around tokenize.
42:        p = parse.Parser(self.grammar, self.convert)
43:        p.setup()
44:        lineno = 1
45:        column = 0
46:        indent_columns = []
47:        type = value = start = end = line_text = None
48:        prefix = ""
49:        for quintuple in tokens:
50:            type, value, start, end, line_text = quintuple
51:            if start != (lineno, column):
52:                assert (lineno, column) <= start, ((lineno, column), start)
53:                s_lineno, s_column = start
54:                if lineno < s_lineno:
55:                    prefix += "\n" * (s_lineno - lineno)
56:                    lineno = s_lineno
57:                    column = 0
58:                if column < s_column:
59:                    prefix += line_text[column:s_column]
60:                    column = s_column
61:            if type in (tokenize.COMMENT, tokenize.NL):
62:                prefix += value
63:                lineno, column = end
64:                if value.endswith("\n"):
65:                    lineno += 1
66:                    column = 0
67:                continue
68:            if type == token.OP:
69:                type = grammar.opmap[value]
70:            if debug:
71:                self.logger.debug("%s %r (prefix=%r)",
72:                                  token.tok_name[type], value, prefix)
73:            if type == token.INDENT:
74:                indent_columns.append(len(value))
75:                _prefix = prefix + value
76:                prefix = ""
77:                value = ""
78:            elif type == token.DEDENT:
79:                _indent_col = indent_columns.pop()
80:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
81:            if p.addtoken(type, value, (prefix, start)):
82:                if debug:
83:                    self.logger.debug("Stop.")
84:                break
85:            prefix = ""
86:            if type in {token.INDENT, token.DEDENT}:
87:                prefix = _prefix
88:            lineno, column = end
89:            if value.endswith("\n"):
90:                lineno += 1
91:                column = 0
92:        else:
93:            # We never broke out -- EOF is too soon (how can this happen???)
94:            raise parse.ParseError("incomplete input",
95:                                   type, value, (prefix, start))
96:        return p.rootnode
97:
98:    def parse_stream_raw(self, stream, debug=False):
99:        """Parse a stream and return the syntax tree."""
100:        tokens = tokenize.generate_tokens(stream.readline)
101:        return self.parse_tokens(tokens, debug)
102:
103:    def parse_stream(self, stream, debug=False):
104:        """Parse a stream and return the syntax tree."""
105:        return self.parse_stream_raw(stream, debug)
106:
107:    def parse_file(self, filename, encoding=None, debug=False):
108:        """Parse a file and return the syntax tree."""
109:        with io.open(filename, "r", encoding=encoding) as stream:
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char in ' \t':
135:                current_column += 1
136:            elif char == '\n':
137:                # unexpected empty line
138:                current_column = 0
139:            else:
140:                # indent is finished
141:                wait_for_nl = True
142:        return ''.join(lines), current_line
143:
144:
145:def _generate_pickle_name(gt, cache_dir=None):
146:    head, tail = os.path.splitext(gt)
147:    if tail == ".txt":
148:        tail = ""
149:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
150:    if cache_dir:
151:        return os.path.join(cache_dir, os.path.basename(name))
152:    else:
153:        return name
154:
155:
156:def load_grammar(gt="Grammar.txt", gp=None,
157:                 save=True, force=False, logger=None):
158:    """Load the grammar (maybe from a pickle)."""
159:    if logger is None:
160:        logger = logging.getLogger()
161:    gp = _generate_pickle_name(gt) if gp is None else gp
162:    if force or not _newer(gp, gt):
163:        logger.info("Generating grammar tables from %s", gt)
164:        g = pgen.generate_grammar(gt)
165:        if save:
166:            logger.info("Writing grammar tables to %s", gp)
167:            try:
168:                g.dump(gp)
169:            except OSError as e:
170:                logger.info("Writing failed: %s", e)
171:    else:
172:        g = grammar.Grammar()
173:        g.load(gp)
174:    return g
175:
176:
177:def _newer(a, b):
178:    """Inquire whether file a was written since file b."""
179:    if not os.path.exists(a):
180:        return False
181:    if not os.path.exists(b):
182:        return True
183:    return os.path.getmtime(a) >= os.path.getmtime(b)
184:
185:
186:def load_packaged_grammar(package, grammar_source, cache_dir=None):
187:    """Normally, loads a pickled grammar by doing
188:        pkgutil.get_data(package, pickled_grammar)
189:    where *pickled_grammar* is computed from *grammar_source* by adding the
190:    Python version and using a ``.pickle`` extension.
191:
192:    However, if *grammar_source* is an extant file, load_grammar(grammar_source)
193:    is called instead. This facilitates using a packaged grammar file when needed
194:    but preserves load_grammar's automatic regeneration behavior when possible.
195:
196:    """
197:    if os.path.isfile(grammar_source):
198:        gp = _generate_pickle_name(grammar_source, cache_dir) if cache_dir else None
199:        return load_grammar(grammar_source, gp=gp)
200:    pickled_name = _generate_pickle_name(os.path.basename(grammar_source), cache_dir)
201:    data = pkgutil.get_data(package, pickled_name)
202:    g = grammar.Grammar()
203:    g.loads(data)
204:    return g
205:
206:
207:def main(*args):
208:    """Main program, when run as a script: produce grammar pickle files.
209:
210:    Calls load_grammar for each argument, a path to a grammar text file.
211:    """
212:    if not args:
213:        args = sys.argv[1:]
214:    logging.basicConfig(level=logging.INFO, stream=sys.stdout,
215:                        format='%(message)s')
216:    for gt in args:
217:        load_grammar(gt, save=True, force=True)
218:    return True
219:
220:if __name__ == "__main__":

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: Found test at tests/test_black.py:512-523. The first case asserts that a tab-indented comment after a dedent formats to match the indentation of the subsequent code line (expected space-indented form). The second case ensures comments inside the block retain inner indentation. This matches the PRs acceptance criteria.


Todo item 8 of 10 is correct
description: 'Verify that `test_comment_indentation` asserts that the comment's indentation matches the indentation of the subsequent code line, reflecting the PR's acceptance criteria.'
Validating todo item 9 of 10
description: 'Ensure `test_comment_indentation` executes the code path that computes comment/indent prefixes via `blib2to3/pgen2/driver.py` (e.g., by running the formatter/parser that relies on this driver) rather than bypassing it.'
Calling tool: grepTool
test_comment_indentation
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/bugsinpy_run_test.sh:1:python -m unittest -q tests.test_black.BlackTestCase.test_comment_indentation
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:512:    def test_comment_indentation(self) -> None:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py
480:    @patch("black.dump_to_file", dump_to_stderr)
481:    def test_fmtonoff2(self) -> None:
482:        source, expected = read_data("fmtonoff2")
483:        actual = fs(source)
484:        self.assertFormatEqual(expected, actual)
485:        black.assert_equivalent(source, actual)
486:        black.assert_stable(source, actual, line_length=ll)
487:
488:    @patch("black.dump_to_file", dump_to_stderr)
489:    def test_remove_empty_parentheses_after_class(self) -> None:
490:        source, expected = read_data("class_blank_parentheses")
491:        actual = fs(source)
492:        self.assertFormatEqual(expected, actual)
493:        black.assert_equivalent(source, actual)
494:        black.assert_stable(source, actual, line_length=ll)
495:
496:    @patch("black.dump_to_file", dump_to_stderr)
497:    def test_new_line_between_class_and_code(self) -> None:
498:        source, expected = read_data("class_methods_new_line")
499:        actual = fs(source)
500:        self.assertFormatEqual(expected, actual)
501:        black.assert_equivalent(source, actual)
502:        black.assert_stable(source, actual, line_length=ll)
503:
504:    @patch("black.dump_to_file", dump_to_stderr)
505:    def test_bracket_match(self) -> None:
506:        source, expected = read_data("bracketmatch")
507:        actual = fs(source)
508:        self.assertFormatEqual(expected, actual)
509:        black.assert_equivalent(source, actual)
510:        black.assert_stable(source, actual, line_length=ll)
511:
512:    def test_comment_indentation(self) -> None:
513:        contents_tab = "if 1:\n\tif 2:\n\t\tpass\n\t# comment\n\tpass\n"
514:        contents_spc = "if 1:\n    if 2:\n        pass\n    # comment\n    pass\n"
515:
516:        self.assertFormatEqual(fs(contents_spc), contents_spc)
517:        self.assertFormatEqual(fs(contents_tab), contents_spc)
518:
519:        contents_tab = "if 1:\n\tif 2:\n\t\tpass\n\t\t# comment\n\tpass\n"
520:        contents_spc = "if 1:\n    if 2:\n        pass\n        # comment\n    pass\n"
521:
522:        self.assertFormatEqual(fs(contents_tab), contents_spc)
523:        self.assertFormatEqual(fs(contents_spc), contents_spc)
524:
525:    def test_report_verbose(self) -> None:
526:        report = black.Report(verbose=True)
527:        out_lines = []
528:        err_lines = []
529:
530:        def out(msg: str, **kwargs: Any) -> None:
531:            out_lines.append(msg)
532:
533:        def err(msg: str, **kwargs: Any) -> None:
534:            err_lines.append(msg)
535:
536:        with patch("black.out", out), patch("black.err", err):
537:            report.done(Path("f1"), black.Changed.NO)
538:            self.assertEqual(len(out_lines), 1)
539:            self.assertEqual(len(err_lines), 0)
540:            self.assertEqual(out_lines[-1], "f1 already well formatted, good job.")
541:            self.assertEqual(unstyle(str(report)), "1 file left unchanged.")
542:            self.assertEqual(report.return_code, 0)
543:            report.done(Path("f2"), black.Changed.YES)
544:            self.assertEqual(len(out_lines), 2)
545:            self.assertEqual(len(err_lines), 0)
546:            self.assertEqual(out_lines[-1], "reformatted f2")
547:            self.assertEqual(
548:                unstyle(str(report)), "1 file reformatted, 1 file left unchanged."
549:            )
550:            report.done(Path("f3"), black.Changed.CACHED)
551:            self.assertEqual(len(out_lines), 3)
552:            self.assertEqual(len(err_lines), 0)
553:            self.assertEqual(
554:                out_lines[-1], "f3 wasn't modified on disk since last run."
555:            )
556:            self.assertEqual(
557:                unstyle(str(report)), "1 file reformatted, 2 files left unchanged."
558:            )
559:            self.assertEqual(report.return_code, 0)
560:            report.check = True
561:            self.assertEqual(report.return_code, 1)
562:            report.check = False
563:            report.failed(Path("e1"), "boom")
564:            self.assertEqual(len(out_lines), 3)
565:            self.assertEqual(len(err_lines), 1)
566:            self.assertEqual(err_lines[-1], "error: cannot format e1: boom")
567:            self.assertEqual(
568:                unstyle(str(report)),
569:                "1 file reformatted, 2 files left unchanged, "
570:                "1 file failed to reformat.",
571:            )
572:            self.assertEqual(report.return_code, 123)
573:            report.done(Path("f3"), black.Changed.YES)
574:            self.assertEqual(len(out_lines), 4)
575:            self.assertEqual(len(err_lines), 1)
576:            self.assertEqual(out_lines[-1], "reformatted f3")
577:            self.assertEqual(
578:                unstyle(str(report)),
579:                "2 files reformatted, 2 files left unchanged, "
580:                "1 file failed to reformat.",
581:            )
582:            self.assertEqual(report.return_code, 123)
583:            report.failed(Path("e2"), "boom")
584:            self.assertEqual(len(out_lines), 4)
585:            self.assertEqual(len(err_lines), 2)
586:            self.assertEqual(err_lines[-1], "error: cannot format e2: boom")
587:            self.assertEqual(
588:                unstyle(str(report)),
589:                "2 files reformatted, 2 files left unchanged, "
590:                "2 files failed to reformat.",
591:            )
592:            self.assertEqual(report.return_code, 123)
593:            report.path_ignored(Path("wat"), "no match")
594:            self.assertEqual(len(out_lines), 5)
595:            self.assertEqual(len(err_lines), 2)
596:            self.assertEqual(out_lines[-1], "wat ignored: no match")
597:            self.assertEqual(
598:                unstyle(str(report)),
599:                "2 files reformatted, 2 files left unchanged, "

Calling tool: grepTool
def fs\(
Grep command failed with exit code 2: rg: /Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/**/tests/test_black.py: IO error for operation on /Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/**/tests/test_black.py: No such file or directory (os error 2)


Calling tool: grepTool
def fs\(
No files found. Call grepTool again with a more generic pattern.

Calling tool: grepTool
\bfs\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:161:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:180:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:189:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:227:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:236:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:244:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:252:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:299:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:307:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:312:        not_normalized = fs(source, mode=mode)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:320:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:328:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:336:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:344:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:352:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:360:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:368:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:376:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:384:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:392:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:400:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:408:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:416:        actual = fs(source, mode=black.FileMode.PYTHON36)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:427:        actual = fs(source, mode=mode)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:435:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:442:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:450:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:458:        actual = fs(source, mode=mode)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:465:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:475:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:483:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:491:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:499:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:507:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:516:        self.assertFormatEqual(fs(contents_spc), contents_spc)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:517:        self.assertFormatEqual(fs(contents_tab), contents_spc)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:522:        self.assertFormatEqual(fs(contents_tab), contents_spc)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:523:        self.assertFormatEqual(fs(contents_spc), contents_spc)

Calling tool: grepTool
\bfs\b\s*=|def fs\b|from .* import fs|as fs
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:42:fs = partial(black.format_str, line_length=ll)

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py
1:#!/usr/bin/env python3
2:import asyncio
3:from concurrent.futures import ThreadPoolExecutor
4:from contextlib import contextmanager, redirect_stderr
5:from functools import partial, wraps
6:from io import BytesIO, TextIOWrapper
7:import os
8:from pathlib import Path
9:import re
10:import sys
11:from tempfile import TemporaryDirectory
12:from typing import (
13:    Any,
14:    BinaryIO,
15:    Callable,
16:    Coroutine,
17:    Generator,
18:    List,
19:    Tuple,
20:    Iterator,
21:    TypeVar,
22:)
23:import unittest
24:from unittest.mock import patch, MagicMock
25:
26:from click import unstyle
27:from click.testing import CliRunner
28:
29:import black
30:
31:try:
32:    import blackd
33:    from aiohttp.test_utils import TestClient, TestServer
34:except ImportError:
35:    has_blackd_deps = False
36:else:
37:    has_blackd_deps = True
38:
39:
40:ll = 88
41:ff = partial(black.format_file_in_place, line_length=ll, fast=True)
42:fs = partial(black.format_str, line_length=ll)
43:THIS_FILE = Path(__file__)
44:THIS_DIR = THIS_FILE.parent
45:EMPTY_LINE = "# EMPTY LINE WITH WHITESPACE" + " (this comment will be removed)"
46:T = TypeVar("T")
47:R = TypeVar("R")
48:
49:
50:def dump_to_stderr(*output: str) -> str:
51:    return "\n" + "\n".join(output) + "\n"
52:
53:
54:def read_data(name: str, data: bool = True) -> Tuple[str, str]:
55:    """read_data('test_name') -> 'input', 'output'"""
56:    if not name.endswith((".py", ".pyi", ".out", ".diff")):
57:        name += ".py"
58:    _input: List[str] = []
59:    _output: List[str] = []
60:    base_dir = THIS_DIR / "data" if data else THIS_DIR
61:    with open(base_dir / name, "r", encoding="utf8") as test:
62:        lines = test.readlines()
63:    result = _input
64:    for line in lines:
65:        line = line.replace(EMPTY_LINE, "")
66:        if line.rstrip() == "# output":
67:            result = _output
68:            continue
69:
70:        result.append(line)
71:    if _input and not _output:
72:        # If there's no output marker, treat the entire file as already pre-formatted.
73:        _output = _input[:]
74:    return "".join(_input).strip() + "\n", "".join(_output).strip() + "\n"
75:
76:
77:@contextmanager
78:def cache_dir(exists: bool = True) -> Iterator[Path]:
79:    with TemporaryDirectory() as workspace:
80:        cache_dir = Path(workspace)

Calling tool: grepTool
def format_str\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black.py:624:def format_str(

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black.py
600:    *,
601:    line_length: int,
602:    fast: bool,
603:    mode: FileMode = FileMode.AUTO_DETECT,
604:) -> FileContent:
605:    """Reformat contents a file and return new contents.
606:
607:    If `fast` is False, additionally confirm that the reformatted code is
608:    valid by calling :func:`assert_equivalent` and :func:`assert_stable` on it.
609:    `line_length` is passed to :func:`format_str`.
610:    """
611:    if src_contents.strip() == "":
612:        raise NothingChanged
613:
614:    dst_contents = format_str(src_contents, line_length=line_length, mode=mode)
615:    if src_contents == dst_contents:
616:        raise NothingChanged
617:
618:    if not fast:
619:        assert_equivalent(src_contents, dst_contents)
620:        assert_stable(src_contents, dst_contents, line_length=line_length, mode=mode)
621:    return dst_contents
622:
623:
624:def format_str(
625:    src_contents: str, line_length: int, *, mode: FileMode = FileMode.AUTO_DETECT
626:) -> FileContent:
627:    """Reformat a string and return new contents.
628:
629:    `line_length` determines how many characters per line are allowed.
630:    """
631:    src_node = lib2to3_parse(src_contents.lstrip())
632:    dst_contents = ""
633:    future_imports = get_future_imports(src_node)
634:    is_pyi = bool(mode & FileMode.PYI)
635:    py36 = bool(mode & FileMode.PYTHON36) or is_python36(src_node)
636:    normalize_strings = not bool(mode & FileMode.NO_STRING_NORMALIZATION)
637:    normalize_fmt_off(src_node)
638:    lines = LineGenerator(
639:        remove_u_prefix=py36 or "unicode_literals" in future_imports,
640:        is_pyi=is_pyi,
641:        normalize_strings=normalize_strings,
642:        allow_underscores=py36
643:        and not bool(mode & FileMode.NO_NUMERIC_UNDERSCORE_NORMALIZATION),
644:    )
645:    elt = EmptyLineTracker(is_pyi=is_pyi)
646:    empty_line = Line()
647:    after = 0
648:    for current_line in lines.visit(src_node):
649:        for _ in range(after):
650:            dst_contents += str(empty_line)
651:        before, after = elt.maybe_empty_lines(current_line)
652:        for _ in range(before):
653:            dst_contents += str(empty_line)
654:        for line in split_line(current_line, line_length=line_length, py36=py36):
655:            dst_contents += str(line)
656:    return dst_contents
657:
658:
659:def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:
660:    """Return a tuple of (decoded_contents, encoding, newline).
661:
662:    `newline` is either CRLF or LF but `decoded_contents` is decoded with
663:    universal newlines (i.e. only contains LF).
664:    """
665:    srcbuf = io.BytesIO(src)
666:    encoding, lines = tokenize.detect_encoding(srcbuf.readline)
667:    if not lines:
668:        return "", encoding, "\n"
669:
670:    newline = "\r\n" if b"\r\n" == lines[0][-2:] else "\n"
671:    srcbuf.seek(0)
672:    with io.TextIOWrapper(srcbuf, encoding) as tiow:
673:        return tiow.read(), encoding, newline
674:
675:
676:GRAMMARS = [
677:    pygram.python_grammar_no_print_statement_no_exec_statement,
678:    pygram.python_grammar_no_print_statement,
679:    pygram.python_grammar,
680:]
681:
682:
683:def lib2to3_parse(src_txt: str) -> Node:
684:    """Given a string with source, return the lib2to3 Node."""
685:    if src_txt[-1:] != "\n":
686:        src_txt += "\n"
687:    for grammar in GRAMMARS:
688:        drv = driver.Driver(grammar, pytree.convert)
689:        try:
690:            result = drv.parse_string(src_txt, True)
691:            break
692:
693:        except ParseError as pe:
694:            lineno, column = pe.context[1]
695:            lines = src_txt.splitlines()
696:            try:
697:                faulty_line = lines[lineno - 1]
698:            except IndexError:
699:                faulty_line = "<line number missing in source>"
700:            exc = InvalidInput(f"Cannot parse: {lineno}:{column}: {faulty_line}")
701:    else:
702:        raise exc from None
703:
704:    if isinstance(result, Leaf):
705:        result = Node(syms.file_input, [result])
706:    return result
707:
708:
709:def lib2to3_unparse(node: Node) -> str:
710:    """Given a lib2to3 node, return its string representation."""
711:    code = str(node)
712:    return code
713:
714:
715:T = TypeVar("T")
716:
717:
718:class Visitor(Generic[T]):
719:    """Basic lib2to3 visitor that yields things of type `T` on `visit()`."""

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Modifications:
5:# Copyright 2006 Google, Inc. All Rights Reserved.
6:# Licensed to PSF under a Contributor Agreement.
7:
8:"""Parser driver.
9:
10:This provides a high-level interface to parse a file into a syntax tree.
11:
12:"""
13:
14:__author__ = "Guido van Rossum <guido@python.org>"
15:
16:__all__ = ["Driver", "load_grammar"]
17:
18:# Python imports
19:import codecs
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(self, grammar, convert=None, logger=None):
33:        self.grammar = grammar
34:        if logger is None:
35:            logger = logging.getLogger()
36:        self.logger = logger
37:        self.convert = convert
38:
39:    def parse_tokens(self, tokens, debug=False):
40:        """Parse a series of tokens and return the syntax tree."""
41:        # XXX Move the prefix computation into a wrapper around tokenize.
42:        p = parse.Parser(self.grammar, self.convert)
43:        p.setup()
44:        lineno = 1
45:        column = 0
46:        indent_columns = []
47:        type = value = start = end = line_text = None
48:        prefix = ""
49:        for quintuple in tokens:
50:            type, value, start, end, line_text = quintuple
51:            if start != (lineno, column):
52:                assert (lineno, column) <= start, ((lineno, column), start)
53:                s_lineno, s_column = start
54:                if lineno < s_lineno:
55:                    prefix += "\n" * (s_lineno - lineno)
56:                    lineno = s_lineno
57:                    column = 0
58:                if column < s_column:
59:                    prefix += line_text[column:s_column]
60:                    column = s_column
61:            if type in (tokenize.COMMENT, tokenize.NL):
62:                prefix += value
63:                lineno, column = end
64:                if value.endswith("\n"):
65:                    lineno += 1
66:                    column = 0
67:                continue
68:            if type == token.OP:
69:                type = grammar.opmap[value]
70:            if debug:
71:                self.logger.debug("%s %r (prefix=%r)",
72:                                  token.tok_name[type], value, prefix)
73:            if type == token.INDENT:
74:                indent_columns.append(len(value))
75:                _prefix = prefix + value
76:                prefix = ""
77:                value = ""
78:            elif type == token.DEDENT:
79:                _indent_col = indent_columns.pop()
80:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
81:            if p.addtoken(type, value, (prefix, start)):
82:                if debug:
83:                    self.logger.debug("Stop.")
84:                break
85:            prefix = ""
86:            if type in {token.INDENT, token.DEDENT}:
87:                prefix = _prefix
88:            lineno, column = end
89:            if value.endswith("\n"):
90:                lineno += 1
91:                column = 0
92:        else:
93:            # We never broke out -- EOF is too soon (how can this happen???)
94:            raise parse.ParseError("incomplete input",
95:                                   type, value, (prefix, start))
96:        return p.rootnode
97:
98:    def parse_stream_raw(self, stream, debug=False):
99:        """Parse a stream and return the syntax tree."""
100:        tokens = tokenize.generate_tokens(stream.readline)
101:        return self.parse_tokens(tokens, debug)
102:
103:    def parse_stream(self, stream, debug=False):
104:        """Parse a stream and return the syntax tree."""
105:        return self.parse_stream_raw(stream, debug)
106:
107:    def parse_file(self, filename, encoding=None, debug=False):
108:        """Parse a file and return the syntax tree."""
109:        with io.open(filename, "r", encoding=encoding) as stream:
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char in ' \t':
135:                current_column += 1
136:            elif char == '\n':
137:                # unexpected empty line
138:                current_column = 0
139:            else:
140:                # indent is finished
141:                wait_for_nl = True
142:        return ''.join(lines), current_line
143:
144:
145:def _generate_pickle_name(gt, cache_dir=None):
146:    head, tail = os.path.splitext(gt)
147:    if tail == ".txt":
148:        tail = ""
149:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
150:    if cache_dir:
151:        return os.path.join(cache_dir, os.path.basename(name))
152:    else:
153:        return name
154:
155:
156:def load_grammar(gt="Grammar.txt", gp=None,
157:                 save=True, force=False, logger=None):
158:    """Load the grammar (maybe from a pickle)."""
159:    if logger is None:
160:        logger = logging.getLogger()
161:    gp = _generate_pickle_name(gt) if gp is None else gp
162:    if force or not _newer(gp, gt):
163:        logger.info("Generating grammar tables from %s", gt)
164:        g = pgen.generate_grammar(gt)
165:        if save:
166:            logger.info("Writing grammar tables to %s", gp)
167:            try:
168:                g.dump(gp)
169:            except OSError as e:
170:                logger.info("Writing failed: %s", e)
171:    else:
172:        g = grammar.Grammar()
173:        g.load(gp)
174:    return g
175:
176:
177:def _newer(a, b):
178:    """Inquire whether file a was written since file b."""
179:    if not os.path.exists(a):
180:        return False
181:    if not os.path.exists(b):
182:        return True
183:    return os.path.getmtime(a) >= os.path.getmtime(b)
184:
185:
186:def load_packaged_grammar(package, grammar_source, cache_dir=None):
187:    """Normally, loads a pickled grammar by doing
188:        pkgutil.get_data(package, pickled_grammar)
189:    where *pickled_grammar* is computed from *grammar_source* by adding the
190:    Python version and using a ``.pickle`` extension.
191:
192:    However, if *grammar_source* is an extant file, load_grammar(grammar_source)
193:    is called instead. This facilitates using a packaged grammar file when needed
194:    but preserves load_grammar's automatic regeneration behavior when possible.
195:
196:    """
197:    if os.path.isfile(grammar_source):
198:        gp = _generate_pickle_name(grammar_source, cache_dir) if cache_dir else None
199:        return load_grammar(grammar_source, gp=gp)
200:    pickled_name = _generate_pickle_name(os.path.basename(grammar_source), cache_dir)

Calling tool: grepTool
from blib2to3
Matched content:
45:from blib2to3.pytree import Node, Leaf, type_repr
46:from blib2to3 import pygram, pytree
47:from blib2to3.pgen2 import driver, token
48:from blib2to3.pgen2.parse import ParseError

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black.py
1:import asyncio
2:from asyncio.base_events import BaseEventLoop
3:from concurrent.futures import Executor, ProcessPoolExecutor
4:from datetime import datetime
5:from enum import Enum, Flag
6:from functools import lru_cache, partial, wraps
7:import io
8:import itertools
9:import logging
10:from multiprocessing import Manager, freeze_support
11:import os
12:from pathlib import Path
13:import pickle
14:import re
15:import signal
16:import sys
17:import tempfile
18:import tokenize
19:from typing import (
20:    Any,
21:    Callable,
22:    Collection,
23:    Dict,
24:    Generator,
25:    Generic,
26:    Iterable,
27:    Iterator,
28:    List,
29:    Optional,
30:    Pattern,
31:    Sequence,
32:    Set,
33:    Tuple,
34:    TypeVar,
35:    Union,
36:    cast,
37:)
38:
39:from appdirs import user_cache_dir
40:from attr import dataclass, Factory
41:import click
42:import toml
43:
44:# lib2to3 fork
45:from blib2to3.pytree import Node, Leaf, type_repr
46:from blib2to3 import pygram, pytree
47:from blib2to3.pgen2 import driver, token
48:from blib2to3.pgen2.parse import ParseError
49:
50:
51:__version__ = "18.9b0"
52:DEFAULT_LINE_LENGTH = 88
53:DEFAULT_EXCLUDES = (
54:    r"/(\.eggs|\.git|\.hg|\.mypy_cache|\.nox|\.tox|\.venv|_build|buck-out|build|dist)/"
55:)
56:DEFAULT_INCLUDES = r"\.pyi?$"
57:CACHE_DIR = Path(user_cache_dir("black", version=__version__))
58:
59:
60:# types
61:FileContent = str
62:Encoding = str
63:NewLine = str
64:Depth = int
65:NodeType = int
66:LeafID = int
67:Priority = int
68:Index = int
69:LN = Union[Leaf, Node]
70:SplitFunc = Callable[["Line", bool], Iterator["Line"]]
71:Timestamp = float
72:FileSize = int
73:CacheInfo = Tuple[Timestamp, FileSize]
74:Cache = Dict[Path, CacheInfo]
75:out = partial(click.secho, bold=True, err=True)
76:err = partial(click.secho, fg="red", err=True)
77:
78:pygram.initialize(CACHE_DIR)
79:syms = pygram.python_symbols
80:

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: The test uses fs = partial(black.format_str, ) and calls fs(contents_tab). format_str calls lib2to3_parse(), which constructs blib2to3.pgen2.driver.Driver and invokes parse_string()  parse_tokens(). In parse_tokens(), DEDENT handling calls _partially_consume_prefix(), the function changed in blib2to3/pgen2/driver.py. The test inputs include tabs and dedents around comments, ensuring this code path is exercised.


Todo item 9 of 10 is correct
description: 'Ensure `test_comment_indentation` executes the code path that computes comment/indent prefixes via `blib2to3/pgen2/driver.py` (e.g., by running the formatter/parser that relies on this driver) rather than bypassing it.'
Validating todo item 10 of 10
description: 'Check that no tests or code still assume a visual tab width of 4 in the context of prefix/indent calculation, to avoid inconsistency with the updated logic.'
Calling tool: grepTool
current_column
Matched content:
120:        current_column = 0
126:                    if current_line.strip() and current_column < column:
132:                    current_column = 0
135:                current_column += 1
138:                current_column = 0

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char in ' \t':
135:                current_column += 1
136:            elif char == '\n':
137:                # unexpected empty line
138:                current_column = 0
139:            else:
140:                # indent is finished
141:                wait_for_nl = True
142:        return ''.join(lines), current_line
143:
144:
145:def _generate_pickle_name(gt, cache_dir=None):
146:    head, tail = os.path.splitext(gt)
147:    if tail == ".txt":
148:        tail = ""
149:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
150:    if cache_dir:
151:        return os.path.join(cache_dir, os.path.basename(name))
152:    else:
153:        return name
154:
155:
156:def load_grammar(gt="Grammar.txt", gp=None,
157:                 save=True, force=False, logger=None):
158:    """Load the grammar (maybe from a pickle)."""
159:    if logger is None:
160:        logger = logging.getLogger()
161:    gp = _generate_pickle_name(gt) if gp is None else gp
162:    if force or not _newer(gp, gt):
163:        logger.info("Generating grammar tables from %s", gt)
164:        g = pgen.generate_grammar(gt)
165:        if save:
166:            logger.info("Writing grammar tables to %s", gp)
167:            try:
168:                g.dump(gp)
169:            except OSError as e:

Calling tool: grepTool
\texpandtabs\(
No files found. Call grepTool again with a more generic pattern.

Calling tool: grepTool
expandtabs\(
No files found. Call grepTool again with a more generic pattern.

Calling tool: grepTool
_partially_consume_prefix|prefix
Showing first 50 results from grep. Consider using a more specific pattern or path.
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:73:        ignoring the prefix string and other context information.
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:193:        effectively equivalent to node.next_sibling.prefix
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:198:        return next_sib.prefix
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:210:                 prefix=None,
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:227:        if prefix is not None:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:228:            self.prefix = prefix
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:273:    def prefix(self):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:279:        return self.children[0].prefix
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:281:    @prefix.setter
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:282:    def prefix(self, prefix):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:284:            self.children[0].prefix = prefix
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:336:    _prefix = ""  # Whitespace and comments preceding this token in the input
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:342:                 prefix=None,
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:352:            self._prefix, (self.lineno, self.column) = context
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:355:        if prefix is not None:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:356:            self._prefix = prefix
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:372:        return self.prefix + str(self.value)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:384:                    (self.prefix, (self.lineno, self.column)),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:399:    def prefix(self):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:403:        return self._prefix
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:405:    @prefix.setter
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:406:    def prefix(self, prefix):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:408:        self._prefix = prefix
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:81:_litprefix = r"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?"
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:82:Triple = group(_litprefix + "'''", _litprefix + '"""')
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:84:String = group(_litprefix + r"'[^\n'\\]*(?:\\.[^\n'\\]*)*'",
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:85:               _litprefix + r'"[^\n"\\]*(?:\\.[^\n"\\]*)*"')
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:103:ContStr = group(_litprefix + r"'[^\n'\\]*(?:\\.[^\n'\\]*)*" +
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:105:                _litprefix + r'"[^\n"\\]*(?:\\.[^\n"\\]*)*' +
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:115:_strprefixes = (
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:123:            **{f"{prefix}'''": single3prog for prefix in _strprefixes},
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:124:            **{f'{prefix}"""': double3prog for prefix in _strprefixes},
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:125:            **{prefix: None for prefix in _strprefixes}}
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:129:    {f"{prefix}'''" for prefix in _strprefixes} |
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:130:    {f'{prefix}"""' for prefix in _strprefixes}
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:134:    {f"{prefix}'" for prefix in _strprefixes} |
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:135:    {f'{prefix}"' for prefix in _strprefixes}
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:41:        # XXX Move the prefix computation into a wrapper around tokenize.
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:48:        prefix = ""
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:55:                    prefix += "\n" * (s_lineno - lineno)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:59:                    prefix += line_text[column:s_column]
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:62:                prefix += value
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:71:                self.logger.debug("%s %r (prefix=%r)",
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:72:                                  token.tok_name[type], value, prefix)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:75:                _prefix = prefix + value
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:76:                prefix = ""
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:80:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:81:            if p.addtoken(type, value, (prefix, start)):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:85:            prefix = ""
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:87:                prefix = _prefix

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Modifications:
5:# Copyright 2006 Google, Inc. All Rights Reserved.
6:# Licensed to PSF under a Contributor Agreement.
7:
8:"""Parser driver.
9:
10:This provides a high-level interface to parse a file into a syntax tree.
11:
12:"""
13:
14:__author__ = "Guido van Rossum <guido@python.org>"
15:
16:__all__ = ["Driver", "load_grammar"]
17:
18:# Python imports
19:import codecs
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(self, grammar, convert=None, logger=None):
33:        self.grammar = grammar
34:        if logger is None:
35:            logger = logging.getLogger()
36:        self.logger = logger
37:        self.convert = convert
38:
39:    def parse_tokens(self, tokens, debug=False):
40:        """Parse a series of tokens and return the syntax tree."""
41:        # XXX Move the prefix computation into a wrapper around tokenize.
42:        p = parse.Parser(self.grammar, self.convert)
43:        p.setup()
44:        lineno = 1
45:        column = 0
46:        indent_columns = []
47:        type = value = start = end = line_text = None
48:        prefix = ""
49:        for quintuple in tokens:
50:            type, value, start, end, line_text = quintuple
51:            if start != (lineno, column):
52:                assert (lineno, column) <= start, ((lineno, column), start)
53:                s_lineno, s_column = start
54:                if lineno < s_lineno:
55:                    prefix += "\n" * (s_lineno - lineno)
56:                    lineno = s_lineno
57:                    column = 0
58:                if column < s_column:
59:                    prefix += line_text[column:s_column]
60:                    column = s_column
61:            if type in (tokenize.COMMENT, tokenize.NL):
62:                prefix += value
63:                lineno, column = end
64:                if value.endswith("\n"):
65:                    lineno += 1
66:                    column = 0
67:                continue
68:            if type == token.OP:
69:                type = grammar.opmap[value]
70:            if debug:
71:                self.logger.debug("%s %r (prefix=%r)",
72:                                  token.tok_name[type], value, prefix)
73:            if type == token.INDENT:
74:                indent_columns.append(len(value))
75:                _prefix = prefix + value
76:                prefix = ""
77:                value = ""
78:            elif type == token.DEDENT:
79:                _indent_col = indent_columns.pop()
80:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
81:            if p.addtoken(type, value, (prefix, start)):
82:                if debug:
83:                    self.logger.debug("Stop.")
84:                break
85:            prefix = ""
86:            if type in {token.INDENT, token.DEDENT}:
87:                prefix = _prefix
88:            lineno, column = end
89:            if value.endswith("\n"):
90:                lineno += 1
91:                column = 0
92:        else:
93:            # We never broke out -- EOF is too soon (how can this happen???)
94:            raise parse.ParseError("incomplete input",
95:                                   type, value, (prefix, start))
96:        return p.rootnode
97:
98:    def parse_stream_raw(self, stream, debug=False):
99:        """Parse a stream and return the syntax tree."""
100:        tokens = tokenize.generate_tokens(stream.readline)
101:        return self.parse_tokens(tokens, debug)
102:
103:    def parse_stream(self, stream, debug=False):
104:        """Parse a stream and return the syntax tree."""
105:        return self.parse_stream_raw(stream, debug)
106:
107:    def parse_file(self, filename, encoding=None, debug=False):
108:        """Parse a file and return the syntax tree."""
109:        with io.open(filename, "r", encoding=encoding) as stream:
110:            return self.parse_stream(stream, debug)

Calling tool: grepTool
INDENT|DEDENT
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:216:            if toknum == INDENT:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:219:            elif toknum == DEDENT:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:424:                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:438:                yield (DEDENT, '', (lnum, pos), (lnum, pos), line)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:562:        yield (DEDENT, '', (lnum, 0), (lnum, 0), '')
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:73:            if type == token.INDENT:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:78:            elif type == token.DEDENT:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:86:            if type in {token.INDENT, token.DEDENT}:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/token.pyi:11:INDENT: int
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/token.pyi:12:DEDENT: int
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/token.py:12:INDENT = 5
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/token.py:13:DEDENT = 6
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/README:12:- INDENT and DEDENT don't hold whitespace and comment prefixes
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/Grammar.txt:83:suite: simple_stmt | NEWLINE INDENT stmt+ DEDENT

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py
380:                       strstart, (lnum, end), contline + line)
381:                contstr, needcont = '', 0
382:                contline = None
383:            elif needcont and line[-2:] != '\\\n' and line[-3:] != '\\\r\n':
384:                yield (ERRORTOKEN, contstr + line,
385:                           strstart, (lnum, len(line)), contline)
386:                contstr = ''
387:                contline = None
388:                continue
389:            else:
390:                contstr = contstr + line
391:                contline = contline + line
392:                continue
393:
394:        elif parenlev == 0 and not continued:  # new statement
395:            if not line: break
396:            column = 0
397:            while pos < max:                   # measure leading whitespace
398:                if line[pos] == ' ': column = column + 1
399:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize
400:                elif line[pos] == '\f': column = 0
401:                else: break
402:                pos = pos + 1
403:            if pos == max: break
404:
405:            if stashed:
406:                yield stashed
407:                stashed = None
408:
409:            if line[pos] in '\r\n':            # skip blank lines
410:                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)
411:                continue
412:
413:            if line[pos] == '#':               # skip comments
414:                comment_token = line[pos:].rstrip('\r\n')
415:                nl_pos = pos + len(comment_token)
416:                yield (COMMENT, comment_token,
417:                        (lnum, pos), (lnum, pos + len(comment_token)), line)
418:                yield (NL, line[nl_pos:],
419:                        (lnum, nl_pos), (lnum, len(line)), line)
420:                continue
421:
422:            if column > indents[-1]:           # count indents
423:                indents.append(column)
424:                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)
425:
426:            while column < indents[-1]:        # count dedents
427:                if column not in indents:
428:                    raise IndentationError(
429:                        "unindent does not match any outer indentation level",
430:                        ("<tokenize>", lnum, pos, line))
431:                indents = indents[:-1]
432:
433:                if async_def and async_def_indent >= indents[-1]:
434:                    async_def = False
435:                    async_def_nl = False
436:                    async_def_indent = 0
437:
438:                yield (DEDENT, '', (lnum, pos), (lnum, pos), line)
439:
440:            if async_def and async_def_nl and async_def_indent >= indents[-1]:
441:                async_def = False
442:                async_def_nl = False
443:                async_def_indent = 0
444:
445:        else:                                  # continued statement
446:            if not line:
447:                raise TokenError("EOF in multi-line statement", (lnum, 0))
448:            continued = 0
449:
450:        while pos < max:
451:            pseudomatch = pseudoprog.match(line, pos)
452:            if pseudomatch:                                # scan for tokens
453:                start, end = pseudomatch.span(1)
454:                spos, epos, pos = (lnum, start), (lnum, end), end
455:                token, initial = line[start:end], line[start]
456:
457:                if initial in numchars or \
458:                   (initial == '.' and token != '.'):      # ordinary number
459:                    yield (NUMBER, token, spos, epos, line)
460:                elif initial in '\r\n':
461:                    newline = NEWLINE
462:                    if parenlev > 0:
463:                        newline = NL
464:                    elif async_def:
465:                        async_def_nl = True
466:                    if stashed:
467:                        yield stashed
468:                        stashed = None
469:                    yield (newline, token, spos, epos, line)
470:
471:                elif initial == '#':
472:                    assert not token.endswith("\n")
473:                    if stashed:
474:                        yield stashed
475:                        stashed = None
476:                    yield (COMMENT, token, spos, epos, line)
477:                elif token in triple_quoted:
478:                    endprog = endprogs[token]
479:                    endmatch = endprog.match(line, pos)
480:                    if endmatch:                           # all on one line
481:                        pos = endmatch.end(0)
482:                        token = line[start:pos]
483:                        if stashed:
484:                            yield stashed
485:                            stashed = None
486:                        yield (STRING, token, spos, (lnum, pos), line)
487:                    else:
488:                        strstart = (lnum, start)           # multiple lines
489:                        contstr = line[start:]
490:                        contline = line
491:                        break
492:                elif initial in single_quoted or \
493:                    token[:2] in single_quoted or \
494:                    token[:3] in single_quoted:
495:                    if token[-1] == '\n':                  # continued string
496:                        strstart = (lnum, start)
497:                        endprog = (endprogs[initial] or endprogs[token[1]] or
498:                                   endprogs[token[2]])
499:                        contstr, needcont = line[start:], 1
500:                        contline = line
501:                        break
502:                    else:                                  # ordinary string
503:                        if stashed:
504:                            yield stashed
505:                            stashed = None
506:                        yield (STRING, token, spos, epos, line)
507:                elif initial.isidentifier():               # ordinary name
508:                    if token in ('async', 'await'):
509:                        if async_def:
510:                            yield (ASYNC if token == 'async' else AWAIT,
511:                                   token, spos, epos, line)
512:                            continue
513:
514:                    tok = (NAME, token, spos, epos, line)
515:                    if token == 'async' and not stashed:
516:                        stashed = tok
517:                        continue
518:
519:                    if token in ('def', 'for'):
520:                        if (stashed
521:                                and stashed[0] == NAME
522:                                and stashed[1] == 'async'):
523:
524:                            if token == 'def':
525:                                async_def = True
526:                                async_def_indent = indents[-1]
527:
528:                            yield (ASYNC, stashed[1],
529:                                   stashed[2], stashed[3],
530:                                   stashed[4])
531:                            stashed = None
532:
533:                    if stashed:
534:                        yield stashed
535:                        stashed = None
536:
537:                    yield tok
538:                elif initial == '\\':                      # continued stmt
539:                    # This yield is new; needed for better idempotency:
540:                    if stashed:
541:                        yield stashed
542:                        stashed = None
543:                    yield (NL, token, spos, (lnum, pos), line)
544:                    continued = 1
545:                else:
546:                    if initial in '([{': parenlev = parenlev + 1
547:                    elif initial in ')]}': parenlev = parenlev - 1
548:                    if stashed:
549:                        yield stashed
550:                        stashed = None
551:                    yield (OP, token, spos, epos, line)
552:            else:
553:                yield (ERRORTOKEN, line[pos],
554:                           (lnum, pos), (lnum, pos+1), line)
555:                pos = pos + 1
556:
557:    if stashed:
558:        yield stashed
559:        stashed = None
560:
561:    for indent in indents[1:]:                 # pop remaining indent levels
562:        yield (DEDENT, '', (lnum, 0), (lnum, 0), '')
563:    yield (ENDMARKER, '', (lnum, 0), (lnum, 0), '')
564:
565:if __name__ == '__main__':                     # testing
566:    import sys
567:    if len(sys.argv) > 1: tokenize(open(sys.argv[1]).readline)
568:    else: tokenize(sys.stdin.readline)

Calling tool: grepTool
tabsize
Matched content:
138:tabsize = 8
399:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize

Calling tool: grepTool
\\t'\):\s*current_column\s*\+=\s*4|\+\=\s*4
No files found. Call grepTool again with a more generic pattern.

Calling tool: grepTool
test_comment_indentation
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/bugsinpy_run_test.sh:1:python -m unittest -q tests.test_black.BlackTestCase.test_comment_indentation
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:512:    def test_comment_indentation(self) -> None:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py
480:    @patch("black.dump_to_file", dump_to_stderr)
481:    def test_fmtonoff2(self) -> None:
482:        source, expected = read_data("fmtonoff2")
483:        actual = fs(source)
484:        self.assertFormatEqual(expected, actual)
485:        black.assert_equivalent(source, actual)
486:        black.assert_stable(source, actual, line_length=ll)
487:
488:    @patch("black.dump_to_file", dump_to_stderr)
489:    def test_remove_empty_parentheses_after_class(self) -> None:
490:        source, expected = read_data("class_blank_parentheses")
491:        actual = fs(source)
492:        self.assertFormatEqual(expected, actual)
493:        black.assert_equivalent(source, actual)
494:        black.assert_stable(source, actual, line_length=ll)
495:
496:    @patch("black.dump_to_file", dump_to_stderr)
497:    def test_new_line_between_class_and_code(self) -> None:
498:        source, expected = read_data("class_methods_new_line")
499:        actual = fs(source)
500:        self.assertFormatEqual(expected, actual)
501:        black.assert_equivalent(source, actual)
502:        black.assert_stable(source, actual, line_length=ll)
503:
504:    @patch("black.dump_to_file", dump_to_stderr)
505:    def test_bracket_match(self) -> None:
506:        source, expected = read_data("bracketmatch")
507:        actual = fs(source)
508:        self.assertFormatEqual(expected, actual)
509:        black.assert_equivalent(source, actual)
510:        black.assert_stable(source, actual, line_length=ll)
511:
512:    def test_comment_indentation(self) -> None:
513:        contents_tab = "if 1:\n\tif 2:\n\t\tpass\n\t# comment\n\tpass\n"
514:        contents_spc = "if 1:\n    if 2:\n        pass\n    # comment\n    pass\n"
515:
516:        self.assertFormatEqual(fs(contents_spc), contents_spc)
517:        self.assertFormatEqual(fs(contents_tab), contents_spc)
518:
519:        contents_tab = "if 1:\n\tif 2:\n\t\tpass\n\t\t# comment\n\tpass\n"
520:        contents_spc = "if 1:\n    if 2:\n        pass\n        # comment\n    pass\n"
521:
522:        self.assertFormatEqual(fs(contents_tab), contents_spc)
523:        self.assertFormatEqual(fs(contents_spc), contents_spc)
524:
525:    def test_report_verbose(self) -> None:
526:        report = black.Report(verbose=True)
527:        out_lines = []
528:        err_lines = []
529:
530:        def out(msg: str, **kwargs: Any) -> None:
531:            out_lines.append(msg)
532:
533:        def err(msg: str, **kwargs: Any) -> None:
534:            err_lines.append(msg)
535:
536:        with patch("black.out", out), patch("black.err", err):
537:            report.done(Path("f1"), black.Changed.NO)
538:            self.assertEqual(len(out_lines), 1)
539:            self.assertEqual(len(err_lines), 0)
540:            self.assertEqual(out_lines[-1], "f1 already well formatted, good job.")
541:            self.assertEqual(unstyle(str(report)), "1 file left unchanged.")
542:            self.assertEqual(report.return_code, 0)
543:            report.done(Path("f2"), black.Changed.YES)
544:            self.assertEqual(len(out_lines), 2)
545:            self.assertEqual(len(err_lines), 0)
546:            self.assertEqual(out_lines[-1], "reformatted f2")
547:            self.assertEqual(
548:                unstyle(str(report)), "1 file reformatted, 1 file left unchanged."
549:            )
550:            report.done(Path("f3"), black.Changed.CACHED)
551:            self.assertEqual(len(out_lines), 3)
552:            self.assertEqual(len(err_lines), 0)
553:            self.assertEqual(
554:                out_lines[-1], "f3 wasn't modified on disk since last run."
555:            )
556:            self.assertEqual(
557:                unstyle(str(report)), "1 file reformatted, 2 files left unchanged."
558:            )
559:            self.assertEqual(report.return_code, 0)

Calling tool: grepTool
\t|tab\s*width|tabwidth|tabs|\b4\b
Grep command failed with exit code 2: rg: /Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black: IO error for operation on /Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black: No such file or directory (os error 2)


Calling tool: globTool
**/*
Showing first 50 results from grep. Consider using a more specific pattern or path.
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/Pipfile.lock
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.travis.yml
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/bugsinpy_patchfile.info
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.git/packed-refs
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/Pipfile
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/bugsinpy_bug.info
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.git/ORIG_HEAD
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/bugsinpy_requirements.txt
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/coverage_bugsinpy.txt
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/setup.cfg
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/setup.py
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/mypy.ini
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.git/index
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.git/description
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/readthedocs.yml
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black.py
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.flake8
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.rgignore
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/MANIFEST.in
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/bugsinpy_run_test.sh
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.appveyor.yml
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/README.md
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/plugin/black.vim
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.gitignore
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/CONTRIBUTING.md
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.pyi
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/PatternGrammar.txt
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/__init__.pyi
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/LICENSE
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/Grammar.txt
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/bugsinpy_compile_flag
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.git/hooks/push-to-checkout.sample
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.git/hooks/update.sample
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.git/hooks/pre-push.sample
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.git/hooks/pre-applypatch.sample
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.git/hooks/pre-merge-commit.sample
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.git/hooks/post-update.sample
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.git/hooks/prepare-commit-msg.sample
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.git/hooks/pre-receive.sample
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.git/hooks/fsmonitor-watchman.sample
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.git/hooks/applypatch-msg.sample
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.git/hooks/pre-commit.sample
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.git/hooks/pre-rebase.sample
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.git/hooks/commit-msg.sample
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/README
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pygram.py
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pygram.pyi
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.github/ISSUE_TEMPLATE.md

Calling tool: grepTool
\t|tab|indent
Matched content:
608:    valid by calling :func:`assert_equivalent` and :func:`assert_stable` on it.
620:        assert_stable(src_contents, dst_contents, line_length=line_length, mode=mode)
749:        indent = " " * (2 * self.tree_depth)
752:            out(f"{indent}{_type}", fg="yellow")
758:            out(f"{indent}/{_type}", fg="yellow", bold=False)
761:            out(f"{indent}{_type}", fg="blue", nl=False)
1292:        indent = "    " * self.depth
1295:        res = f"{first.prefix}{indent}{first.value}"
1432:    def line(self, indent: int = 0) -> Iterator[Line]:
1441:            self.current_line.depth += indent
1445:        self.current_line = Line(depth=complete_line.depth + indent)
1479:        """Increase indentation level, maybe yield a line."""
1485:        """Decrease indentation level, maybe yield a line."""
1492:        # that belong to the current indentation level.  Get 'em.
2328:    If `is_body` is True, the result line is one-indented inside brackets and as such
2336:            # Since body is a new indent level, remove spurious leading whitespace.
2352:def dont_increase_indentation(split_func: SplitFunc) -> SplitFunc:
2367:@dont_increase_indentation
2431:@dont_increase_indentation
2731:                # the previous leaf is indentation, it's a standalone comment in
3340:def assert_stable(
3480:    avoid some unfortunate formattings (mostly around wrapping unsplittable code

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black.py
1240:            comment.prefix = ""
1241:            return False
1242:
1243:        else:
1244:            leaf_id = id(self.leaves[-1])
1245:            if leaf_id not in self.comments:
1246:                self.comments[leaf_id] = [comment]
1247:            else:
1248:                self.comments[leaf_id].append(comment)
1249:            return True
1250:
1251:    def comments_after(self, leaf: Leaf) -> List[Leaf]:
1252:        """Generate comments that should appear directly after `leaf`."""
1253:        return self.comments.get(id(leaf), [])
1254:
1255:    def remove_trailing_comma(self) -> None:
1256:        """Remove the trailing comma and moves the comments attached to it."""
1257:        # Remember, the LeafID keys of self.comments are ordered by the
1258:        # corresponding leaf's index in self.leaves
1259:        # If id(self.leaves[-2]) is in self.comments, the order doesn't change.
1260:        # Otherwise, we insert it into self.comments, and it becomes the last entry.
1261:        # However, since we delete id(self.leaves[-1]) from self.comments, the invariant
1262:        # is maintained
1263:        self.comments.setdefault(id(self.leaves[-2]), []).extend(
1264:            self.comments.get(id(self.leaves[-1]), [])
1265:        )
1266:        self.comments.pop(id(self.leaves[-1]), None)
1267:        self.leaves.pop()
1268:
1269:    def is_complex_subscript(self, leaf: Leaf) -> bool:
1270:        """Return True iff `leaf` is part of a slice with non-trivial exprs."""
1271:        open_lsqb = self.bracket_tracker.get_open_lsqb()
1272:        if open_lsqb is None:
1273:            return False
1274:
1275:        subscript_start = open_lsqb.next_sibling
1276:
1277:        if isinstance(subscript_start, Node):
1278:            if subscript_start.type == syms.listmaker:
1279:                return False
1280:
1281:            if subscript_start.type == syms.subscriptlist:
1282:                subscript_start = child_towards(subscript_start, leaf)
1283:        return subscript_start is not None and any(
1284:            n.type in TEST_DESCENDANTS for n in subscript_start.pre_order()
1285:        )
1286:
1287:    def __str__(self) -> str:
1288:        """Render the line."""
1289:        if not self:
1290:            return "\n"
1291:
1292:        indent = "    " * self.depth
1293:        leaves = iter(self.leaves)
1294:        first = next(leaves)
1295:        res = f"{first.prefix}{indent}{first.value}"
1296:        for leaf in leaves:
1297:            res += str(leaf)
1298:        for comment in itertools.chain.from_iterable(self.comments.values()):
1299:            res += str(comment)
1300:        return res + "\n"
1301:
1302:    def __bool__(self) -> bool:
1303:        """Return True if the line has leaves or comments."""
1304:        return bool(self.leaves or self.comments)
1305:
1306:
1307:@dataclass
1308:class EmptyLineTracker:
1309:    """Provides a stateful method that returns the number of potential extra
1310:    empty lines needed before and after the currently processed line.
1311:
1312:    Note: this tracker works on lines that haven't been split yet.  It assumes
1313:    the prefix of the first leaf consists of optional newlines.  Those newlines
1314:    are consumed by `maybe_empty_lines()` and included in the computation.
1315:    """
1316:
1317:    is_pyi: bool = False
1318:    previous_line: Optional[Line] = None
1319:    previous_after: int = 0
1320:    previous_defs: List[int] = Factory(list)
1321:
1322:    def maybe_empty_lines(self, current_line: Line) -> Tuple[int, int]:
1323:        """Return the number of extra empty lines before and after the `current_line`.
1324:
1325:        This is for separating `def`, `async def` and `class` with extra empty
1326:        lines (two on module-level).
1327:        """
1328:        before, after = self._maybe_empty_lines(current_line)
1329:        before -= self.previous_after
1330:        self.previous_after = after
1331:        self.previous_line = current_line
1332:        return before, after
1333:
1334:    def _maybe_empty_lines(self, current_line: Line) -> Tuple[int, int]:
1335:        max_allowed = 1
1336:        if current_line.depth == 0:
1337:            max_allowed = 1 if self.is_pyi else 2
1338:        if current_line.leaves:
1339:            # Consume the first leaf's extra newlines.
1340:            first_leaf = current_line.leaves[0]
1341:            before = first_leaf.prefix.count("\n")
1342:            before = min(before, max_allowed)
1343:            first_leaf.prefix = ""
1344:        else:
1345:            before = 0
1346:        depth = current_line.depth
1347:        while self.previous_defs and self.previous_defs[-1] >= depth:
1348:            self.previous_defs.pop()
1349:            if self.is_pyi:
1350:                before = 0 if depth else 1
1351:            else:
1352:                before = 1 if depth else 2
1353:        if current_line.is_decorator or current_line.is_def or current_line.is_class:
1354:            return self._maybe_empty_lines_for_class_or_def(current_line, before)
1355:
1356:        if (
1357:            self.previous_line
1358:            and self.previous_line.is_import
1359:            and not current_line.is_import

Calling tool: grepTool
\t
No files found. Call grepTool again with a more generic pattern.

Calling tool: grepTool
column\b
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:338:    column = 0    # Column where this token tarts in the input
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:352:            self._prefix, (self.lineno, self.column) = context
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:384:                    (self.prefix, (self.lineno, self.column)),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:13:    the starting (row, column) indices of the token (a 2-tuple of ints)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:14:    the ending (row, column) indices of the token (a 2-tuple of ints)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:348:    column where the token begins in the source; a 2-tuple (erow, ecol) of
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:349:    ints specifying the row and column where the token ends in the source;
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:396:            column = 0
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:398:                if line[pos] == ' ': column = column + 1
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:399:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:400:                elif line[pos] == '\f': column = 0
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:422:            if column > indents[-1]:           # count indents
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:423:                indents.append(column)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:426:            while column < indents[-1]:        # count dedents
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:427:                if column not in indents:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:45:        column = 0
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:51:            if start != (lineno, column):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:52:                assert (lineno, column) <= start, ((lineno, column), start)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:53:                s_lineno, s_column = start
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:57:                    column = 0
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:58:                if column < s_column:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:59:                    prefix += line_text[column:s_column]
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:60:                    column = s_column
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:63:                lineno, column = end
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:66:                    column = 0
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:88:            lineno, column = end
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:91:                column = 0
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:117:    def _partially_consume_prefix(self, prefix, column):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:120:        current_column = 0
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:126:                    if current_line.strip() and current_column < column:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:132:                    current_column = 0
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:135:                current_column += 1
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:138:                current_column = 0
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.pyi:55:    column: int

Calling tool: grepTool
indent_columns
Matched content:
46:        indent_columns = []
74:                indent_columns.append(len(value))
79:                _indent_col = indent_columns.pop()

Calling tool: grepTool
tab\s*width|tabwidth|visual\s*tab|\b4\s*spaces
No files found. Call grepTool again with a more generic pattern.

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: Searched the codebase for any assumptions of tab width 4 in prefix/indent handling. The only relevant logic is in blib2to3/pgen2/driver.py:_partially_consume_prefix, which now counts both space and tab as +1. No other code uses +4 or expandtabs; tokenize uses tabsize=8 (unrelated). The test test_comment_indentation expects correct behavior and doesnt assume tab width 4. Thus, no remaining 4-width assumptions remain.


Todo item 10 of 10 is correct
description: 'Check that no tests or code still assume a visual tab width of 4 in the context of prefix/indent calculation, to avoid inconsistency with the updated logic.'
Validating todo item 1 of 10
description: 'In `blib2to3/pgen2/driver.py`, verify that the whitespace handling now uses `elif char in ' \t': current_column += 1` so that both spaces and tabs increment `current_column` by 1.'

Calling tool: globTool
**/blib2to3/pgen2/driver.py
Matched files:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py

Calling tool: grepTool
elif char in ' \t':
No files found. Call grepTool again with a more generic pattern.

Calling tool: grepTool
current_column \+=
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:135:                current_column += 1

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char in ' \t':
135:                current_column += 1
136:            elif char == '\n':
137:                # unexpected empty line
138:                current_column = 0
139:            else:
140:                # indent is finished
141:                wait_for_nl = True
142:        return ''.join(lines), current_line
143:
144:
145:def _generate_pickle_name(gt, cache_dir=None):
146:    head, tail = os.path.splitext(gt)
147:    if tail == ".txt":
148:        tail = ""
149:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
150:    if cache_dir:
151:        return os.path.join(cache_dir, os.path.basename(name))
152:    else:
153:        return name
154:
155:
156:def load_grammar(gt="Grammar.txt", gp=None,
157:                 save=True, force=False, logger=None):
158:    """Load the grammar (maybe from a pickle)."""
159:    if logger is None:
160:        logger = logging.getLogger()
161:    gp = _generate_pickle_name(gt) if gp is None else gp
162:    if force or not _newer(gp, gt):
163:        logger.info("Generating grammar tables from %s", gt)
164:        g = pgen.generate_grammar(gt)
165:        if save:
166:            logger.info("Writing grammar tables to %s", gp)
167:            try:
168:                g.dump(gp)
169:            except OSError as e:

Calling tool: grepTool
\\t
Matched content:
134:            elif char in ' \t':

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: In blib2to3/pgen2/driver.py within _partially_consume_prefix, the whitespace handling now uses `elif char in ' \t': current_column += 1` (lines 134-135), ensuring both spaces and tabs increment current_column by 1. No other tab handling exists in the file.

Todo item 1 of 10 is correct
description: 'In `blib2to3/pgen2/driver.py`, verify that the whitespace handling now uses `elif char in ' \t': current_column += 1` so that both spaces and tabs increment `current_column` by 1.'

Validating todo item 2 of 10
description: 'Confirm that the previous branch `elif char == '\t': current_column += 4` has been fully removed from `blib2to3/pgen2/driver.py` and is not present elsewhere in the same function or class.'

Calling tool: grepTool
driver\.py
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/bugsinpy_patchfile.info:1:blib2to3/pgen2/driver.py;
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/parse.py:45:    See driver.py for how to get input tokens by tokenizing a file or
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black.py:2008:    in `pgen2/driver.py:Driver.parse_tokens()`.  This was a brilliant implementation

Calling tool: globTool
**/blib2to3/pgen2/driver.py
Matched files:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py

Calling tool: grepTool
char == '\\t'
No files found. Call grepTool again with a more generic pattern.

Calling tool: grepTool
current_column \+= 4
No files found. Call grepTool again with a more generic pattern.

Calling tool: grepTool
elif char in ' \\t':
Matched content:
134:            elif char in ' \t':

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char in ' \t':
135:                current_column += 1
136:            elif char == '\n':
137:                # unexpected empty line
138:                current_column = 0
139:            else:
140:                # indent is finished
141:                wait_for_nl = True
142:        return ''.join(lines), current_line
143:
144:
145:def _generate_pickle_name(gt, cache_dir=None):
146:    head, tail = os.path.splitext(gt)
147:    if tail == ".txt":
148:        tail = ""
149:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
150:    if cache_dir:
151:        return os.path.join(cache_dir, os.path.basename(name))
152:    else:
153:        return name
154:
155:
156:def load_grammar(gt="Grammar.txt", gp=None,
157:                 save=True, force=False, logger=None):
158:    """Load the grammar (maybe from a pickle)."""
159:    if logger is None:
160:        logger = logging.getLogger()
161:    gp = _generate_pickle_name(gt) if gp is None else gp
162:    if force or not _newer(gp, gt):
163:        logger.info("Generating grammar tables from %s", gt)
164:        g = pgen.generate_grammar(gt)
165:        if save:
166:            logger.info("Writing grammar tables to %s", gp)
167:            try:
168:                g.dump(gp)
169:            except OSError as e:
170:                logger.info("Writing failed: %s", e)
171:    else:
172:        g = grammar.Grammar()
173:        g.load(gp)
174:    return g
175:
176:
177:def _newer(a, b):
178:    """Inquire whether file a was written since file b."""
179:    if not os.path.exists(a):
180:        return False
181:    if not os.path.exists(b):
182:        return True
183:    return os.path.getmtime(a) >= os.path.getmtime(b)
184:
185:
186:def load_packaged_grammar(package, grammar_source, cache_dir=None):
187:    """Normally, loads a pickled grammar by doing
188:        pkgutil.get_data(package, pickled_grammar)
189:    where *pickled_grammar* is computed from *grammar_source* by adding the
190:    Python version and using a ``.pickle`` extension.
191:
192:    However, if *grammar_source* is an extant file, load_grammar(grammar_source)
193:    is called instead. This facilitates using a packaged grammar file when needed
194:    but preserves load_grammar's automatic regeneration behavior when possible.
195:
196:    """
197:    if os.path.isfile(grammar_source):
198:        gp = _generate_pickle_name(grammar_source, cache_dir) if cache_dir else None
199:        return load_grammar(grammar_source, gp=gp)

Calling tool: grepTool
current_column \+=
Matched content:
135:                current_column += 1

Calling tool: grepTool
elif char ==
Matched content:
136:            elif char == '\n':

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Modifications:
5:# Copyright 2006 Google, Inc. All Rights Reserved.
6:# Licensed to PSF under a Contributor Agreement.
7:
8:"""Parser driver.
9:
10:This provides a high-level interface to parse a file into a syntax tree.
11:
12:"""
13:
14:__author__ = "Guido van Rossum <guido@python.org>"
15:
16:__all__ = ["Driver", "load_grammar"]
17:
18:# Python imports
19:import codecs
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(self, grammar, convert=None, logger=None):
33:        self.grammar = grammar
34:        if logger is None:
35:            logger = logging.getLogger()
36:        self.logger = logger
37:        self.convert = convert
38:
39:    def parse_tokens(self, tokens, debug=False):
40:        """Parse a series of tokens and return the syntax tree."""
41:        # XXX Move the prefix computation into a wrapper around tokenize.
42:        p = parse.Parser(self.grammar, self.convert)
43:        p.setup()
44:        lineno = 1
45:        column = 0
46:        indent_columns = []
47:        type = value = start = end = line_text = None
48:        prefix = ""
49:        for quintuple in tokens:
50:            type, value, start, end, line_text = quintuple
51:            if start != (lineno, column):
52:                assert (lineno, column) <= start, ((lineno, column), start)
53:                s_lineno, s_column = start
54:                if lineno < s_lineno:
55:                    prefix += "\n" * (s_lineno - lineno)
56:                    lineno = s_lineno
57:                    column = 0
58:                if column < s_column:
59:                    prefix += line_text[column:s_column]
60:                    column = s_column
61:            if type in (tokenize.COMMENT, tokenize.NL):
62:                prefix += value
63:                lineno, column = end
64:                if value.endswith("\n"):
65:                    lineno += 1
66:                    column = 0
67:                continue
68:            if type == token.OP:
69:                type = grammar.opmap[value]
70:            if debug:
71:                self.logger.debug("%s %r (prefix=%r)",
72:                                  token.tok_name[type], value, prefix)
73:            if type == token.INDENT:
74:                indent_columns.append(len(value))
75:                _prefix = prefix + value
76:                prefix = ""
77:                value = ""
78:            elif type == token.DEDENT:
79:                _indent_col = indent_columns.pop()
80:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
81:            if p.addtoken(type, value, (prefix, start)):
82:                if debug:
83:                    self.logger.debug("Stop.")
84:                break
85:            prefix = ""
86:            if type in {token.INDENT, token.DEDENT}:
87:                prefix = _prefix
88:            lineno, column = end
89:            if value.endswith("\n"):
90:                lineno += 1
91:                column = 0
92:        else:
93:            # We never broke out -- EOF is too soon (how can this happen???)
94:            raise parse.ParseError("incomplete input",
95:                                   type, value, (prefix, start))
96:        return p.rootnode
97:
98:    def parse_stream_raw(self, stream, debug=False):
99:        """Parse a stream and return the syntax tree."""
100:        tokens = tokenize.generate_tokens(stream.readline)
101:        return self.parse_tokens(tokens, debug)
102:
103:    def parse_stream(self, stream, debug=False):
104:        """Parse a stream and return the syntax tree."""
105:        return self.parse_stream_raw(stream, debug)
106:
107:    def parse_file(self, filename, encoding=None, debug=False):
108:        """Parse a file and return the syntax tree."""
109:        with io.open(filename, "r", encoding=encoding) as stream:
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char in ' \t':
135:                current_column += 1
136:            elif char == '\n':
137:                # unexpected empty line
138:                current_column = 0
139:            else:
140:                # indent is finished

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
140:                # indent is finished
141:                wait_for_nl = True
142:        return ''.join(lines), current_line
143:
144:
145:def _generate_pickle_name(gt, cache_dir=None):
146:    head, tail = os.path.splitext(gt)
147:    if tail == ".txt":
148:        tail = ""
149:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
150:    if cache_dir:
151:        return os.path.join(cache_dir, os.path.basename(name))
152:    else:
153:        return name
154:
155:
156:def load_grammar(gt="Grammar.txt", gp=None,
157:                 save=True, force=False, logger=None):
158:    """Load the grammar (maybe from a pickle)."""
159:    if logger is None:
160:        logger = logging.getLogger()
161:    gp = _generate_pickle_name(gt) if gp is None else gp
162:    if force or not _newer(gp, gt):
163:        logger.info("Generating grammar tables from %s", gt)
164:        g = pgen.generate_grammar(gt)
165:        if save:
166:            logger.info("Writing grammar tables to %s", gp)
167:            try:
168:                g.dump(gp)
169:            except OSError as e:
170:                logger.info("Writing failed: %s", e)
171:    else:
172:        g = grammar.Grammar()
173:        g.load(gp)
174:    return g
175:
176:
177:def _newer(a, b):
178:    """Inquire whether file a was written since file b."""
179:    if not os.path.exists(a):

Calling tool: grepTool
\\t
Matched content:
134:            elif char in ' \t':

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: Searched `blib2to3/pgen2/driver.py` for the previous branch and related patterns. No occurrences of `elif char == '\t':` or `current_column += 4` remain. The tab handling is now unified under `elif char in ' \t': current_column += 1` within `_partially_consume_prefix`, and no other similar logic exists in the Driver class.

Todo item 2 of 10 is correct
description: 'Confirm that the previous branch `elif char == '\t': current_column += 4` has been fully removed from `blib2to3/pgen2/driver.py` and is not present elsewhere in the same function or class.'

Validating todo item 3 of 10
description: 'Search the repository (especially under `blib2to3/`) for any remaining logic that increments indent/column by 4 when encountering a tab (e.g., `current_column += 4` tied to `\t`) to ensure no conflicting tab-width assumptions remain for prefix/indent calculation.'

Calling tool: grepTool
current_column
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:120:        current_column = 0
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:126:                    if current_line.strip() and current_column < column:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:132:                    current_column = 0
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:135:                current_column += 1
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:138:                current_column = 0

Calling tool: grepTool
\+=\s*4
No files found. Call grepTool again with a more generic pattern.

Calling tool: grepTool
'\\t'
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:399:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize

Calling tool: grepTool
char\s*==\s*'\\t'
No files found. Call grepTool again with a more generic pattern.

Calling tool: grepTool
\\t
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:56:Whitespace = r'[ \f\t]*'
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:147:    print("%d,%d-%d,%d:\t%s\t%s" % \
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:229:cookie_re = re.compile(r'^[ \t\f]*#.*?coding[:=][ \t]*([-\w.]+)', re.ASCII)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:230:blank_re = re.compile(br'^[ \t\f]*(?:[#\r\n]|$)', re.ASCII)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:399:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:134:            elif char in ' \t':
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/data/function.py:59:        [\t ]*
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/data/function.py:62:            [^#\t\n]+?
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/data/function.py:69:        [\t ]*
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/data/function.py:203:        [\t ]*
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/data/function.py:206:            [^#\t\n]+?
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/data/function.py:213:        [\t ]*
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/conv.py:228:        assert line == "\tdfas,\n", (lineno, line)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/literals.py:13:                  "t": "\t",
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/data/fmtonoff.py:127:        [\t ]*
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/data/fmtonoff.py:130:            [^#\t\n]+?
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/data/fmtonoff.py:137:        [\t ]*
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/data/fmtonoff.py:345:        [\t ]*
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/data/fmtonoff.py:348:            [^#\t\n]+?
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/data/fmtonoff.py:355:        [\t ]*
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black.py:541:        src_name = f"{src}\t{then} +0000"
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black.py:542:        dst_name = f"{src}\t{now} +0000"
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black.py:592:            src_name = f"STDIN\t{then} +0000"
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black.py:593:            dst_name = f"STDOUT\t{now} +0000"
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:209:            rf"(STDIN|STDOUT)\t\d\d\d\d-\d\d-\d\d "
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:276:            rf"{re.escape(str(tmp_file))}\t\d\d\d\d-\d\d-\d\d "
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:513:        contents_tab = "if 1:\n\tif 2:\n\t\tpass\n\t# comment\n\tpass\n"
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:519:        contents_tab = "if 1:\n\tif 2:\n\t\tpass\n\t\t# comment\n\tpass\n"

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char in ' \t':
135:                current_column += 1
136:            elif char == '\n':
137:                # unexpected empty line
138:                current_column = 0
139:            else:
140:                # indent is finished
141:                wait_for_nl = True
142:        return ''.join(lines), current_line
143:
144:
145:def _generate_pickle_name(gt, cache_dir=None):
146:    head, tail = os.path.splitext(gt)
147:    if tail == ".txt":
148:        tail = ""
149:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
150:    if cache_dir:
151:        return os.path.join(cache_dir, os.path.basename(name))
152:    else:
153:        return name
154:
155:
156:def load_grammar(gt="Grammar.txt", gp=None,
157:                 save=True, force=False, logger=None):
158:    """Load the grammar (maybe from a pickle)."""
159:    if logger is None:
160:        logger = logging.getLogger()
161:    gp = _generate_pickle_name(gt) if gp is None else gp
162:    if force or not _newer(gp, gt):
163:        logger.info("Generating grammar tables from %s", gt)
164:        g = pgen.generate_grammar(gt)
165:        if save:
166:            logger.info("Writing grammar tables to %s", gp)
167:            try:
168:                g.dump(gp)
169:            except OSError as e:

Calling tool: grepTool
tabsize
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:138:tabsize = 8
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:399:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize

Calling tool: grepTool
==\s*'\\t'
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:399:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: Searched the repository for any instances of incrementing by 4 on encountering a tab. Only relevant code is in blib2to3/pgen2/driver.py where tabs now increment current_column by 1 (char in ' \t'). No other occurrences of "+= 4" or tab-specific increments were found; tokenize uses tabsize=8 but not for prefix/indent consumption.

Todo item 3 of 10 is correct
description: 'Search the repository (especially under `blib2to3/`) for any remaining logic that increments indent/column by 4 when encountering a tab (e.g., `current_column += 4` tied to `\t`) to ensure no conflicting tab-width assumptions remain for prefix/indent calculation.'

Validating todo item 4 of 10
description: 'Verify that the newline handling `elif char == '\n': current_column = 0` remains intact and appears after the whitespace branch, preserving the prior control flow order and semantics.'

Calling tool: grepTool
char == '\\n'
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:125:                if char == '\n':
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:136:            elif char == '\n':

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char in ' \t':
135:                current_column += 1
136:            elif char == '\n':
137:                # unexpected empty line
138:                current_column = 0
139:            else:
140:                # indent is finished
141:                wait_for_nl = True
142:        return ''.join(lines), current_line
143:
144:
145:def _generate_pickle_name(gt, cache_dir=None):
146:    head, tail = os.path.splitext(gt)
147:    if tail == ".txt":
148:        tail = ""
149:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
150:    if cache_dir:
151:        return os.path.join(cache_dir, os.path.basename(name))
152:    else:
153:        return name
154:
155:
156:def load_grammar(gt="Grammar.txt", gp=None,
157:                 save=True, force=False, logger=None):
158:    """Load the grammar (maybe from a pickle)."""
159:    if logger is None:
160:        logger = logging.getLogger()
161:    gp = _generate_pickle_name(gt) if gp is None else gp
162:    if force or not _newer(gp, gt):
163:        logger.info("Generating grammar tables from %s", gt)
164:        g = pgen.generate_grammar(gt)
165:        if save:
166:            logger.info("Writing grammar tables to %s", gp)
167:            try:
168:                g.dump(gp)
169:            except OSError as e:

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: In blib2to3/pgen2/driver.py within _partially_consume_prefix, the newline branch `elif char == '\n': current_column = 0` remains present and follows the whitespace branch `elif char in ' \t': current_column += 1` (lines 134-139), preserving the original control flow and semantics.

Todo item 4 of 10 is correct
description: 'Verify that the newline handling `elif char == '\n': current_column = 0` remains intact and appears after the whitespace branch, preserving the prior control flow order and semantics.'

Validating todo item 5 of 10
description: 'Check that the surrounding logic resetting state (e.g., `current_line = ''`, `current_column = 0`, `wait_for_nl = False`) remains unchanged aside from the tab/space handling and still precedes the whitespace branch.'

Calling tool: grepTool
current_line\s*=\s*""|current_line\s*=\s*''
Grep command failed with exit code 2: rg: /Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/{{FOLDER_PATH}}: IO error for operation on /Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/{{FOLDER_PATH}}: No such file or directory (os error 2)


Calling tool: grepTool
current_line\s*=\s*""|current_line\s*=\s*''
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:119:        current_line = ""
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:131:                    current_line = ""

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
100:        tokens = tokenize.generate_tokens(stream.readline)
101:        return self.parse_tokens(tokens, debug)
102:
103:    def parse_stream(self, stream, debug=False):
104:        """Parse a stream and return the syntax tree."""
105:        return self.parse_stream_raw(stream, debug)
106:
107:    def parse_file(self, filename, encoding=None, debug=False):
108:        """Parse a file and return the syntax tree."""
109:        with io.open(filename, "r", encoding=encoding) as stream:
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char in ' \t':
135:                current_column += 1
136:            elif char == '\n':
137:                # unexpected empty line
138:                current_column = 0
139:            else:
140:                # indent is finished
141:                wait_for_nl = True
142:        return ''.join(lines), current_line
143:
144:
145:def _generate_pickle_name(gt, cache_dir=None):
146:    head, tail = os.path.splitext(gt)
147:    if tail == ".txt":
148:        tail = ""
149:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
150:    if cache_dir:
151:        return os.path.join(cache_dir, os.path.basename(name))
152:    else:
153:        return name
154:
155:
156:def load_grammar(gt="Grammar.txt", gp=None,
157:                 save=True, force=False, logger=None):
158:    """Load the grammar (maybe from a pickle)."""
159:    if logger is None:
160:        logger = logging.getLogger()
161:    gp = _generate_pickle_name(gt) if gp is None else gp
162:    if force or not _newer(gp, gt):
163:        logger.info("Generating grammar tables from %s", gt)
164:        g = pgen.generate_grammar(gt)
165:        if save:
166:            logger.info("Writing grammar tables to %s", gp)
167:            try:
168:                g.dump(gp)
169:            except OSError as e:
170:                logger.info("Writing failed: %s", e)
171:    else:
172:        g = grammar.Grammar()
173:        g.load(gp)
174:    return g
175:
176:
177:def _newer(a, b):
178:    """Inquire whether file a was written since file b."""
179:    if not os.path.exists(a):

Calling tool: grepTool
wait_for_nl
Matched content:
121:        wait_for_nl = False
124:            if wait_for_nl:
133:                    wait_for_nl = False
141:                wait_for_nl = True

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: In blib2to3/pgen2/driver.py:_partially_consume_prefix(), the state reset (current_line="", current_column=0, wait_for_nl=False) remains intact at lines 131-133 and occurs within the wait_for_nl branch before the whitespace handling. Only the whitespace handling changed to `elif char in ' \t': current_column += 1`, removing the separate tab branch, matching the diff.

Todo item 5 of 10 is correct
description: 'Check that the surrounding logic resetting state (e.g., `current_line = ''`, `current_column = 0`, `wait_for_nl = False`) remains unchanged aside from the tab/space handling and still precedes the whitespace branch.'

Validating todo item 6 of 10
description: 'Confirm that only space and tab characters advance `current_column` in this loop (i.e., no other whitespace characters like `\r` or `\f` are being incremented), aligning with the PR intent to match lib2to3's prefix consumption.'

Calling tool: grepTool
class Driver\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:30:class Driver(object):

Calling tool: grepTool
current_column
Matched content:
120:        current_column = 0
126:                    if current_line.strip() and current_column < column:
132:                    current_column = 0
135:                current_column += 1
138:                current_column = 0

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
100:        tokens = tokenize.generate_tokens(stream.readline)
101:        return self.parse_tokens(tokens, debug)
102:
103:    def parse_stream(self, stream, debug=False):
104:        """Parse a stream and return the syntax tree."""
105:        return self.parse_stream_raw(stream, debug)
106:
107:    def parse_file(self, filename, encoding=None, debug=False):
108:        """Parse a file and return the syntax tree."""
109:        with io.open(filename, "r", encoding=encoding) as stream:
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char in ' \t':
135:                current_column += 1
136:            elif char == '\n':
137:                # unexpected empty line
138:                current_column = 0
139:            else:
140:                # indent is finished
141:                wait_for_nl = True
142:        return ''.join(lines), current_line
143:
144:
145:def _generate_pickle_name(gt, cache_dir=None):
146:    head, tail = os.path.splitext(gt)
147:    if tail == ".txt":
148:        tail = ""
149:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
150:    if cache_dir:
151:        return os.path.join(cache_dir, os.path.basename(name))
152:    else:
153:        return name
154:
155:
156:def load_grammar(gt="Grammar.txt", gp=None,
157:                 save=True, force=False, logger=None):
158:    """Load the grammar (maybe from a pickle)."""
159:    if logger is None:
160:        logger = logging.getLogger()
161:    gp = _generate_pickle_name(gt) if gp is None else gp
162:    if force or not _newer(gp, gt):
163:        logger.info("Generating grammar tables from %s", gt)
164:        g = pgen.generate_grammar(gt)
165:        if save:
166:            logger.info("Writing grammar tables to %s", gp)
167:            try:
168:                g.dump(gp)
169:            except OSError as e:
170:                logger.info("Writing failed: %s", e)
171:    else:
172:        g = grammar.Grammar()
173:        g.load(gp)
174:    return g
175:
176:
177:def _newer(a, b):
178:    """Inquire whether file a was written since file b."""
179:    if not os.path.exists(a):

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: In _partially_consume_prefix (driver.py lines 117-142), only the branch `elif char in ' \t': current_column += 1` advances the column. Newlines reset the column and all other characters (including \r, \f, etc.) trigger the indent-finished branch without incrementing, matching the PR intent.

Todo item 6 of 10 is correct
description: 'Confirm that only space and tab characters advance `current_column` in this loop (i.e., no other whitespace characters like `\r` or `\f` are being incremented), aligning with the PR intent to match lib2to3's prefix consumption.'

Validating todo item 7 of 10
description: 'Locate the test `test_comment_indentation` and verify that its input uses tabs (not spaces) for indentation and includes a dedent followed by a comment line and then a code line at the same indentation level.'

Calling tool: grepTool
test_comment_indentation
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/bugsinpy_run_test.sh:1:python -m unittest -q tests.test_black.BlackTestCase.test_comment_indentation
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:512:    def test_comment_indentation(self) -> None:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py
500:        self.assertFormatEqual(expected, actual)
501:        black.assert_equivalent(source, actual)
502:        black.assert_stable(source, actual, line_length=ll)
503:
504:    @patch("black.dump_to_file", dump_to_stderr)
505:    def test_bracket_match(self) -> None:
506:        source, expected = read_data("bracketmatch")
507:        actual = fs(source)
508:        self.assertFormatEqual(expected, actual)
509:        black.assert_equivalent(source, actual)
510:        black.assert_stable(source, actual, line_length=ll)
511:
512:    def test_comment_indentation(self) -> None:
513:        contents_tab = "if 1:\n\tif 2:\n\t\tpass\n\t# comment\n\tpass\n"
514:        contents_spc = "if 1:\n    if 2:\n        pass\n    # comment\n    pass\n"
515:
516:        self.assertFormatEqual(fs(contents_spc), contents_spc)
517:        self.assertFormatEqual(fs(contents_tab), contents_spc)
518:
519:        contents_tab = "if 1:\n\tif 2:\n\t\tpass\n\t\t# comment\n\tpass\n"
520:        contents_spc = "if 1:\n    if 2:\n        pass\n        # comment\n    pass\n"
521:
522:        self.assertFormatEqual(fs(contents_tab), contents_spc)
523:        self.assertFormatEqual(fs(contents_spc), contents_spc)
524:
525:    def test_report_verbose(self) -> None:
526:        report = black.Report(verbose=True)
527:        out_lines = []
528:        err_lines = []
529:
530:        def out(msg: str, **kwargs: Any) -> None:
531:            out_lines.append(msg)
532:
533:        def err(msg: str, **kwargs: Any) -> None:
534:            err_lines.append(msg)
535:
536:        with patch("black.out", out), patch("black.err", err):
537:            report.done(Path("f1"), black.Changed.NO)
538:            self.assertEqual(len(out_lines), 1)
539:            self.assertEqual(len(err_lines), 0)
540:            self.assertEqual(out_lines[-1], "f1 already well formatted, good job.")
541:            self.assertEqual(unstyle(str(report)), "1 file left unchanged.")
542:            self.assertEqual(report.return_code, 0)
543:            report.done(Path("f2"), black.Changed.YES)
544:            self.assertEqual(len(out_lines), 2)
545:            self.assertEqual(len(err_lines), 0)
546:            self.assertEqual(out_lines[-1], "reformatted f2")
547:            self.assertEqual(
548:                unstyle(str(report)), "1 file reformatted, 1 file left unchanged."
549:            )
550:            report.done(Path("f3"), black.Changed.CACHED)
551:            self.assertEqual(len(out_lines), 3)
552:            self.assertEqual(len(err_lines), 0)
553:            self.assertEqual(
554:                out_lines[-1], "f3 wasn't modified on disk since last run."
555:            )
556:            self.assertEqual(
557:                unstyle(str(report)), "1 file reformatted, 2 files left unchanged."
558:            )
559:            self.assertEqual(report.return_code, 0)
560:            report.check = True
561:            self.assertEqual(report.return_code, 1)
562:            report.check = False
563:            report.failed(Path("e1"), "boom")
564:            self.assertEqual(len(out_lines), 3)
565:            self.assertEqual(len(err_lines), 1)
566:            self.assertEqual(err_lines[-1], "error: cannot format e1: boom")
567:            self.assertEqual(
568:                unstyle(str(report)),
569:                "1 file reformatted, 2 files left unchanged, "
570:                "1 file failed to reformat.",
571:            )
572:            self.assertEqual(report.return_code, 123)
573:            report.done(Path("f3"), black.Changed.YES)
574:            self.assertEqual(len(out_lines), 4)
575:            self.assertEqual(len(err_lines), 1)
576:            self.assertEqual(out_lines[-1], "reformatted f3")
577:            self.assertEqual(
578:                unstyle(str(report)),
579:                "2 files reformatted, 2 files left unchanged, "

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: Located test in tests/test_black.py lines 512-523. The first contents_tab string uses literal tabs (\t) and shows a dedent from level 2 to 1 followed by a comment line and then a code line (pass) at the same indentation level.

Todo item 7 of 10 is correct
description: 'Locate the test `test_comment_indentation` and verify that its input uses tabs (not spaces) for indentation and includes a dedent followed by a comment line and then a code line at the same indentation level.'

Validating todo item 8 of 10
description: 'Verify that `test_comment_indentation` asserts that the comment's indentation matches the indentation of the subsequent code line, reflecting the PR's acceptance criteria.'

Calling tool: grepTool
test_comment_indentation
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/bugsinpy_run_test.sh:1:python -m unittest -q tests.test_black.BlackTestCase.test_comment_indentation
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:512:    def test_comment_indentation(self) -> None:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py
480:    @patch("black.dump_to_file", dump_to_stderr)
481:    def test_fmtonoff2(self) -> None:
482:        source, expected = read_data("fmtonoff2")
483:        actual = fs(source)
484:        self.assertFormatEqual(expected, actual)
485:        black.assert_equivalent(source, actual)
486:        black.assert_stable(source, actual, line_length=ll)
487:
488:    @patch("black.dump_to_file", dump_to_stderr)
489:    def test_remove_empty_parentheses_after_class(self) -> None:
490:        source, expected = read_data("class_blank_parentheses")
491:        actual = fs(source)
492:        self.assertFormatEqual(expected, actual)
493:        black.assert_equivalent(source, actual)
494:        black.assert_stable(source, actual, line_length=ll)
495:
496:    @patch("black.dump_to_file", dump_to_stderr)
497:    def test_new_line_between_class_and_code(self) -> None:
498:        source, expected = read_data("class_methods_new_line")
499:        actual = fs(source)
500:        self.assertFormatEqual(expected, actual)
501:        black.assert_equivalent(source, actual)
502:        black.assert_stable(source, actual, line_length=ll)
503:
504:    @patch("black.dump_to_file", dump_to_stderr)
505:    def test_bracket_match(self) -> None:
506:        source, expected = read_data("bracketmatch")
507:        actual = fs(source)
508:        self.assertFormatEqual(expected, actual)
509:        black.assert_equivalent(source, actual)
510:        black.assert_stable(source, actual, line_length=ll)
511:
512:    def test_comment_indentation(self) -> None:
513:        contents_tab = "if 1:\n\tif 2:\n\t\tpass\n\t# comment\n\tpass\n"
514:        contents_spc = "if 1:\n    if 2:\n        pass\n    # comment\n    pass\n"
515:
516:        self.assertFormatEqual(fs(contents_spc), contents_spc)
517:        self.assertFormatEqual(fs(contents_tab), contents_spc)
518:
519:        contents_tab = "if 1:\n\tif 2:\n\t\tpass\n\t\t# comment\n\tpass\n"
520:        contents_spc = "if 1:\n    if 2:\n        pass\n        # comment\n    pass\n"
521:
522:        self.assertFormatEqual(fs(contents_tab), contents_spc)
523:        self.assertFormatEqual(fs(contents_spc), contents_spc)
524:
525:    def test_report_verbose(self) -> None:
526:        report = black.Report(verbose=True)
527:        out_lines = []
528:        err_lines = []
529:
530:        def out(msg: str, **kwargs: Any) -> None:
531:            out_lines.append(msg)
532:
533:        def err(msg: str, **kwargs: Any) -> None:
534:            err_lines.append(msg)
535:
536:        with patch("black.out", out), patch("black.err", err):
537:            report.done(Path("f1"), black.Changed.NO)
538:            self.assertEqual(len(out_lines), 1)
539:            self.assertEqual(len(err_lines), 0)
540:            self.assertEqual(out_lines[-1], "f1 already well formatted, good job.")
541:            self.assertEqual(unstyle(str(report)), "1 file left unchanged.")
542:            self.assertEqual(report.return_code, 0)
543:            report.done(Path("f2"), black.Changed.YES)
544:            self.assertEqual(len(out_lines), 2)
545:            self.assertEqual(len(err_lines), 0)
546:            self.assertEqual(out_lines[-1], "reformatted f2")
547:            self.assertEqual(
548:                unstyle(str(report)), "1 file reformatted, 1 file left unchanged."
549:            )
550:            report.done(Path("f3"), black.Changed.CACHED)
551:            self.assertEqual(len(out_lines), 3)
552:            self.assertEqual(len(err_lines), 0)
553:            self.assertEqual(
554:                out_lines[-1], "f3 wasn't modified on disk since last run."
555:            )
556:            self.assertEqual(
557:                unstyle(str(report)), "1 file reformatted, 2 files left unchanged."
558:            )
559:            self.assertEqual(report.return_code, 0)
560:            report.check = True
561:            self.assertEqual(report.return_code, 1)
562:            report.check = False
563:            report.failed(Path("e1"), "boom")
564:            self.assertEqual(len(out_lines), 3)
565:            self.assertEqual(len(err_lines), 1)
566:            self.assertEqual(err_lines[-1], "error: cannot format e1: boom")
567:            self.assertEqual(
568:                unstyle(str(report)),
569:                "1 file reformatted, 2 files left unchanged, "
570:                "1 file failed to reformat.",
571:            )
572:            self.assertEqual(report.return_code, 123)
573:            report.done(Path("f3"), black.Changed.YES)
574:            self.assertEqual(len(out_lines), 4)
575:            self.assertEqual(len(err_lines), 1)
576:            self.assertEqual(out_lines[-1], "reformatted f3")
577:            self.assertEqual(
578:                unstyle(str(report)),
579:                "2 files reformatted, 2 files left unchanged, "
580:                "1 file failed to reformat.",
581:            )
582:            self.assertEqual(report.return_code, 123)
583:            report.failed(Path("e2"), "boom")
584:            self.assertEqual(len(out_lines), 4)
585:            self.assertEqual(len(err_lines), 2)
586:            self.assertEqual(err_lines[-1], "error: cannot format e2: boom")
587:            self.assertEqual(
588:                unstyle(str(report)),
589:                "2 files reformatted, 2 files left unchanged, "
590:                "2 files failed to reformat.",
591:            )
592:            self.assertEqual(report.return_code, 123)
593:            report.path_ignored(Path("wat"), "no match")
594:            self.assertEqual(len(out_lines), 5)
595:            self.assertEqual(len(err_lines), 2)
596:            self.assertEqual(out_lines[-1], "wat ignored: no match")
597:            self.assertEqual(
598:                unstyle(str(report)),
599:                "2 files reformatted, 2 files left unchanged, "

Calling tool: grepTool
class Driver\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:30:class Driver(object):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Modifications:
5:# Copyright 2006 Google, Inc. All Rights Reserved.
6:# Licensed to PSF under a Contributor Agreement.
7:
8:"""Parser driver.
9:
10:This provides a high-level interface to parse a file into a syntax tree.
11:
12:"""
13:
14:__author__ = "Guido van Rossum <guido@python.org>"
15:
16:__all__ = ["Driver", "load_grammar"]
17:
18:# Python imports
19:import codecs
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(self, grammar, convert=None, logger=None):
33:        self.grammar = grammar
34:        if logger is None:
35:            logger = logging.getLogger()
36:        self.logger = logger
37:        self.convert = convert
38:
39:    def parse_tokens(self, tokens, debug=False):
40:        """Parse a series of tokens and return the syntax tree."""
41:        # XXX Move the prefix computation into a wrapper around tokenize.
42:        p = parse.Parser(self.grammar, self.convert)
43:        p.setup()
44:        lineno = 1
45:        column = 0
46:        indent_columns = []
47:        type = value = start = end = line_text = None
48:        prefix = ""
49:        for quintuple in tokens:
50:            type, value, start, end, line_text = quintuple
51:            if start != (lineno, column):
52:                assert (lineno, column) <= start, ((lineno, column), start)
53:                s_lineno, s_column = start
54:                if lineno < s_lineno:
55:                    prefix += "\n" * (s_lineno - lineno)
56:                    lineno = s_lineno
57:                    column = 0
58:                if column < s_column:
59:                    prefix += line_text[column:s_column]
60:                    column = s_column
61:            if type in (tokenize.COMMENT, tokenize.NL):
62:                prefix += value
63:                lineno, column = end
64:                if value.endswith("\n"):
65:                    lineno += 1
66:                    column = 0
67:                continue
68:            if type == token.OP:
69:                type = grammar.opmap[value]
70:            if debug:
71:                self.logger.debug("%s %r (prefix=%r)",
72:                                  token.tok_name[type], value, prefix)
73:            if type == token.INDENT:
74:                indent_columns.append(len(value))
75:                _prefix = prefix + value
76:                prefix = ""
77:                value = ""
78:            elif type == token.DEDENT:
79:                _indent_col = indent_columns.pop()
80:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
81:            if p.addtoken(type, value, (prefix, start)):
82:                if debug:
83:                    self.logger.debug("Stop.")
84:                break
85:            prefix = ""
86:            if type in {token.INDENT, token.DEDENT}:
87:                prefix = _prefix
88:            lineno, column = end
89:            if value.endswith("\n"):
90:                lineno += 1
91:                column = 0
92:        else:
93:            # We never broke out -- EOF is too soon (how can this happen???)
94:            raise parse.ParseError("incomplete input",
95:                                   type, value, (prefix, start))
96:        return p.rootnode
97:
98:    def parse_stream_raw(self, stream, debug=False):
99:        """Parse a stream and return the syntax tree."""
100:        tokens = tokenize.generate_tokens(stream.readline)
101:        return self.parse_tokens(tokens, debug)
102:
103:    def parse_stream(self, stream, debug=False):
104:        """Parse a stream and return the syntax tree."""
105:        return self.parse_stream_raw(stream, debug)
106:
107:    def parse_file(self, filename, encoding=None, debug=False):
108:        """Parse a file and return the syntax tree."""
109:        with io.open(filename, "r", encoding=encoding) as stream:
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char in ' \t':
135:                current_column += 1
136:            elif char == '\n':
137:                # unexpected empty line
138:                current_column = 0
139:            else:
140:                # indent is finished
141:                wait_for_nl = True
142:        return ''.join(lines), current_line
143:
144:
145:def _generate_pickle_name(gt, cache_dir=None):
146:    head, tail = os.path.splitext(gt)
147:    if tail == ".txt":
148:        tail = ""
149:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
150:    if cache_dir:
151:        return os.path.join(cache_dir, os.path.basename(name))
152:    else:
153:        return name
154:
155:
156:def load_grammar(gt="Grammar.txt", gp=None,
157:                 save=True, force=False, logger=None):
158:    """Load the grammar (maybe from a pickle)."""
159:    if logger is None:
160:        logger = logging.getLogger()
161:    gp = _generate_pickle_name(gt) if gp is None else gp
162:    if force or not _newer(gp, gt):
163:        logger.info("Generating grammar tables from %s", gt)
164:        g = pgen.generate_grammar(gt)
165:        if save:
166:            logger.info("Writing grammar tables to %s", gp)
167:            try:
168:                g.dump(gp)
169:            except OSError as e:
170:                logger.info("Writing failed: %s", e)
171:    else:
172:        g = grammar.Grammar()
173:        g.load(gp)
174:    return g
175:
176:
177:def _newer(a, b):
178:    """Inquire whether file a was written since file b."""
179:    if not os.path.exists(a):
180:        return False
181:    if not os.path.exists(b):
182:        return True
183:    return os.path.getmtime(a) >= os.path.getmtime(b)
184:
185:
186:def load_packaged_grammar(package, grammar_source, cache_dir=None):
187:    """Normally, loads a pickled grammar by doing
188:        pkgutil.get_data(package, pickled_grammar)
189:    where *pickled_grammar* is computed from *grammar_source* by adding the
190:    Python version and using a ``.pickle`` extension.
191:
192:    However, if *grammar_source* is an extant file, load_grammar(grammar_source)
193:    is called instead. This facilitates using a packaged grammar file when needed
194:    but preserves load_grammar's automatic regeneration behavior when possible.
195:
196:    """
197:    if os.path.isfile(grammar_source):
198:        gp = _generate_pickle_name(grammar_source, cache_dir) if cache_dir else None
199:        return load_grammar(grammar_source, gp=gp)
200:    pickled_name = _generate_pickle_name(os.path.basename(grammar_source), cache_dir)
201:    data = pkgutil.get_data(package, pickled_name)
202:    g = grammar.Grammar()
203:    g.loads(data)
204:    return g
205:
206:
207:def main(*args):
208:    """Main program, when run as a script: produce grammar pickle files.
209:
210:    Calls load_grammar for each argument, a path to a grammar text file.
211:    """
212:    if not args:
213:        args = sys.argv[1:]
214:    logging.basicConfig(level=logging.INFO, stream=sys.stdout,
215:                        format='%(message)s')
216:    for gt in args:
217:        load_grammar(gt, save=True, force=True)
218:    return True
219:
220:if __name__ == "__main__":

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: Found test at tests/test_black.py:512-523. The first case asserts that a tab-indented comment after a dedent formats to match the indentation of the subsequent code line (expected space-indented form). The second case ensures comments inside the block retain inner indentation. This matches the PRs acceptance criteria.

Todo item 8 of 10 is correct
description: 'Verify that `test_comment_indentation` asserts that the comment's indentation matches the indentation of the subsequent code line, reflecting the PR's acceptance criteria.'

Validating todo item 9 of 10
description: 'Ensure `test_comment_indentation` executes the code path that computes comment/indent prefixes via `blib2to3/pgen2/driver.py` (e.g., by running the formatter/parser that relies on this driver) rather than bypassing it.'

Calling tool: grepTool
test_comment_indentation
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/bugsinpy_run_test.sh:1:python -m unittest -q tests.test_black.BlackTestCase.test_comment_indentation
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:512:    def test_comment_indentation(self) -> None:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py
480:    @patch("black.dump_to_file", dump_to_stderr)
481:    def test_fmtonoff2(self) -> None:
482:        source, expected = read_data("fmtonoff2")
483:        actual = fs(source)
484:        self.assertFormatEqual(expected, actual)
485:        black.assert_equivalent(source, actual)
486:        black.assert_stable(source, actual, line_length=ll)
487:
488:    @patch("black.dump_to_file", dump_to_stderr)
489:    def test_remove_empty_parentheses_after_class(self) -> None:
490:        source, expected = read_data("class_blank_parentheses")
491:        actual = fs(source)
492:        self.assertFormatEqual(expected, actual)
493:        black.assert_equivalent(source, actual)
494:        black.assert_stable(source, actual, line_length=ll)
495:
496:    @patch("black.dump_to_file", dump_to_stderr)
497:    def test_new_line_between_class_and_code(self) -> None:
498:        source, expected = read_data("class_methods_new_line")
499:        actual = fs(source)
500:        self.assertFormatEqual(expected, actual)
501:        black.assert_equivalent(source, actual)
502:        black.assert_stable(source, actual, line_length=ll)
503:
504:    @patch("black.dump_to_file", dump_to_stderr)
505:    def test_bracket_match(self) -> None:
506:        source, expected = read_data("bracketmatch")
507:        actual = fs(source)
508:        self.assertFormatEqual(expected, actual)
509:        black.assert_equivalent(source, actual)
510:        black.assert_stable(source, actual, line_length=ll)
511:
512:    def test_comment_indentation(self) -> None:
513:        contents_tab = "if 1:\n\tif 2:\n\t\tpass\n\t# comment\n\tpass\n"
514:        contents_spc = "if 1:\n    if 2:\n        pass\n    # comment\n    pass\n"
515:
516:        self.assertFormatEqual(fs(contents_spc), contents_spc)
517:        self.assertFormatEqual(fs(contents_tab), contents_spc)
518:
519:        contents_tab = "if 1:\n\tif 2:\n\t\tpass\n\t\t# comment\n\tpass\n"
520:        contents_spc = "if 1:\n    if 2:\n        pass\n        # comment\n    pass\n"
521:
522:        self.assertFormatEqual(fs(contents_tab), contents_spc)
523:        self.assertFormatEqual(fs(contents_spc), contents_spc)
524:
525:    def test_report_verbose(self) -> None:
526:        report = black.Report(verbose=True)
527:        out_lines = []
528:        err_lines = []
529:
530:        def out(msg: str, **kwargs: Any) -> None:
531:            out_lines.append(msg)
532:
533:        def err(msg: str, **kwargs: Any) -> None:
534:            err_lines.append(msg)
535:
536:        with patch("black.out", out), patch("black.err", err):
537:            report.done(Path("f1"), black.Changed.NO)
538:            self.assertEqual(len(out_lines), 1)
539:            self.assertEqual(len(err_lines), 0)
540:            self.assertEqual(out_lines[-1], "f1 already well formatted, good job.")
541:            self.assertEqual(unstyle(str(report)), "1 file left unchanged.")
542:            self.assertEqual(report.return_code, 0)
543:            report.done(Path("f2"), black.Changed.YES)
544:            self.assertEqual(len(out_lines), 2)
545:            self.assertEqual(len(err_lines), 0)
546:            self.assertEqual(out_lines[-1], "reformatted f2")
547:            self.assertEqual(
548:                unstyle(str(report)), "1 file reformatted, 1 file left unchanged."
549:            )
550:            report.done(Path("f3"), black.Changed.CACHED)
551:            self.assertEqual(len(out_lines), 3)
552:            self.assertEqual(len(err_lines), 0)
553:            self.assertEqual(
554:                out_lines[-1], "f3 wasn't modified on disk since last run."
555:            )
556:            self.assertEqual(
557:                unstyle(str(report)), "1 file reformatted, 2 files left unchanged."
558:            )
559:            self.assertEqual(report.return_code, 0)
560:            report.check = True
561:            self.assertEqual(report.return_code, 1)
562:            report.check = False
563:            report.failed(Path("e1"), "boom")
564:            self.assertEqual(len(out_lines), 3)
565:            self.assertEqual(len(err_lines), 1)
566:            self.assertEqual(err_lines[-1], "error: cannot format e1: boom")
567:            self.assertEqual(
568:                unstyle(str(report)),
569:                "1 file reformatted, 2 files left unchanged, "
570:                "1 file failed to reformat.",
571:            )
572:            self.assertEqual(report.return_code, 123)
573:            report.done(Path("f3"), black.Changed.YES)
574:            self.assertEqual(len(out_lines), 4)
575:            self.assertEqual(len(err_lines), 1)
576:            self.assertEqual(out_lines[-1], "reformatted f3")
577:            self.assertEqual(
578:                unstyle(str(report)),
579:                "2 files reformatted, 2 files left unchanged, "
580:                "1 file failed to reformat.",
581:            )
582:            self.assertEqual(report.return_code, 123)
583:            report.failed(Path("e2"), "boom")
584:            self.assertEqual(len(out_lines), 4)
585:            self.assertEqual(len(err_lines), 2)
586:            self.assertEqual(err_lines[-1], "error: cannot format e2: boom")
587:            self.assertEqual(
588:                unstyle(str(report)),
589:                "2 files reformatted, 2 files left unchanged, "
590:                "2 files failed to reformat.",
591:            )
592:            self.assertEqual(report.return_code, 123)
593:            report.path_ignored(Path("wat"), "no match")
594:            self.assertEqual(len(out_lines), 5)
595:            self.assertEqual(len(err_lines), 2)
596:            self.assertEqual(out_lines[-1], "wat ignored: no match")
597:            self.assertEqual(
598:                unstyle(str(report)),
599:                "2 files reformatted, 2 files left unchanged, "

Calling tool: grepTool
def fs\(
Grep command failed with exit code 2: rg: /Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/**/tests/test_black.py: IO error for operation on /Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/**/tests/test_black.py: No such file or directory (os error 2)


Calling tool: grepTool
def fs\(
No files found. Call grepTool again with a more generic pattern.

Calling tool: grepTool
\bfs\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:161:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:180:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:189:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:227:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:236:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:244:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:252:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:299:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:307:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:312:        not_normalized = fs(source, mode=mode)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:320:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:328:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:336:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:344:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:352:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:360:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:368:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:376:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:384:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:392:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:400:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:408:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:416:        actual = fs(source, mode=black.FileMode.PYTHON36)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:427:        actual = fs(source, mode=mode)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:435:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:442:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:450:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:458:        actual = fs(source, mode=mode)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:465:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:475:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:483:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:491:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:499:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:507:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:516:        self.assertFormatEqual(fs(contents_spc), contents_spc)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:517:        self.assertFormatEqual(fs(contents_tab), contents_spc)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:522:        self.assertFormatEqual(fs(contents_tab), contents_spc)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:523:        self.assertFormatEqual(fs(contents_spc), contents_spc)

Calling tool: grepTool
\bfs\b\s*=|def fs\b|from .* import fs|as fs
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:42:fs = partial(black.format_str, line_length=ll)

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py
1:#!/usr/bin/env python3
2:import asyncio
3:from concurrent.futures import ThreadPoolExecutor
4:from contextlib import contextmanager, redirect_stderr
5:from functools import partial, wraps
6:from io import BytesIO, TextIOWrapper
7:import os
8:from pathlib import Path
9:import re
10:import sys
11:from tempfile import TemporaryDirectory
12:from typing import (
13:    Any,
14:    BinaryIO,
15:    Callable,
16:    Coroutine,
17:    Generator,
18:    List,
19:    Tuple,
20:    Iterator,
21:    TypeVar,
22:)
23:import unittest
24:from unittest.mock import patch, MagicMock
25:
26:from click import unstyle
27:from click.testing import CliRunner
28:
29:import black
30:
31:try:
32:    import blackd
33:    from aiohttp.test_utils import TestClient, TestServer
34:except ImportError:
35:    has_blackd_deps = False
36:else:
37:    has_blackd_deps = True
38:
39:
40:ll = 88
41:ff = partial(black.format_file_in_place, line_length=ll, fast=True)
42:fs = partial(black.format_str, line_length=ll)
43:THIS_FILE = Path(__file__)
44:THIS_DIR = THIS_FILE.parent
45:EMPTY_LINE = "# EMPTY LINE WITH WHITESPACE" + " (this comment will be removed)"
46:T = TypeVar("T")
47:R = TypeVar("R")
48:
49:
50:def dump_to_stderr(*output: str) -> str:
51:    return "\n" + "\n".join(output) + "\n"
52:
53:
54:def read_data(name: str, data: bool = True) -> Tuple[str, str]:
55:    """read_data('test_name') -> 'input', 'output'"""
56:    if not name.endswith((".py", ".pyi", ".out", ".diff")):
57:        name += ".py"
58:    _input: List[str] = []
59:    _output: List[str] = []
60:    base_dir = THIS_DIR / "data" if data else THIS_DIR
61:    with open(base_dir / name, "r", encoding="utf8") as test:
62:        lines = test.readlines()
63:    result = _input
64:    for line in lines:
65:        line = line.replace(EMPTY_LINE, "")
66:        if line.rstrip() == "# output":
67:            result = _output
68:            continue
69:
70:        result.append(line)
71:    if _input and not _output:
72:        # If there's no output marker, treat the entire file as already pre-formatted.
73:        _output = _input[:]
74:    return "".join(_input).strip() + "\n", "".join(_output).strip() + "\n"
75:
76:
77:@contextmanager
78:def cache_dir(exists: bool = True) -> Iterator[Path]:
79:    with TemporaryDirectory() as workspace:
80:        cache_dir = Path(workspace)

Calling tool: grepTool
def format_str\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black.py:624:def format_str(

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black.py
600:    *,
601:    line_length: int,
602:    fast: bool,
603:    mode: FileMode = FileMode.AUTO_DETECT,
604:) -> FileContent:
605:    """Reformat contents a file and return new contents.
606:
607:    If `fast` is False, additionally confirm that the reformatted code is
608:    valid by calling :func:`assert_equivalent` and :func:`assert_stable` on it.
609:    `line_length` is passed to :func:`format_str`.
610:    """
611:    if src_contents.strip() == "":
612:        raise NothingChanged
613:
614:    dst_contents = format_str(src_contents, line_length=line_length, mode=mode)
615:    if src_contents == dst_contents:
616:        raise NothingChanged
617:
618:    if not fast:
619:        assert_equivalent(src_contents, dst_contents)
620:        assert_stable(src_contents, dst_contents, line_length=line_length, mode=mode)
621:    return dst_contents
622:
623:
624:def format_str(
625:    src_contents: str, line_length: int, *, mode: FileMode = FileMode.AUTO_DETECT
626:) -> FileContent:
627:    """Reformat a string and return new contents.
628:
629:    `line_length` determines how many characters per line are allowed.
630:    """
631:    src_node = lib2to3_parse(src_contents.lstrip())
632:    dst_contents = ""
633:    future_imports = get_future_imports(src_node)
634:    is_pyi = bool(mode & FileMode.PYI)
635:    py36 = bool(mode & FileMode.PYTHON36) or is_python36(src_node)
636:    normalize_strings = not bool(mode & FileMode.NO_STRING_NORMALIZATION)
637:    normalize_fmt_off(src_node)
638:    lines = LineGenerator(
639:        remove_u_prefix=py36 or "unicode_literals" in future_imports,
640:        is_pyi=is_pyi,
641:        normalize_strings=normalize_strings,
642:        allow_underscores=py36
643:        and not bool(mode & FileMode.NO_NUMERIC_UNDERSCORE_NORMALIZATION),
644:    )
645:    elt = EmptyLineTracker(is_pyi=is_pyi)
646:    empty_line = Line()
647:    after = 0
648:    for current_line in lines.visit(src_node):
649:        for _ in range(after):
650:            dst_contents += str(empty_line)
651:        before, after = elt.maybe_empty_lines(current_line)
652:        for _ in range(before):
653:            dst_contents += str(empty_line)
654:        for line in split_line(current_line, line_length=line_length, py36=py36):
655:            dst_contents += str(line)
656:    return dst_contents
657:
658:
659:def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:
660:    """Return a tuple of (decoded_contents, encoding, newline).
661:
662:    `newline` is either CRLF or LF but `decoded_contents` is decoded with
663:    universal newlines (i.e. only contains LF).
664:    """
665:    srcbuf = io.BytesIO(src)
666:    encoding, lines = tokenize.detect_encoding(srcbuf.readline)
667:    if not lines:
668:        return "", encoding, "\n"
669:
670:    newline = "\r\n" if b"\r\n" == lines[0][-2:] else "\n"
671:    srcbuf.seek(0)
672:    with io.TextIOWrapper(srcbuf, encoding) as tiow:
673:        return tiow.read(), encoding, newline
674:
675:
676:GRAMMARS = [
677:    pygram.python_grammar_no_print_statement_no_exec_statement,
678:    pygram.python_grammar_no_print_statement,
679:    pygram.python_grammar,
680:]
681:
682:
683:def lib2to3_parse(src_txt: str) -> Node:
684:    """Given a string with source, return the lib2to3 Node."""
685:    if src_txt[-1:] != "\n":
686:        src_txt += "\n"
687:    for grammar in GRAMMARS:
688:        drv = driver.Driver(grammar, pytree.convert)
689:        try:
690:            result = drv.parse_string(src_txt, True)
691:            break
692:
693:        except ParseError as pe:
694:            lineno, column = pe.context[1]
695:            lines = src_txt.splitlines()
696:            try:
697:                faulty_line = lines[lineno - 1]
698:            except IndexError:
699:                faulty_line = "<line number missing in source>"
700:            exc = InvalidInput(f"Cannot parse: {lineno}:{column}: {faulty_line}")
701:    else:
702:        raise exc from None
703:
704:    if isinstance(result, Leaf):
705:        result = Node(syms.file_input, [result])
706:    return result
707:
708:
709:def lib2to3_unparse(node: Node) -> str:
710:    """Given a lib2to3 node, return its string representation."""
711:    code = str(node)
712:    return code
713:
714:
715:T = TypeVar("T")
716:
717:
718:class Visitor(Generic[T]):
719:    """Basic lib2to3 visitor that yields things of type `T` on `visit()`."""

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Modifications:
5:# Copyright 2006 Google, Inc. All Rights Reserved.
6:# Licensed to PSF under a Contributor Agreement.
7:
8:"""Parser driver.
9:
10:This provides a high-level interface to parse a file into a syntax tree.
11:
12:"""
13:
14:__author__ = "Guido van Rossum <guido@python.org>"
15:
16:__all__ = ["Driver", "load_grammar"]
17:
18:# Python imports
19:import codecs
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(self, grammar, convert=None, logger=None):
33:        self.grammar = grammar
34:        if logger is None:
35:            logger = logging.getLogger()
36:        self.logger = logger
37:        self.convert = convert
38:
39:    def parse_tokens(self, tokens, debug=False):
40:        """Parse a series of tokens and return the syntax tree."""
41:        # XXX Move the prefix computation into a wrapper around tokenize.
42:        p = parse.Parser(self.grammar, self.convert)
43:        p.setup()
44:        lineno = 1
45:        column = 0
46:        indent_columns = []
47:        type = value = start = end = line_text = None
48:        prefix = ""
49:        for quintuple in tokens:
50:            type, value, start, end, line_text = quintuple
51:            if start != (lineno, column):
52:                assert (lineno, column) <= start, ((lineno, column), start)
53:                s_lineno, s_column = start
54:                if lineno < s_lineno:
55:                    prefix += "\n" * (s_lineno - lineno)
56:                    lineno = s_lineno
57:                    column = 0
58:                if column < s_column:
59:                    prefix += line_text[column:s_column]
60:                    column = s_column
61:            if type in (tokenize.COMMENT, tokenize.NL):
62:                prefix += value
63:                lineno, column = end
64:                if value.endswith("\n"):
65:                    lineno += 1
66:                    column = 0
67:                continue
68:            if type == token.OP:
69:                type = grammar.opmap[value]
70:            if debug:
71:                self.logger.debug("%s %r (prefix=%r)",
72:                                  token.tok_name[type], value, prefix)
73:            if type == token.INDENT:
74:                indent_columns.append(len(value))
75:                _prefix = prefix + value
76:                prefix = ""
77:                value = ""
78:            elif type == token.DEDENT:
79:                _indent_col = indent_columns.pop()
80:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
81:            if p.addtoken(type, value, (prefix, start)):
82:                if debug:
83:                    self.logger.debug("Stop.")
84:                break
85:            prefix = ""
86:            if type in {token.INDENT, token.DEDENT}:
87:                prefix = _prefix
88:            lineno, column = end
89:            if value.endswith("\n"):
90:                lineno += 1
91:                column = 0
92:        else:
93:            # We never broke out -- EOF is too soon (how can this happen???)
94:            raise parse.ParseError("incomplete input",
95:                                   type, value, (prefix, start))
96:        return p.rootnode
97:
98:    def parse_stream_raw(self, stream, debug=False):
99:        """Parse a stream and return the syntax tree."""
100:        tokens = tokenize.generate_tokens(stream.readline)
101:        return self.parse_tokens(tokens, debug)
102:
103:    def parse_stream(self, stream, debug=False):
104:        """Parse a stream and return the syntax tree."""
105:        return self.parse_stream_raw(stream, debug)
106:
107:    def parse_file(self, filename, encoding=None, debug=False):
108:        """Parse a file and return the syntax tree."""
109:        with io.open(filename, "r", encoding=encoding) as stream:
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char in ' \t':
135:                current_column += 1
136:            elif char == '\n':
137:                # unexpected empty line
138:                current_column = 0
139:            else:
140:                # indent is finished
141:                wait_for_nl = True
142:        return ''.join(lines), current_line
143:
144:
145:def _generate_pickle_name(gt, cache_dir=None):
146:    head, tail = os.path.splitext(gt)
147:    if tail == ".txt":
148:        tail = ""
149:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
150:    if cache_dir:
151:        return os.path.join(cache_dir, os.path.basename(name))
152:    else:
153:        return name
154:
155:
156:def load_grammar(gt="Grammar.txt", gp=None,
157:                 save=True, force=False, logger=None):
158:    """Load the grammar (maybe from a pickle)."""
159:    if logger is None:
160:        logger = logging.getLogger()
161:    gp = _generate_pickle_name(gt) if gp is None else gp
162:    if force or not _newer(gp, gt):
163:        logger.info("Generating grammar tables from %s", gt)
164:        g = pgen.generate_grammar(gt)
165:        if save:
166:            logger.info("Writing grammar tables to %s", gp)
167:            try:
168:                g.dump(gp)
169:            except OSError as e:
170:                logger.info("Writing failed: %s", e)
171:    else:
172:        g = grammar.Grammar()
173:        g.load(gp)
174:    return g
175:
176:
177:def _newer(a, b):
178:    """Inquire whether file a was written since file b."""
179:    if not os.path.exists(a):
180:        return False
181:    if not os.path.exists(b):
182:        return True
183:    return os.path.getmtime(a) >= os.path.getmtime(b)
184:
185:
186:def load_packaged_grammar(package, grammar_source, cache_dir=None):
187:    """Normally, loads a pickled grammar by doing
188:        pkgutil.get_data(package, pickled_grammar)
189:    where *pickled_grammar* is computed from *grammar_source* by adding the
190:    Python version and using a ``.pickle`` extension.
191:
192:    However, if *grammar_source* is an extant file, load_grammar(grammar_source)
193:    is called instead. This facilitates using a packaged grammar file when needed
194:    but preserves load_grammar's automatic regeneration behavior when possible.
195:
196:    """
197:    if os.path.isfile(grammar_source):
198:        gp = _generate_pickle_name(grammar_source, cache_dir) if cache_dir else None
199:        return load_grammar(grammar_source, gp=gp)
200:    pickled_name = _generate_pickle_name(os.path.basename(grammar_source), cache_dir)

Calling tool: grepTool
from blib2to3
Matched content:
45:from blib2to3.pytree import Node, Leaf, type_repr
46:from blib2to3 import pygram, pytree
47:from blib2to3.pgen2 import driver, token
48:from blib2to3.pgen2.parse import ParseError

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black.py
1:import asyncio
2:from asyncio.base_events import BaseEventLoop
3:from concurrent.futures import Executor, ProcessPoolExecutor
4:from datetime import datetime
5:from enum import Enum, Flag
6:from functools import lru_cache, partial, wraps
7:import io
8:import itertools
9:import logging
10:from multiprocessing import Manager, freeze_support
11:import os
12:from pathlib import Path
13:import pickle
14:import re
15:import signal
16:import sys
17:import tempfile
18:import tokenize
19:from typing import (
20:    Any,
21:    Callable,
22:    Collection,
23:    Dict,
24:    Generator,
25:    Generic,
26:    Iterable,
27:    Iterator,
28:    List,
29:    Optional,
30:    Pattern,
31:    Sequence,
32:    Set,
33:    Tuple,
34:    TypeVar,
35:    Union,
36:    cast,
37:)
38:
39:from appdirs import user_cache_dir
40:from attr import dataclass, Factory
41:import click
42:import toml
43:
44:# lib2to3 fork
45:from blib2to3.pytree import Node, Leaf, type_repr
46:from blib2to3 import pygram, pytree
47:from blib2to3.pgen2 import driver, token
48:from blib2to3.pgen2.parse import ParseError
49:
50:
51:__version__ = "18.9b0"
52:DEFAULT_LINE_LENGTH = 88
53:DEFAULT_EXCLUDES = (
54:    r"/(\.eggs|\.git|\.hg|\.mypy_cache|\.nox|\.tox|\.venv|_build|buck-out|build|dist)/"
55:)
56:DEFAULT_INCLUDES = r"\.pyi?$"
57:CACHE_DIR = Path(user_cache_dir("black", version=__version__))
58:
59:
60:# types
61:FileContent = str
62:Encoding = str
63:NewLine = str
64:Depth = int
65:NodeType = int
66:LeafID = int
67:Priority = int
68:Index = int
69:LN = Union[Leaf, Node]
70:SplitFunc = Callable[["Line", bool], Iterator["Line"]]
71:Timestamp = float
72:FileSize = int
73:CacheInfo = Tuple[Timestamp, FileSize]
74:Cache = Dict[Path, CacheInfo]
75:out = partial(click.secho, bold=True, err=True)
76:err = partial(click.secho, fg="red", err=True)
77:
78:pygram.initialize(CACHE_DIR)
79:syms = pygram.python_symbols
80:

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: The test uses fs = partial(black.format_str, ) and calls fs(contents_tab). format_str calls lib2to3_parse(), which constructs blib2to3.pgen2.driver.Driver and invokes parse_string()  parse_tokens(). In parse_tokens(), DEDENT handling calls _partially_consume_prefix(), the function changed in blib2to3/pgen2/driver.py. The test inputs include tabs and dedents around comments, ensuring this code path is exercised.

Todo item 9 of 10 is correct
description: 'Ensure `test_comment_indentation` executes the code path that computes comment/indent prefixes via `blib2to3/pgen2/driver.py` (e.g., by running the formatter/parser that relies on this driver) rather than bypassing it.'

Validating todo item 10 of 10
description: 'Check that no tests or code still assume a visual tab width of 4 in the context of prefix/indent calculation, to avoid inconsistency with the updated logic.'

Calling tool: grepTool
current_column
Matched content:
120:        current_column = 0
126:                    if current_line.strip() and current_column < column:
132:                    current_column = 0
135:                current_column += 1
138:                current_column = 0

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char in ' \t':
135:                current_column += 1
136:            elif char == '\n':
137:                # unexpected empty line
138:                current_column = 0
139:            else:
140:                # indent is finished
141:                wait_for_nl = True
142:        return ''.join(lines), current_line
143:
144:
145:def _generate_pickle_name(gt, cache_dir=None):
146:    head, tail = os.path.splitext(gt)
147:    if tail == ".txt":
148:        tail = ""
149:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
150:    if cache_dir:
151:        return os.path.join(cache_dir, os.path.basename(name))
152:    else:
153:        return name
154:
155:
156:def load_grammar(gt="Grammar.txt", gp=None,
157:                 save=True, force=False, logger=None):
158:    """Load the grammar (maybe from a pickle)."""
159:    if logger is None:
160:        logger = logging.getLogger()
161:    gp = _generate_pickle_name(gt) if gp is None else gp
162:    if force or not _newer(gp, gt):
163:        logger.info("Generating grammar tables from %s", gt)
164:        g = pgen.generate_grammar(gt)
165:        if save:
166:            logger.info("Writing grammar tables to %s", gp)
167:            try:
168:                g.dump(gp)
169:            except OSError as e:

Calling tool: grepTool
\texpandtabs\(
No files found. Call grepTool again with a more generic pattern.

Calling tool: grepTool
expandtabs\(
No files found. Call grepTool again with a more generic pattern.

Calling tool: grepTool
_partially_consume_prefix|prefix
Showing first 50 results from grep. Consider using a more specific pattern or path.
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:73:        ignoring the prefix string and other context information.
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:193:        effectively equivalent to node.next_sibling.prefix
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:198:        return next_sib.prefix
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:210:                 prefix=None,
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:227:        if prefix is not None:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:228:            self.prefix = prefix
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:273:    def prefix(self):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:279:        return self.children[0].prefix
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:281:    @prefix.setter
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:282:    def prefix(self, prefix):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:284:            self.children[0].prefix = prefix
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:336:    _prefix = ""  # Whitespace and comments preceding this token in the input
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:342:                 prefix=None,
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:352:            self._prefix, (self.lineno, self.column) = context
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:355:        if prefix is not None:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:356:            self._prefix = prefix
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:372:        return self.prefix + str(self.value)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:384:                    (self.prefix, (self.lineno, self.column)),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:399:    def prefix(self):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:403:        return self._prefix
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:405:    @prefix.setter
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:406:    def prefix(self, prefix):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:408:        self._prefix = prefix
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:81:_litprefix = r"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?"
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:82:Triple = group(_litprefix + "'''", _litprefix + '"""')
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:84:String = group(_litprefix + r"'[^\n'\\]*(?:\\.[^\n'\\]*)*'",
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:85:               _litprefix + r'"[^\n"\\]*(?:\\.[^\n"\\]*)*"')
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:103:ContStr = group(_litprefix + r"'[^\n'\\]*(?:\\.[^\n'\\]*)*" +
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:105:                _litprefix + r'"[^\n"\\]*(?:\\.[^\n"\\]*)*' +
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:115:_strprefixes = (
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:123:            **{f"{prefix}'''": single3prog for prefix in _strprefixes},
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:124:            **{f'{prefix}"""': double3prog for prefix in _strprefixes},
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:125:            **{prefix: None for prefix in _strprefixes}}
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:129:    {f"{prefix}'''" for prefix in _strprefixes} |
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:130:    {f'{prefix}"""' for prefix in _strprefixes}
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:134:    {f"{prefix}'" for prefix in _strprefixes} |
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:135:    {f'{prefix}"' for prefix in _strprefixes}
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:41:        # XXX Move the prefix computation into a wrapper around tokenize.
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:48:        prefix = ""
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:55:                    prefix += "\n" * (s_lineno - lineno)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:59:                    prefix += line_text[column:s_column]
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:62:                prefix += value
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:71:                self.logger.debug("%s %r (prefix=%r)",
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:72:                                  token.tok_name[type], value, prefix)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:75:                _prefix = prefix + value
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:76:                prefix = ""
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:80:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:81:            if p.addtoken(type, value, (prefix, start)):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:85:            prefix = ""
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:87:                prefix = _prefix

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Modifications:
5:# Copyright 2006 Google, Inc. All Rights Reserved.
6:# Licensed to PSF under a Contributor Agreement.
7:
8:"""Parser driver.
9:
10:This provides a high-level interface to parse a file into a syntax tree.
11:
12:"""
13:
14:__author__ = "Guido van Rossum <guido@python.org>"
15:
16:__all__ = ["Driver", "load_grammar"]
17:
18:# Python imports
19:import codecs
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(self, grammar, convert=None, logger=None):
33:        self.grammar = grammar
34:        if logger is None:
35:            logger = logging.getLogger()
36:        self.logger = logger
37:        self.convert = convert
38:
39:    def parse_tokens(self, tokens, debug=False):
40:        """Parse a series of tokens and return the syntax tree."""
41:        # XXX Move the prefix computation into a wrapper around tokenize.
42:        p = parse.Parser(self.grammar, self.convert)
43:        p.setup()
44:        lineno = 1
45:        column = 0
46:        indent_columns = []
47:        type = value = start = end = line_text = None
48:        prefix = ""
49:        for quintuple in tokens:
50:            type, value, start, end, line_text = quintuple
51:            if start != (lineno, column):
52:                assert (lineno, column) <= start, ((lineno, column), start)
53:                s_lineno, s_column = start
54:                if lineno < s_lineno:
55:                    prefix += "\n" * (s_lineno - lineno)
56:                    lineno = s_lineno
57:                    column = 0
58:                if column < s_column:
59:                    prefix += line_text[column:s_column]
60:                    column = s_column
61:            if type in (tokenize.COMMENT, tokenize.NL):
62:                prefix += value
63:                lineno, column = end
64:                if value.endswith("\n"):
65:                    lineno += 1
66:                    column = 0
67:                continue
68:            if type == token.OP:
69:                type = grammar.opmap[value]
70:            if debug:
71:                self.logger.debug("%s %r (prefix=%r)",
72:                                  token.tok_name[type], value, prefix)
73:            if type == token.INDENT:
74:                indent_columns.append(len(value))
75:                _prefix = prefix + value
76:                prefix = ""
77:                value = ""
78:            elif type == token.DEDENT:
79:                _indent_col = indent_columns.pop()
80:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
81:            if p.addtoken(type, value, (prefix, start)):
82:                if debug:
83:                    self.logger.debug("Stop.")
84:                break
85:            prefix = ""
86:            if type in {token.INDENT, token.DEDENT}:
87:                prefix = _prefix
88:            lineno, column = end
89:            if value.endswith("\n"):
90:                lineno += 1
91:                column = 0
92:        else:
93:            # We never broke out -- EOF is too soon (how can this happen???)
94:            raise parse.ParseError("incomplete input",
95:                                   type, value, (prefix, start))
96:        return p.rootnode
97:
98:    def parse_stream_raw(self, stream, debug=False):
99:        """Parse a stream and return the syntax tree."""
100:        tokens = tokenize.generate_tokens(stream.readline)
101:        return self.parse_tokens(tokens, debug)
102:
103:    def parse_stream(self, stream, debug=False):
104:        """Parse a stream and return the syntax tree."""
105:        return self.parse_stream_raw(stream, debug)
106:
107:    def parse_file(self, filename, encoding=None, debug=False):
108:        """Parse a file and return the syntax tree."""
109:        with io.open(filename, "r", encoding=encoding) as stream:
110:            return self.parse_stream(stream, debug)

Calling tool: grepTool
INDENT|DEDENT
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:216:            if toknum == INDENT:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:219:            elif toknum == DEDENT:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:424:                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:438:                yield (DEDENT, '', (lnum, pos), (lnum, pos), line)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:562:        yield (DEDENT, '', (lnum, 0), (lnum, 0), '')
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:73:            if type == token.INDENT:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:78:            elif type == token.DEDENT:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:86:            if type in {token.INDENT, token.DEDENT}:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/token.pyi:11:INDENT: int
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/token.pyi:12:DEDENT: int
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/token.py:12:INDENT = 5
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/token.py:13:DEDENT = 6
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/README:12:- INDENT and DEDENT don't hold whitespace and comment prefixes
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/Grammar.txt:83:suite: simple_stmt | NEWLINE INDENT stmt+ DEDENT

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py
380:                       strstart, (lnum, end), contline + line)
381:                contstr, needcont = '', 0
382:                contline = None
383:            elif needcont and line[-2:] != '\\\n' and line[-3:] != '\\\r\n':
384:                yield (ERRORTOKEN, contstr + line,
385:                           strstart, (lnum, len(line)), contline)
386:                contstr = ''
387:                contline = None
388:                continue
389:            else:
390:                contstr = contstr + line
391:                contline = contline + line
392:                continue
393:
394:        elif parenlev == 0 and not continued:  # new statement
395:            if not line: break
396:            column = 0
397:            while pos < max:                   # measure leading whitespace
398:                if line[pos] == ' ': column = column + 1
399:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize
400:                elif line[pos] == '\f': column = 0
401:                else: break
402:                pos = pos + 1
403:            if pos == max: break
404:
405:            if stashed:
406:                yield stashed
407:                stashed = None
408:
409:            if line[pos] in '\r\n':            # skip blank lines
410:                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)
411:                continue
412:
413:            if line[pos] == '#':               # skip comments
414:                comment_token = line[pos:].rstrip('\r\n')
415:                nl_pos = pos + len(comment_token)
416:                yield (COMMENT, comment_token,
417:                        (lnum, pos), (lnum, pos + len(comment_token)), line)
418:                yield (NL, line[nl_pos:],
419:                        (lnum, nl_pos), (lnum, len(line)), line)
420:                continue
421:
422:            if column > indents[-1]:           # count indents
423:                indents.append(column)
424:                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)
425:
426:            while column < indents[-1]:        # count dedents
427:                if column not in indents:
428:                    raise IndentationError(
429:                        "unindent does not match any outer indentation level",
430:                        ("<tokenize>", lnum, pos, line))
431:                indents = indents[:-1]
432:
433:                if async_def and async_def_indent >= indents[-1]:
434:                    async_def = False
435:                    async_def_nl = False
436:                    async_def_indent = 0
437:
438:                yield (DEDENT, '', (lnum, pos), (lnum, pos), line)
439:
440:            if async_def and async_def_nl and async_def_indent >= indents[-1]:
441:                async_def = False
442:                async_def_nl = False
443:                async_def_indent = 0
444:
445:        else:                                  # continued statement
446:            if not line:
447:                raise TokenError("EOF in multi-line statement", (lnum, 0))
448:            continued = 0
449:
450:        while pos < max:
451:            pseudomatch = pseudoprog.match(line, pos)
452:            if pseudomatch:                                # scan for tokens
453:                start, end = pseudomatch.span(1)
454:                spos, epos, pos = (lnum, start), (lnum, end), end
455:                token, initial = line[start:end], line[start]
456:
457:                if initial in numchars or \
458:                   (initial == '.' and token != '.'):      # ordinary number
459:                    yield (NUMBER, token, spos, epos, line)
460:                elif initial in '\r\n':
461:                    newline = NEWLINE
462:                    if parenlev > 0:
463:                        newline = NL
464:                    elif async_def:
465:                        async_def_nl = True
466:                    if stashed:
467:                        yield stashed
468:                        stashed = None
469:                    yield (newline, token, spos, epos, line)
470:
471:                elif initial == '#':
472:                    assert not token.endswith("\n")
473:                    if stashed:
474:                        yield stashed
475:                        stashed = None
476:                    yield (COMMENT, token, spos, epos, line)
477:                elif token in triple_quoted:
478:                    endprog = endprogs[token]
479:                    endmatch = endprog.match(line, pos)
480:                    if endmatch:                           # all on one line
481:                        pos = endmatch.end(0)
482:                        token = line[start:pos]
483:                        if stashed:
484:                            yield stashed
485:                            stashed = None
486:                        yield (STRING, token, spos, (lnum, pos), line)
487:                    else:
488:                        strstart = (lnum, start)           # multiple lines
489:                        contstr = line[start:]
490:                        contline = line
491:                        break
492:                elif initial in single_quoted or \
493:                    token[:2] in single_quoted or \
494:                    token[:3] in single_quoted:
495:                    if token[-1] == '\n':                  # continued string
496:                        strstart = (lnum, start)
497:                        endprog = (endprogs[initial] or endprogs[token[1]] or
498:                                   endprogs[token[2]])
499:                        contstr, needcont = line[start:], 1
500:                        contline = line
501:                        break
502:                    else:                                  # ordinary string
503:                        if stashed:
504:                            yield stashed
505:                            stashed = None
506:                        yield (STRING, token, spos, epos, line)
507:                elif initial.isidentifier():               # ordinary name
508:                    if token in ('async', 'await'):
509:                        if async_def:
510:                            yield (ASYNC if token == 'async' else AWAIT,
511:                                   token, spos, epos, line)
512:                            continue
513:
514:                    tok = (NAME, token, spos, epos, line)
515:                    if token == 'async' and not stashed:
516:                        stashed = tok
517:                        continue
518:
519:                    if token in ('def', 'for'):
520:                        if (stashed
521:                                and stashed[0] == NAME
522:                                and stashed[1] == 'async'):
523:
524:                            if token == 'def':
525:                                async_def = True
526:                                async_def_indent = indents[-1]
527:
528:                            yield (ASYNC, stashed[1],
529:                                   stashed[2], stashed[3],
530:                                   stashed[4])
531:                            stashed = None
532:
533:                    if stashed:
534:                        yield stashed
535:                        stashed = None
536:
537:                    yield tok
538:                elif initial == '\\':                      # continued stmt
539:                    # This yield is new; needed for better idempotency:
540:                    if stashed:
541:                        yield stashed
542:                        stashed = None
543:                    yield (NL, token, spos, (lnum, pos), line)
544:                    continued = 1
545:                else:
546:                    if initial in '([{': parenlev = parenlev + 1
547:                    elif initial in ')]}': parenlev = parenlev - 1
548:                    if stashed:
549:                        yield stashed
550:                        stashed = None
551:                    yield (OP, token, spos, epos, line)
552:            else:
553:                yield (ERRORTOKEN, line[pos],
554:                           (lnum, pos), (lnum, pos+1), line)
555:                pos = pos + 1
556:
557:    if stashed:
558:        yield stashed
559:        stashed = None
560:
561:    for indent in indents[1:]:                 # pop remaining indent levels
562:        yield (DEDENT, '', (lnum, 0), (lnum, 0), '')
563:    yield (ENDMARKER, '', (lnum, 0), (lnum, 0), '')
564:
565:if __name__ == '__main__':                     # testing
566:    import sys
567:    if len(sys.argv) > 1: tokenize(open(sys.argv[1]).readline)
568:    else: tokenize(sys.stdin.readline)

Calling tool: grepTool
tabsize
Matched content:
138:tabsize = 8
399:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize

Calling tool: grepTool
\\t'\):\s*current_column\s*\+=\s*4|\+\=\s*4
No files found. Call grepTool again with a more generic pattern.

Calling tool: grepTool
test_comment_indentation
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/bugsinpy_run_test.sh:1:python -m unittest -q tests.test_black.BlackTestCase.test_comment_indentation
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:512:    def test_comment_indentation(self) -> None:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py
480:    @patch("black.dump_to_file", dump_to_stderr)
481:    def test_fmtonoff2(self) -> None:
482:        source, expected = read_data("fmtonoff2")
483:        actual = fs(source)
484:        self.assertFormatEqual(expected, actual)
485:        black.assert_equivalent(source, actual)
486:        black.assert_stable(source, actual, line_length=ll)
487:
488:    @patch("black.dump_to_file", dump_to_stderr)
489:    def test_remove_empty_parentheses_after_class(self) -> None:
490:        source, expected = read_data("class_blank_parentheses")
491:        actual = fs(source)
492:        self.assertFormatEqual(expected, actual)
493:        black.assert_equivalent(source, actual)
494:        black.assert_stable(source, actual, line_length=ll)
495:
496:    @patch("black.dump_to_file", dump_to_stderr)
497:    def test_new_line_between_class_and_code(self) -> None:
498:        source, expected = read_data("class_methods_new_line")
499:        actual = fs(source)
500:        self.assertFormatEqual(expected, actual)
501:        black.assert_equivalent(source, actual)
502:        black.assert_stable(source, actual, line_length=ll)
503:
504:    @patch("black.dump_to_file", dump_to_stderr)
505:    def test_bracket_match(self) -> None:
506:        source, expected = read_data("bracketmatch")
507:        actual = fs(source)
508:        self.assertFormatEqual(expected, actual)
509:        black.assert_equivalent(source, actual)
510:        black.assert_stable(source, actual, line_length=ll)
511:
512:    def test_comment_indentation(self) -> None:
513:        contents_tab = "if 1:\n\tif 2:\n\t\tpass\n\t# comment\n\tpass\n"
514:        contents_spc = "if 1:\n    if 2:\n        pass\n    # comment\n    pass\n"
515:
516:        self.assertFormatEqual(fs(contents_spc), contents_spc)
517:        self.assertFormatEqual(fs(contents_tab), contents_spc)
518:
519:        contents_tab = "if 1:\n\tif 2:\n\t\tpass\n\t\t# comment\n\tpass\n"
520:        contents_spc = "if 1:\n    if 2:\n        pass\n        # comment\n    pass\n"
521:
522:        self.assertFormatEqual(fs(contents_tab), contents_spc)
523:        self.assertFormatEqual(fs(contents_spc), contents_spc)
524:
525:    def test_report_verbose(self) -> None:
526:        report = black.Report(verbose=True)
527:        out_lines = []
528:        err_lines = []
529:
530:        def out(msg: str, **kwargs: Any) -> None:
531:            out_lines.append(msg)
532:
533:        def err(msg: str, **kwargs: Any) -> None:
534:            err_lines.append(msg)
535:
536:        with patch("black.out", out), patch("black.err", err):
537:            report.done(Path("f1"), black.Changed.NO)
538:            self.assertEqual(len(out_lines), 1)
539:            self.assertEqual(len(err_lines), 0)
540:            self.assertEqual(out_lines[-1], "f1 already well formatted, good job.")
541:            self.assertEqual(unstyle(str(report)), "1 file left unchanged.")
542:            self.assertEqual(report.return_code, 0)
543:            report.done(Path("f2"), black.Changed.YES)
544:            self.assertEqual(len(out_lines), 2)
545:            self.assertEqual(len(err_lines), 0)
546:            self.assertEqual(out_lines[-1], "reformatted f2")
547:            self.assertEqual(
548:                unstyle(str(report)), "1 file reformatted, 1 file left unchanged."
549:            )
550:            report.done(Path("f3"), black.Changed.CACHED)
551:            self.assertEqual(len(out_lines), 3)
552:            self.assertEqual(len(err_lines), 0)
553:            self.assertEqual(
554:                out_lines[-1], "f3 wasn't modified on disk since last run."
555:            )
556:            self.assertEqual(
557:                unstyle(str(report)), "1 file reformatted, 2 files left unchanged."
558:            )
559:            self.assertEqual(report.return_code, 0)

Calling tool: grepTool
\t|tab\s*width|tabwidth|tabs|\b4\b
Grep command failed with exit code 2: rg: /Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black: IO error for operation on /Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black: No such file or directory (os error 2)


Calling tool: globTool
**/*
Showing first 50 results from grep. Consider using a more specific pattern or path.
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/Pipfile.lock
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.travis.yml
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/bugsinpy_patchfile.info
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.git/packed-refs
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/Pipfile
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/bugsinpy_bug.info
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.git/ORIG_HEAD
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/bugsinpy_requirements.txt
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/coverage_bugsinpy.txt
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/setup.cfg
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/setup.py
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/mypy.ini
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.git/index
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.git/description
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/readthedocs.yml
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black.py
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.flake8
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.rgignore
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/MANIFEST.in
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/bugsinpy_run_test.sh
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.appveyor.yml
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/README.md
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/plugin/black.vim
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.gitignore
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/CONTRIBUTING.md
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.pyi
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/PatternGrammar.txt
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/__init__.pyi
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/LICENSE
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/Grammar.txt
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/bugsinpy_compile_flag
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.git/hooks/push-to-checkout.sample
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.git/hooks/update.sample
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.git/hooks/pre-push.sample
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.git/hooks/pre-applypatch.sample
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.git/hooks/pre-merge-commit.sample
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.git/hooks/post-update.sample
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.git/hooks/prepare-commit-msg.sample
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.git/hooks/pre-receive.sample
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.git/hooks/fsmonitor-watchman.sample
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.git/hooks/applypatch-msg.sample
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.git/hooks/pre-commit.sample
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.git/hooks/pre-rebase.sample
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.git/hooks/commit-msg.sample
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/README
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pygram.py
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pygram.pyi
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/.github/ISSUE_TEMPLATE.md

Calling tool: grepTool
\t|tab|indent
Matched content:
608:    valid by calling :func:`assert_equivalent` and :func:`assert_stable` on it.
620:        assert_stable(src_contents, dst_contents, line_length=line_length, mode=mode)
749:        indent = " " * (2 * self.tree_depth)
752:            out(f"{indent}{_type}", fg="yellow")
758:            out(f"{indent}/{_type}", fg="yellow", bold=False)
761:            out(f"{indent}{_type}", fg="blue", nl=False)
1292:        indent = "    " * self.depth
1295:        res = f"{first.prefix}{indent}{first.value}"
1432:    def line(self, indent: int = 0) -> Iterator[Line]:
1441:            self.current_line.depth += indent
1445:        self.current_line = Line(depth=complete_line.depth + indent)
1479:        """Increase indentation level, maybe yield a line."""
1485:        """Decrease indentation level, maybe yield a line."""
1492:        # that belong to the current indentation level.  Get 'em.
2328:    If `is_body` is True, the result line is one-indented inside brackets and as such
2336:            # Since body is a new indent level, remove spurious leading whitespace.
2352:def dont_increase_indentation(split_func: SplitFunc) -> SplitFunc:
2367:@dont_increase_indentation
2431:@dont_increase_indentation
2731:                # the previous leaf is indentation, it's a standalone comment in
3340:def assert_stable(
3480:    avoid some unfortunate formattings (mostly around wrapping unsplittable code

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black.py
1240:            comment.prefix = ""
1241:            return False
1242:
1243:        else:
1244:            leaf_id = id(self.leaves[-1])
1245:            if leaf_id not in self.comments:
1246:                self.comments[leaf_id] = [comment]
1247:            else:
1248:                self.comments[leaf_id].append(comment)
1249:            return True
1250:
1251:    def comments_after(self, leaf: Leaf) -> List[Leaf]:
1252:        """Generate comments that should appear directly after `leaf`."""
1253:        return self.comments.get(id(leaf), [])
1254:
1255:    def remove_trailing_comma(self) -> None:
1256:        """Remove the trailing comma and moves the comments attached to it."""
1257:        # Remember, the LeafID keys of self.comments are ordered by the
1258:        # corresponding leaf's index in self.leaves
1259:        # If id(self.leaves[-2]) is in self.comments, the order doesn't change.
1260:        # Otherwise, we insert it into self.comments, and it becomes the last entry.
1261:        # However, since we delete id(self.leaves[-1]) from self.comments, the invariant
1262:        # is maintained
1263:        self.comments.setdefault(id(self.leaves[-2]), []).extend(
1264:            self.comments.get(id(self.leaves[-1]), [])
1265:        )
1266:        self.comments.pop(id(self.leaves[-1]), None)
1267:        self.leaves.pop()
1268:
1269:    def is_complex_subscript(self, leaf: Leaf) -> bool:
1270:        """Return True iff `leaf` is part of a slice with non-trivial exprs."""
1271:        open_lsqb = self.bracket_tracker.get_open_lsqb()
1272:        if open_lsqb is None:
1273:            return False
1274:
1275:        subscript_start = open_lsqb.next_sibling
1276:
1277:        if isinstance(subscript_start, Node):
1278:            if subscript_start.type == syms.listmaker:
1279:                return False
1280:
1281:            if subscript_start.type == syms.subscriptlist:
1282:                subscript_start = child_towards(subscript_start, leaf)
1283:        return subscript_start is not None and any(
1284:            n.type in TEST_DESCENDANTS for n in subscript_start.pre_order()
1285:        )
1286:
1287:    def __str__(self) -> str:
1288:        """Render the line."""
1289:        if not self:
1290:            return "\n"
1291:
1292:        indent = "    " * self.depth
1293:        leaves = iter(self.leaves)
1294:        first = next(leaves)
1295:        res = f"{first.prefix}{indent}{first.value}"
1296:        for leaf in leaves:
1297:            res += str(leaf)
1298:        for comment in itertools.chain.from_iterable(self.comments.values()):
1299:            res += str(comment)
1300:        return res + "\n"
1301:
1302:    def __bool__(self) -> bool:
1303:        """Return True if the line has leaves or comments."""
1304:        return bool(self.leaves or self.comments)
1305:
1306:
1307:@dataclass
1308:class EmptyLineTracker:
1309:    """Provides a stateful method that returns the number of potential extra
1310:    empty lines needed before and after the currently processed line.
1311:
1312:    Note: this tracker works on lines that haven't been split yet.  It assumes
1313:    the prefix of the first leaf consists of optional newlines.  Those newlines
1314:    are consumed by `maybe_empty_lines()` and included in the computation.
1315:    """
1316:
1317:    is_pyi: bool = False
1318:    previous_line: Optional[Line] = None
1319:    previous_after: int = 0
1320:    previous_defs: List[int] = Factory(list)
1321:
1322:    def maybe_empty_lines(self, current_line: Line) -> Tuple[int, int]:
1323:        """Return the number of extra empty lines before and after the `current_line`.
1324:
1325:        This is for separating `def`, `async def` and `class` with extra empty
1326:        lines (two on module-level).
1327:        """
1328:        before, after = self._maybe_empty_lines(current_line)
1329:        before -= self.previous_after
1330:        self.previous_after = after
1331:        self.previous_line = current_line
1332:        return before, after
1333:
1334:    def _maybe_empty_lines(self, current_line: Line) -> Tuple[int, int]:
1335:        max_allowed = 1
1336:        if current_line.depth == 0:
1337:            max_allowed = 1 if self.is_pyi else 2
1338:        if current_line.leaves:
1339:            # Consume the first leaf's extra newlines.
1340:            first_leaf = current_line.leaves[0]
1341:            before = first_leaf.prefix.count("\n")
1342:            before = min(before, max_allowed)
1343:            first_leaf.prefix = ""
1344:        else:
1345:            before = 0
1346:        depth = current_line.depth
1347:        while self.previous_defs and self.previous_defs[-1] >= depth:
1348:            self.previous_defs.pop()
1349:            if self.is_pyi:
1350:                before = 0 if depth else 1
1351:            else:
1352:                before = 1 if depth else 2
1353:        if current_line.is_decorator or current_line.is_def or current_line.is_class:
1354:            return self._maybe_empty_lines_for_class_or_def(current_line, before)
1355:
1356:        if (
1357:            self.previous_line
1358:            and self.previous_line.is_import
1359:            and not current_line.is_import

Calling tool: grepTool
\t
No files found. Call grepTool again with a more generic pattern.

Calling tool: grepTool
column\b
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:338:    column = 0    # Column where this token tarts in the input
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:352:            self._prefix, (self.lineno, self.column) = context
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.py:384:                    (self.prefix, (self.lineno, self.column)),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:13:    the starting (row, column) indices of the token (a 2-tuple of ints)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:14:    the ending (row, column) indices of the token (a 2-tuple of ints)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:348:    column where the token begins in the source; a 2-tuple (erow, ecol) of
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:349:    ints specifying the row and column where the token ends in the source;
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:396:            column = 0
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:398:                if line[pos] == ' ': column = column + 1
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:399:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:400:                elif line[pos] == '\f': column = 0
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:422:            if column > indents[-1]:           # count indents
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:423:                indents.append(column)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:426:            while column < indents[-1]:        # count dedents
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:427:                if column not in indents:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:45:        column = 0
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:51:            if start != (lineno, column):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:52:                assert (lineno, column) <= start, ((lineno, column), start)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:53:                s_lineno, s_column = start
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:57:                    column = 0
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:58:                if column < s_column:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:59:                    prefix += line_text[column:s_column]
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:60:                    column = s_column
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:63:                lineno, column = end
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:66:                    column = 0
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:88:            lineno, column = end
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:91:                column = 0
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:117:    def _partially_consume_prefix(self, prefix, column):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:120:        current_column = 0
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:126:                    if current_line.strip() and current_column < column:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:132:                    current_column = 0
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:135:                current_column += 1
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:138:                current_column = 0
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.pyi:55:    column: int

Calling tool: grepTool
indent_columns
Matched content:
46:        indent_columns = []
74:                indent_columns.append(len(value))
79:                _indent_col = indent_columns.pop()

Calling tool: grepTool
tab\s*width|tabwidth|visual\s*tab|\b4\s*spaces
No files found. Call grepTool again with a more generic pattern.

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: Searched the codebase for any assumptions of tab width 4 in prefix/indent handling. The only relevant logic is in blib2to3/pgen2/driver.py:_partially_consume_prefix, which now counts both space and tab as +1. No other code uses +4 or expandtabs; tokenize uses tabsize=8 (unrelated). The test test_comment_indentation expects correct behavior and doesnt assume tab width 4. Thus, no remaining 4-width assumptions remain.

Todo item 10 of 10 is correct
description: 'Check that no tests or code still assume a visual tab width of 4 in the context of prefix/indent calculation, to avoid inconsistency with the updated logic.'


=== PHASE 2 COMPLETE ===

