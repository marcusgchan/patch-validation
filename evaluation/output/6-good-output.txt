=== PHASE 1: ANALYSIS ===

[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "(?i)from\\s+.*import\\s+dataclass",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "def supports_feature\\(",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "def get_parser_configs\\(|class ParserConfig",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "test_async_as_identifier",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "async_as_identifier.py",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/data"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "generate_tokens\\(",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "get_grammars\\(", "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black" ]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "get_parser_configs\\(|lib2to3_parse\\(",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black"
]
Calling tool: grepTool
(?i)from\s+.*import\s+dataclass
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:40:from attr import dataclass, evolve, Factory
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi:5:from attr import dataclass
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:34:from attr import dataclass
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/black.py:40:from attr import dataclass, evolve, Factory
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/dataclasses-0.6.dist-info/METADATA:47:  from dataclasses import dataclass
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:14:from dataclasses import dataclass
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pip/_vendor/typing_extensions.py:2015:            from pip._vendor.typing_extensions import dataclass_transform
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pip/_internal/self_outdated_check.py:9:from dataclasses import dataclass
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pip/_vendor/rich/_windows.py:2:from dataclasses import dataclass
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pip/_vendor/rich/progress.py:8:from dataclasses import dataclass, field
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pip/_vendor/rich/traceback.py:6:from dataclasses import dataclass, field
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pip/_vendor/rich/table.py:1:from dataclasses import dataclass, field, replace
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pip/_vendor/rich/pretty.py:9:from dataclasses import dataclass, fields, is_dataclass
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pip/_vendor/rich/console.py:9:from dataclasses import dataclass, field
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pip/_vendor/rich/_ratio.py:150:    from dataclasses import dataclass
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pip/_internal/models/link.py:8:from dataclasses import dataclass
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pip/_internal/utils/logging.py:8:from dataclasses import dataclass
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/mypy/plugins/default.py:95:        from mypy.plugins import dataclasses

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
1:import asyncio
2:from asyncio.base_events import BaseEventLoop
3:from concurrent.futures import Executor, ProcessPoolExecutor
4:from datetime import datetime
5:from enum import Enum
6:from functools import lru_cache, partial, wraps
7:import io
8:import itertools
9:import logging
10:from multiprocessing import Manager, freeze_support
11:import os
12:from pathlib import Path
13:import pickle
14:import re
15:import signal
16:import sys
17:import tempfile
18:import tokenize
19:from typing import (
20:    Any,
21:    Callable,
22:    Collection,
23:    Dict,
24:    Generator,
25:    Generic,
26:    Iterable,
27:    Iterator,
28:    List,
29:    Optional,
30:    Pattern,
31:    Sequence,
32:    Set,
33:    Tuple,
34:    TypeVar,
35:    Union,
36:    cast,
37:)
38:
39:from appdirs import user_cache_dir
40:from attr import dataclass, evolve, Factory
41:import click
42:import toml
43:from typed_ast import ast3, ast27
44:
45:# lib2to3 fork
46:from blib2to3.pytree import Node, Leaf, type_repr
47:from blib2to3 import pygram, pytree
48:from blib2to3.pgen2 import driver, token
49:from blib2to3.pgen2.grammar import Grammar
50:from blib2to3.pgen2.parse import ParseError
51:from blib2to3.pgen2.tokenize import TokenizerConfig
52:
53:
54:__version__ = "19.3b0"
55:DEFAULT_LINE_LENGTH = 88
56:DEFAULT_EXCLUDES = (
57:    r"/(\.eggs|\.git|\.hg|\.mypy_cache|\.nox|\.tox|\.venv|_build|buck-out|build|dist)/"
58:)
59:DEFAULT_INCLUDES = r"\.pyi?$"
60:CACHE_DIR = Path(user_cache_dir("black", version=__version__))
61:
62:
63:# types
64:FileContent = str
65:Encoding = str
66:NewLine = str
67:Depth = int
68:NodeType = int
69:LeafID = int
70:Priority = int
71:Index = int
72:LN = Union[Leaf, Node]
73:SplitFunc = Callable[["Line", Collection["Feature"]], Iterator["Line"]]
74:Timestamp = float
75:FileSize = int
76:CacheInfo = Tuple[Timestamp, FileSize]
77:Cache = Dict[Path, CacheInfo]
78:out = partial(click.secho, bold=True, err=True)
79:err = partial(click.secho, fg="red", err=True)
80:
81:pygram.initialize(CACHE_DIR)
82:syms = pygram.python_symbols
83:
84:
85:class NothingChanged(UserWarning):
86:    """Raised when reformatted code is the same as source."""
87:
88:
89:class CannotSplit(Exception):
90:    """A readable split that fits the allotted line length is impossible."""
91:
92:
93:class InvalidInput(ValueError):
94:    """Raised when input source code fails all parse attempts."""
95:
96:
97:class WriteBack(Enum):
98:    NO = 0
99:    YES = 1
100:    DIFF = 2
101:    CHECK = 3
102:
103:    @classmethod
104:    def from_configuration(cls, *, check: bool, diff: bool) -> "WriteBack":
105:        if check and not diff:
106:            return cls.CHECK
107:
108:        return cls.DIFF if diff else cls.YES
109:
110:
111:class Changed(Enum):
112:    NO = 0
113:    CACHED = 1
114:    YES = 2
115:
116:
117:class TargetVersion(Enum):
118:    PY27 = 2
119:    PY33 = 3
120:    PY34 = 4

Calling tool: grepTool
def supports_feature\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:206:def supports_feature(target_versions: Set[TargetVersion], feature: Feature) -> bool:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/black.py:194:def supports_feature(target_versions: Set[TargetVersion], feature: Feature) -> bool:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
120:    PY34 = 4
121:    PY35 = 5
122:    PY36 = 6
123:    PY37 = 7
124:    PY38 = 8
125:
126:    def is_python2(self) -> bool:
127:        return self is TargetVersion.PY27
128:
129:
130:PY36_VERSIONS = {TargetVersion.PY36, TargetVersion.PY37, TargetVersion.PY38}
131:
132:
133:class Feature(Enum):
134:    # All string literals are unicode
135:    UNICODE_LITERALS = 1
136:    F_STRINGS = 2
137:    NUMERIC_UNDERSCORES = 3
138:    TRAILING_COMMA_IN_CALL = 4
139:    TRAILING_COMMA_IN_DEF = 5
140:    # The following two feature-flags are mutually exclusive, and exactly one should be
141:    # set for every version of python.
142:    ASYNC_IS_VALID_IDENTIFIER = 6
143:    ASYNC_IS_RESERVED_KEYWORD = 7
144:
145:
146:VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {
147:    TargetVersion.PY27: {Feature.ASYNC_IS_VALID_IDENTIFIER},
148:    TargetVersion.PY33: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
149:    TargetVersion.PY34: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
150:    TargetVersion.PY35: {
151:        Feature.UNICODE_LITERALS,
152:        Feature.TRAILING_COMMA_IN_CALL,
153:        Feature.ASYNC_IS_VALID_IDENTIFIER,
154:    },
155:    TargetVersion.PY36: {
156:        Feature.UNICODE_LITERALS,
157:        Feature.F_STRINGS,
158:        Feature.NUMERIC_UNDERSCORES,
159:        Feature.TRAILING_COMMA_IN_CALL,
160:        Feature.TRAILING_COMMA_IN_DEF,
161:        Feature.ASYNC_IS_VALID_IDENTIFIER,
162:    },
163:    TargetVersion.PY37: {
164:        Feature.UNICODE_LITERALS,
165:        Feature.F_STRINGS,
166:        Feature.NUMERIC_UNDERSCORES,
167:        Feature.TRAILING_COMMA_IN_CALL,
168:        Feature.TRAILING_COMMA_IN_DEF,
169:        Feature.ASYNC_IS_RESERVED_KEYWORD,
170:    },
171:    TargetVersion.PY38: {
172:        Feature.UNICODE_LITERALS,
173:        Feature.F_STRINGS,
174:        Feature.NUMERIC_UNDERSCORES,
175:        Feature.TRAILING_COMMA_IN_CALL,
176:        Feature.TRAILING_COMMA_IN_DEF,
177:        Feature.ASYNC_IS_RESERVED_KEYWORD,
178:    },
179:}
180:
181:
182:@dataclass
183:class FileMode:
184:    target_versions: Set[TargetVersion] = Factory(set)
185:    line_length: int = DEFAULT_LINE_LENGTH
186:    string_normalization: bool = True
187:    is_pyi: bool = False
188:
189:    def get_cache_key(self) -> str:
190:        if self.target_versions:
191:            version_str = ",".join(
192:                str(version.value)
193:                for version in sorted(self.target_versions, key=lambda v: v.value)
194:            )
195:        else:
196:            version_str = "-"
197:        parts = [
198:            version_str,
199:            str(self.line_length),
200:            str(int(self.string_normalization)),
201:            str(int(self.is_pyi)),
202:        ]
203:        return ".".join(parts)
204:
205:
206:def supports_feature(target_versions: Set[TargetVersion], feature: Feature) -> bool:
207:    return all(feature in VERSION_TO_FEATURES[version] for version in target_versions)
208:
209:
210:def read_pyproject_toml(
211:    ctx: click.Context, param: click.Parameter, value: Union[str, int, bool, None]
212:) -> Optional[str]:
213:    """Inject Black configuration from "pyproject.toml" into defaults in `ctx`.
214:
215:    Returns the path to a successfully found and read configuration file, None
216:    otherwise.
217:    """
218:    assert not isinstance(value, (int, bool)), "Invalid parameter type passed"
219:    if not value:
220:        root = find_project_root(ctx.params.get("src", ()))
221:        path = root / "pyproject.toml"
222:        if path.is_file():
223:            value = str(path)
224:        else:
225:            return None
226:
227:    try:
228:        pyproject_toml = toml.load(value)
229:        config = pyproject_toml.get("tool", {}).get("black", {})
230:    except (toml.TomlDecodeError, OSError) as e:
231:        raise click.FileError(
232:            filename=value, hint=f"Error reading configuration file: {e}"
233:        )
234:
235:    if not config:
236:        return None
237:
238:    if ctx.default_map is None:
239:        ctx.default_map = {}
240:    ctx.default_map.update(  # type: ignore  # bad types in .pyi
241:        {k.replace("--", "").replace("-", "_"): v for k, v in config.items()}
242:    )
243:    return value
244:
245:
246:@click.command(context_settings=dict(help_option_names=["-h", "--help"]))
247:@click.option("-c", "--code", type=str, help="Format the code passed in as a string.")
248:@click.option(
249:    "-l",

Calling tool: grepTool
def get_parser_configs\(|class ParserConfig
Matched content:
764:class ParserConfig:
769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
740:            current_line, line_length=mode.line_length, features=split_line_features
741:        ):
742:            dst_contents += str(line)
743:    return dst_contents
744:
745:
746:def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:
747:    """Return a tuple of (decoded_contents, encoding, newline).
748:
749:    `newline` is either CRLF or LF but `decoded_contents` is decoded with
750:    universal newlines (i.e. only contains LF).
751:    """
752:    srcbuf = io.BytesIO(src)
753:    encoding, lines = tokenize.detect_encoding(srcbuf.readline)
754:    if not lines:
755:        return "", encoding, "\n"
756:
757:    newline = "\r\n" if b"\r\n" == lines[0][-2:] else "\n"
758:    srcbuf.seek(0)
759:    with io.TextIOWrapper(srcbuf, encoding) as tiow:
760:        return tiow.read(), encoding, newline
761:
762:
763:@dataclass(frozen=True)
764:class ParserConfig:
765:    grammar: Grammar
766:    tokenizer_config: TokenizerConfig = TokenizerConfig()
767:
768:
769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:
770:    if not target_versions:
771:        # No target_version specified, so try all grammars.
772:        return [
773:            # Python 3.7+
774:            ParserConfig(
775:                pygram.python_grammar_no_print_statement_no_exec_statement,
776:                TokenizerConfig(async_is_reserved_keyword=True),
777:            ),
778:            # Python 3.0-3.6
779:            ParserConfig(
780:                pygram.python_grammar_no_print_statement_no_exec_statement,
781:                TokenizerConfig(async_is_reserved_keyword=False),
782:            ),
783:            # Python 2.7 with future print_function import
784:            ParserConfig(pygram.python_grammar_no_print_statement),
785:            # Python 2.7
786:            ParserConfig(pygram.python_grammar),
787:        ]
788:    elif all(version.is_python2() for version in target_versions):
789:        # Python 2-only code, so try Python 2 grammars.
790:        return [
791:            # Python 2.7 with future print_function import
792:            ParserConfig(pygram.python_grammar_no_print_statement),
793:            # Python 2.7
794:            ParserConfig(pygram.python_grammar),
795:        ]
796:    else:
797:        # Python 3-compatible code, so only try Python 3 grammar.
798:        configs = []
799:        # If we have to parse both, try to parse async as a keyword first
800:        if not supports_feature(target_versions, Feature.ASYNC_IS_VALID_IDENTIFIER):
801:            # Python 3.7+
802:            configs.append(
803:                ParserConfig(
804:                    pygram.python_grammar_no_print_statement_no_exec_statement,
805:                    TokenizerConfig(async_is_reserved_keyword=True),
806:                )
807:            )
808:        if not supports_feature(target_versions, Feature.ASYNC_IS_RESERVED_KEYWORD):
809:            # Python 3.0-3.6
810:            configs.append(
811:                ParserConfig(
812:                    pygram.python_grammar_no_print_statement_no_exec_statement,
813:                    TokenizerConfig(async_is_reserved_keyword=False),
814:                )
815:            )
816:        # At least one of the above branches must have been taken, because every Python
817:        # version has exactly one of the two 'ASYNC_IS_*' flags
818:        return configs
819:
820:
821:def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:
822:    """Given a string with source, return the lib2to3 Node."""
823:    if src_txt[-1:] != "\n":
824:        src_txt += "\n"
825:
826:    for parser_config in get_parser_configs(set(target_versions)):
827:        drv = driver.Driver(
828:            parser_config.grammar,
829:            pytree.convert,
830:            tokenizer_config=parser_config.tokenizer_config,
831:        )
832:        try:
833:            result = drv.parse_string(src_txt, True)
834:            break
835:
836:        except ParseError as pe:
837:            lineno, column = pe.context[1]
838:            lines = src_txt.splitlines()
839:            try:
840:                faulty_line = lines[lineno - 1]
841:            except IndexError:
842:                faulty_line = "<line number missing in source>"
843:            exc = InvalidInput(f"Cannot parse: {lineno}:{column}: {faulty_line}")
844:    else:
845:        raise exc from None
846:
847:    if isinstance(result, Leaf):
848:        result = Node(syms.file_input, [result])
849:    return result
850:
851:
852:def lib2to3_unparse(node: Node) -> str:
853:    """Given a lib2to3 node, return its string representation."""
854:    code = str(node)
855:    return code
856:
857:
858:T = TypeVar("T")
859:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Modifications:
5:# Copyright 2006 Google, Inc. All Rights Reserved.
6:# Licensed to PSF under a Contributor Agreement.
7:
8:"""Parser driver.
9:
10:This provides a high-level interface to parse a file into a syntax tree.
11:
12:"""
13:
14:__author__ = "Guido van Rossum <guido@python.org>"
15:
16:__all__ = ["Driver", "load_grammar"]
17:
18:# Python imports
19:import codecs
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(
33:        self,
34:        grammar,
35:        convert=None,
36:        logger=None,
37:        tokenizer_config=tokenize.TokenizerConfig(),
38:    ):
39:        self.grammar = grammar
40:        if logger is None:
41:            logger = logging.getLogger(__name__)
42:        self.logger = logger
43:        self.convert = convert
44:        self.tokenizer_config = tokenizer_config
45:
46:    def parse_tokens(self, tokens, debug=False):
47:        """Parse a series of tokens and return the syntax tree."""
48:        # XXX Move the prefix computation into a wrapper around tokenize.
49:        p = parse.Parser(self.grammar, self.convert)
50:        p.setup()
51:        lineno = 1
52:        column = 0
53:        indent_columns = []
54:        type = value = start = end = line_text = None
55:        prefix = ""
56:        for quintuple in tokens:
57:            type, value, start, end, line_text = quintuple
58:            if start != (lineno, column):
59:                assert (lineno, column) <= start, ((lineno, column), start)
60:                s_lineno, s_column = start
61:                if lineno < s_lineno:
62:                    prefix += "\n" * (s_lineno - lineno)
63:                    lineno = s_lineno
64:                    column = 0
65:                if column < s_column:
66:                    prefix += line_text[column:s_column]
67:                    column = s_column
68:            if type in (tokenize.COMMENT, tokenize.NL):
69:                prefix += value
70:                lineno, column = end
71:                if value.endswith("\n"):
72:                    lineno += 1
73:                    column = 0
74:                continue
75:            if type == token.OP:
76:                type = grammar.opmap[value]
77:            if debug:
78:                self.logger.debug("%s %r (prefix=%r)",
79:                                  token.tok_name[type], value, prefix)
80:            if type == token.INDENT:
81:                indent_columns.append(len(value))
82:                _prefix = prefix + value
83:                prefix = ""
84:                value = ""
85:            elif type == token.DEDENT:
86:                _indent_col = indent_columns.pop()
87:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
88:            if p.addtoken(type, value, (prefix, start)):
89:                if debug:
90:                    self.logger.debug("Stop.")
91:                break
92:            prefix = ""
93:            if type in {token.INDENT, token.DEDENT}:
94:                prefix = _prefix
95:            lineno, column = end
96:            if value.endswith("\n"):
97:                lineno += 1
98:                column = 0
99:        else:
100:            # We never broke out -- EOF is too soon (how can this happen???)
101:            raise parse.ParseError("incomplete input",
102:                                   type, value, (prefix, start))
103:        return p.rootnode
104:
105:    def parse_stream_raw(self, stream, debug=False):
106:        """Parse a stream and return the syntax tree."""
107:        tokens = tokenize.generate_tokens(stream.readline, config=self.tokenizer_config)
108:        return self.parse_tokens(tokens, debug)
109:
110:    def parse_stream(self, stream, debug=False):
111:        """Parse a stream and return the syntax tree."""
112:        return self.parse_stream_raw(stream, debug)
113:
114:    def parse_file(self, filename, encoding=None, debug=False):
115:        """Parse a file and return the syntax tree."""
116:        with io.open(filename, "r", encoding=encoding) as stream:
117:            return self.parse_stream(stream, debug)
118:
119:    def parse_string(self, text, debug=False):
120:        """Parse a string and return the syntax tree."""
121:        tokens = tokenize.generate_tokens(
122:            io.StringIO(text).readline,
123:            config=self.tokenizer_config,
124:        )
125:        return self.parse_tokens(tokens, debug)
126:
127:    def _partially_consume_prefix(self, prefix, column):
128:        lines = []
129:        current_line = ""
130:        current_column = 0
131:        wait_for_nl = False
132:        for char in prefix:
133:            current_line += char
134:            if wait_for_nl:
135:                if char == '\n':
136:                    if current_line.strip() and current_column < column:
137:                        res = ''.join(lines)
138:                        return res, prefix[len(res):]
139:
140:                    lines.append(current_line)

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
1:# Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006 Python Software Foundation.
2:# All rights reserved.
3:
4:"""Tokenization help for Python programs.
5:
6:generate_tokens(readline) is a generator that breaks a stream of
7:text into Python tokens.  It accepts a readline-like method which is called
8:repeatedly to get the next line of input (or "" for EOF).  It generates
9:5-tuples with these members:
10:
11:    the token type (see token.py)
12:    the token (a string)
13:    the starting (row, column) indices of the token (a 2-tuple of ints)
14:    the ending (row, column) indices of the token (a 2-tuple of ints)
15:    the original line (string)
16:
17:It is designed to match the working of the Python tokenizer exactly, except
18:that it produces COMMENT tokens for comments and gives type OP for all
19:operators
20:
21:Older entry points
22:    tokenize_loop(readline, tokeneater)
23:    tokenize(readline, tokeneater=printtoken)
24:are the same, except instead of generating tokens, tokeneater is a callback
25:function to which the 5 fields described above are passed as 5 arguments,
26:each time a new token is found."""
27:
28:__author__ = 'Ka-Ping Yee <ping@lfw.org>'
29:__credits__ = \
30:    'GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro'
31:
32:import re
33:from codecs import BOM_UTF8, lookup
34:from attr import dataclass
35:from blib2to3.pgen2.token import *
36:
37:from . import token
38:__all__ = [x for x in dir(token) if x[0] != '_'] + ["tokenize",
39:           "generate_tokens", "untokenize"]
40:del token
41:
42:try:
43:    bytes
44:except NameError:
45:    # Support bytes type in Python <= 2.5, so 2to3 turns itself into
46:    # valid Python 3 code.
47:    bytes = str
48:
49:def group(*choices): return '(' + '|'.join(choices) + ')'
50:def any(*choices): return group(*choices) + '*'
51:def maybe(*choices): return group(*choices) + '?'
52:def _combinations(*l):
53:    return set(
54:        x + y for x in l for y in l + ("",) if x.casefold() != y.casefold()
55:    )
56:
57:Whitespace = r'[ \f\t]*'
58:Comment = r'#[^\r\n]*'
59:Ignore = Whitespace + any(r'\\\r?\n' + Whitespace) + maybe(Comment)
60:Name = r'\w+'  # this is invalid but it's fine because Name comes after Number in all groups
61:
62:Binnumber = r'0[bB]_?[01]+(?:_[01]+)*'
63:Hexnumber = r'0[xX]_?[\da-fA-F]+(?:_[\da-fA-F]+)*[lL]?'
64:Octnumber = r'0[oO]?_?[0-7]+(?:_[0-7]+)*[lL]?'
65:Decnumber = group(r'[1-9]\d*(?:_\d+)*[lL]?', '0[lL]?')
66:Intnumber = group(Binnumber, Hexnumber, Octnumber, Decnumber)
67:Exponent = r'[eE][-+]?\d+(?:_\d+)*'
68:Pointfloat = group(r'\d+(?:_\d+)*\.(?:\d+(?:_\d+)*)?', r'\.\d+(?:_\d+)*') + maybe(Exponent)
69:Expfloat = r'\d+(?:_\d+)*' + Exponent
70:Floatnumber = group(Pointfloat, Expfloat)
71:Imagnumber = group(r'\d+(?:_\d+)*[jJ]', Floatnumber + r'[jJ]')
72:Number = group(Imagnumber, Floatnumber, Intnumber)
73:
74:# Tail end of ' string.
75:Single = r"[^'\\]*(?:\\.[^'\\]*)*'"
76:# Tail end of " string.
77:Double = r'[^"\\]*(?:\\.[^"\\]*)*"'
78:# Tail end of ''' string.
79:Single3 = r"[^'\\]*(?:(?:\\.|'(?!''))[^'\\]*)*'''"
80:# Tail end of """ string.
81:Double3 = r'[^"\\]*(?:(?:\\.|"(?!""))[^"\\]*)*"""'
82:_litprefix = r"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?"
83:Triple = group(_litprefix + "'''", _litprefix + '"""')
84:# Single-line ' or " string.
85:String = group(_litprefix + r"'[^\n'\\]*(?:\\.[^\n'\\]*)*'",
86:               _litprefix + r'"[^\n"\\]*(?:\\.[^\n"\\]*)*"')
87:
88:# Because of leftmost-then-longest match semantics, be sure to put the
89:# longest operators first (e.g., if = came before ==, == would get
90:# recognized as two instances of =).
91:Operator = group(r"\*\*=?", r">>=?", r"<<=?", r"<>", r"!=",
92:                 r"//=?", r"->",
93:                 r"[+\-*/%&@|^=<>]=?",
94:                 r"~")
95:
96:Bracket = '[][(){}]'
97:Special = group(r'\r?\n', r'[:;.,`@]')
98:Funny = group(Operator, Bracket, Special)
99:
100:PlainToken = group(Number, Funny, String, Name)
101:Token = Ignore + PlainToken
102:
103:# First (or only) line of ' or " string.
104:ContStr = group(_litprefix + r"'[^\n'\\]*(?:\\.[^\n'\\]*)*" +
105:                group("'", r'\\\r?\n'),
106:                _litprefix + r'"[^\n"\\]*(?:\\.[^\n"\\]*)*' +
107:                group('"', r'\\\r?\n'))
108:PseudoExtras = group(r'\\\r?\n', Comment, Triple)
109:PseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)
110:
111:tokenprog = re.compile(Token, re.UNICODE)
112:pseudoprog = re.compile(PseudoToken, re.UNICODE)
113:single3prog = re.compile(Single3)
114:double3prog = re.compile(Double3)
115:
116:_strprefixes = (
117:    _combinations('r', 'R', 'f', 'F') |
118:    _combinations('r', 'R', 'b', 'B') |
119:    {'u', 'U', 'ur', 'uR', 'Ur', 'UR'}
120:)
121:
122:endprogs = {"'": re.compile(Single), '"': re.compile(Double),
123:            "'''": single3prog, '"""': double3prog,
124:            **{f"{prefix}'''": single3prog for prefix in _strprefixes},
125:            **{f'{prefix}"""': double3prog for prefix in _strprefixes},
126:            **{prefix: None for prefix in _strprefixes}}
127:
128:triple_quoted = (
129:    {"'''", '"""'} |
130:    {f"{prefix}'''" for prefix in _strprefixes} |
131:    {f'{prefix}"""' for prefix in _strprefixes}
132:)
133:single_quoted = (
134:    {"'", '"'} |
135:    {f"{prefix}'" for prefix in _strprefixes} |
136:    {f'{prefix}"' for prefix in _strprefixes}
137:)
138:
139:tabsize = 8
140:
141:@dataclass(frozen=True)
142:class TokenizerConfig:
143:    async_is_reserved_keyword: bool = False
144:
145:class TokenError(Exception): pass
146:
147:class StopTokenizing(Exception): pass
148:
149:def printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line): # for testing
150:    (srow, scol) = xxx_todo_changeme
151:    (erow, ecol) = xxx_todo_changeme1
152:    print("%d,%d-%d,%d:\t%s\t%s" % \
153:        (srow, scol, erow, ecol, tok_name[type], repr(token)))
154:
155:def tokenize(readline, tokeneater=printtoken):
156:    """
157:    The tokenize() function accepts two parameters: one representing the
158:    input stream, and one providing an output mechanism for tokenize().
159:
160:    The first parameter, readline, must be a callable object which provides
161:    the same interface as the readline() method of built-in file objects.
162:    Each call to the function should return one line of input as a string.
163:
164:    The second parameter, tokeneater, must also be a callable object. It is
165:    called once for each token, with five arguments, corresponding to the
166:    tuples generated by generate_tokens().
167:    """
168:    try:
169:        tokenize_loop(readline, tokeneater)
170:    except StopTokenizing:
171:        pass
172:
173:# backwards compatible interface
174:def tokenize_loop(readline, tokeneater):
175:    for token_info in generate_tokens(readline):
176:        tokeneater(*token_info)
177:
178:class Untokenizer:
179:
180:    def __init__(self):
181:        self.tokens = []
182:        self.prev_row = 1
183:        self.prev_col = 0
184:
185:    def add_whitespace(self, start):
186:        row, col = start
187:        assert row <= self.prev_row
188:        col_offset = col - self.prev_col
189:        if col_offset:
190:            self.tokens.append(" " * col_offset)
191:
192:    def untokenize(self, iterable):
193:        for t in iterable:
194:            if len(t) == 2:
195:                self.compat(t, iterable)
196:                break
197:            tok_type, token, start, end, line = t
198:            self.add_whitespace(start)
199:            self.tokens.append(token)
200:            self.prev_row, self.prev_col = end
201:            if tok_type in (NEWLINE, NL):
202:                self.prev_row += 1
203:                self.prev_col = 0
204:        return "".join(self.tokens)
205:
206:    def compat(self, token, iterable):
207:        startline = False
208:        indents = []
209:        toks_append = self.tokens.append
210:        toknum, tokval = token
211:        if toknum in (NAME, NUMBER):
212:            tokval += ' '
213:        if toknum in (NEWLINE, NL):
214:            startline = True
215:        for tok in iterable:
216:            toknum, tokval = tok[:2]
217:
218:            if toknum in (NAME, NUMBER, ASYNC, AWAIT):
219:                tokval += ' '
220:
221:            if toknum == INDENT:
222:                indents.append(tokval)
223:                continue
224:            elif toknum == DEDENT:
225:                indents.pop()
226:                continue
227:            elif toknum in (NEWLINE, NL):
228:                startline = True
229:            elif startline and indents:
230:                toks_append(indents[-1])
231:                startline = False
232:            toks_append(tokval)
233:
234:cookie_re = re.compile(r'^[ \t\f]*#.*?coding[:=][ \t]*([-\w.]+)', re.ASCII)
235:blank_re = re.compile(br'^[ \t\f]*(?:[#\r\n]|$)', re.ASCII)
236:
237:def _get_normal_name(orig_enc):
238:    """Imitates get_normal_name in tokenizer.c."""
239:    # Only care about the first 12 characters.
240:    enc = orig_enc[:12].lower().replace("_", "-")
241:    if enc == "utf-8" or enc.startswith("utf-8-"):
242:        return "utf-8"
243:    if enc in ("latin-1", "iso-8859-1", "iso-latin-1") or \
244:       enc.startswith(("latin-1-", "iso-8859-1-", "iso-latin-1-")):
245:        return "iso-8859-1"
246:    return orig_enc
247:
248:def detect_encoding(readline):
249:    """
250:    The detect_encoding() function is used to detect the encoding that should
251:    be used to decode a Python source file. It requires one argument, readline,
252:    in the same way as the tokenize() generator.
253:
254:    It will call readline a maximum of twice, and return the encoding used
255:    (as a string) and a list of any lines (left as bytes) it has read
256:    in.
257:
258:    It detects the encoding from the presence of a utf-8 bom or an encoding
259:    cookie as specified in pep-0263. If both a bom and a cookie are present, but
260:    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid
261:    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,
262:    'utf-8-sig' is returned.
263:
264:    If no encoding is specified, then the default of 'utf-8' will be returned.
265:    """
266:    bom_found = False
267:    encoding = None
268:    default = 'utf-8'
269:    def read_or_stop():
270:        try:
271:            return readline()
272:        except StopIteration:
273:            return bytes()
274:
275:    def find_cookie(line):
276:        try:
277:            line_string = line.decode('ascii')
278:        except UnicodeDecodeError:
279:            return None
280:        match = cookie_re.match(line_string)
281:        if not match:
282:            return None
283:        encoding = _get_normal_name(match.group(1))
284:        try:
285:            codec = lookup(encoding)
286:        except LookupError:
287:            # This behaviour mimics the Python interpreter
288:            raise SyntaxError("unknown encoding: " + encoding)
289:
290:        if bom_found:
291:            if codec.name != 'utf-8':
292:                # This behaviour mimics the Python interpreter
293:                raise SyntaxError('encoding problem: utf-8')
294:            encoding += '-sig'
295:        return encoding
296:
297:    first = read_or_stop()
298:    if first.startswith(BOM_UTF8):
299:        bom_found = True
300:        first = first[3:]
301:        default = 'utf-8-sig'
302:    if not first:
303:        return default, []
304:
305:    encoding = find_cookie(first)
306:    if encoding:
307:        return encoding, [first]
308:    if not blank_re.match(first):
309:        return default, [first]
310:
311:    second = read_or_stop()
312:    if not second:
313:        return default, [first]
314:
315:    encoding = find_cookie(second)
316:    if encoding:
317:        return encoding, [first, second]
318:
319:    return default, [first, second]
320:
321:def untokenize(iterable):
322:    """Transform tokens back into Python source code.
323:
324:    Each element returned by the iterable must be a token sequence
325:    with at least two elements, a token number and token value.  If
326:    only two tokens are passed, the resulting output is poor.
327:
328:    Round-trip invariant for full input:
329:        Untokenized source will match input source exactly
330:
331:    Round-trip invariant for limited intput:
332:        # Output text will tokenize the back to the input
333:        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
334:        newcode = untokenize(t1)
335:        readline = iter(newcode.splitlines(1)).next
336:        t2 = [tok[:2] for tokin generate_tokens(readline)]
337:        assert t1 == t2
338:    """
339:    ut = Untokenizer()
340:    return ut.untokenize(iterable)
341:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
343:    """
344:    The generate_tokens() generator requires one argument, readline, which
345:    must be a callable object which provides the same interface as the
346:    readline() method of built-in file objects. Each call to the function
347:    should return one line of input as a string.  Alternately, readline
348:    can be a callable function terminating with StopIteration:
349:        readline = open(myfile).next    # Example of alternate readline
350:
351:    The generator produces 5-tuples with these members: the token type; the
352:    token string; a 2-tuple (srow, scol) of ints specifying the row and
353:    column where the token begins in the source; a 2-tuple (erow, ecol) of
354:    ints specifying the row and column where the token ends in the source;
355:    and the line on which the token was found. The line passed is the
356:    logical line; continuation lines are included.
357:    """
358:    lnum = parenlev = continued = 0
359:    numchars = '0123456789'
360:    contstr, needcont = '', 0
361:    contline = None
362:    indents = [0]
363:
364:    # If we know we're parsing 3.7+, we can unconditionally parse `async` and
365:    # `await` as keywords.
366:    async_is_reserved_keyword = config.async_is_reserved_keyword
367:    # 'stashed' and 'async_*' are used for async/await parsing
368:    stashed = None
369:    async_def = False
370:    async_def_indent = 0
371:    async_def_nl = False
372:
373:    while 1:                                   # loop over lines in stream
374:        try:
375:            line = readline()
376:        except StopIteration:
377:            line = ''
378:        lnum = lnum + 1
379:        pos, max = 0, len(line)
380:
381:        if contstr:                            # continued string
382:            if not line:
383:                raise TokenError("EOF in multi-line string", strstart)
384:            endmatch = endprog.match(line)
385:            if endmatch:
386:                pos = end = endmatch.end(0)
387:                yield (STRING, contstr + line[:end],
388:                       strstart, (lnum, end), contline + line)
389:                contstr, needcont = '', 0
390:                contline = None
391:            elif needcont and line[-2:] != '\\\n' and line[-3:] != '\\\r\n':
392:                yield (ERRORTOKEN, contstr + line,
393:                           strstart, (lnum, len(line)), contline)
394:                contstr = ''
395:                contline = None
396:                continue
397:            else:
398:                contstr = contstr + line
399:                contline = contline + line
400:                continue
401:
402:        elif parenlev == 0 and not continued:  # new statement
403:            if not line: break
404:            column = 0
405:            while pos < max:                   # measure leading whitespace
406:                if line[pos] == ' ': column = column + 1
407:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize
408:                elif line[pos] == '\f': column = 0
409:                else: break
410:                pos = pos + 1
411:            if pos == max: break
412:
413:            if stashed:
414:                yield stashed
415:                stashed = None
416:
417:            if line[pos] in '\r\n':            # skip blank lines
418:                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)
419:                continue
420:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
420:
421:            if line[pos] == '#':               # skip comments
422:                comment_token = line[pos:].rstrip('\r\n')
423:                nl_pos = pos + len(comment_token)
424:                yield (COMMENT, comment_token,
425:                        (lnum, pos), (lnum, pos + len(comment_token)), line)
426:                yield (NL, line[nl_pos:],
427:                        (lnum, nl_pos), (lnum, len(line)), line)
428:                continue
429:
430:            if column > indents[-1]:           # count indents
431:                indents.append(column)
432:                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)
433:
434:            while column < indents[-1]:        # count dedents
435:                if column not in indents:
436:                    raise IndentationError(
437:                        "unindent does not match any outer indentation level",
438:                        ("<tokenize>", lnum, pos, line))
439:                indents = indents[:-1]
440:
441:                if async_def and async_def_indent >= indents[-1]:
442:                    async_def = False
443:                    async_def_nl = False
444:                    async_def_indent = 0
445:
446:                yield (DEDENT, '', (lnum, pos), (lnum, pos), line)
447:
448:            if async_def and async_def_nl and async_def_indent >= indents[-1]:
449:                async_def = False
450:                async_def_nl = False
451:                async_def_indent = 0
452:
453:        else:                                  # continued statement
454:            if not line:
455:                raise TokenError("EOF in multi-line statement", (lnum, 0))
456:            continued = 0
457:
458:        while pos < max:
459:            pseudomatch = pseudoprog.match(line, pos)
460:            if pseudomatch:                                # scan for tokens
461:                start, end = pseudomatch.span(1)
462:                spos, epos, pos = (lnum, start), (lnum, end), end
463:                token, initial = line[start:end], line[start]
464:
465:                if initial in numchars or \
466:                   (initial == '.' and token != '.'):      # ordinary number
467:                    yield (NUMBER, token, spos, epos, line)
468:                elif initial in '\r\n':
469:                    newline = NEWLINE
470:                    if parenlev > 0:
471:                        newline = NL
472:                    elif async_def:
473:                        async_def_nl = True
474:                    if stashed:
475:                        yield stashed
476:                        stashed = None
477:                    yield (newline, token, spos, epos, line)
478:
479:                elif initial == '#':
480:                    assert not token.endswith("\n")
481:                    if stashed:
482:                        yield stashed
483:                        stashed = None
484:                    yield (COMMENT, token, spos, epos, line)
485:                elif token in triple_quoted:
486:                    endprog = endprogs[token]
487:                    endmatch = endprog.match(line, pos)
488:                    if endmatch:                           # all on one line
489:                        pos = endmatch.end(0)
490:                        token = line[start:pos]
491:                        if stashed:
492:                            yield stashed
493:                            stashed = None
494:                        yield (STRING, token, spos, (lnum, pos), line)
495:                    else:
496:                        strstart = (lnum, start)           # multiple lines
497:                        contstr = line[start:]
498:                        contline = line
499:                        break
500:                elif initial in single_quoted or \
501:                    token[:2] in single_quoted or \
502:                    token[:3] in single_quoted:
503:                    if token[-1] == '\n':                  # continued string
504:                        strstart = (lnum, start)
505:                        endprog = (endprogs[initial] or endprogs[token[1]] or
506:                                   endprogs[token[2]])
507:                        contstr, needcont = line[start:], 1
508:                        contline = line
509:                        break
510:                    else:                                  # ordinary string
511:                        if stashed:
512:                            yield stashed
513:                            stashed = None
514:                        yield (STRING, token, spos, epos, line)
515:                elif initial.isidentifier():               # ordinary name
516:                    if token in ('async', 'await'):
517:                        if async_is_reserved_keyword or async_def:
518:                            yield (ASYNC if token == 'async' else AWAIT,
519:                                   token, spos, epos, line)
520:                            continue
521:
522:                    tok = (NAME, token, spos, epos, line)
523:                    if token == 'async' and not stashed:
524:                        stashed = tok
525:                        continue
526:
527:                    if token in ('def', 'for'):
528:                        if (stashed
529:                                and stashed[0] == NAME
530:                                and stashed[1] == 'async'):
531:
532:                            if token == 'def':
533:                                async_def = True
534:                                async_def_indent = indents[-1]
535:
536:                            yield (ASYNC, stashed[1],
537:                                   stashed[2], stashed[3],
538:                                   stashed[4])
539:                            stashed = None
540:
541:                    if stashed:
542:                        yield stashed
543:                        stashed = None
544:
545:                    yield tok
546:                elif initial == '\\':                      # continued stmt
547:                    # This yield is new; needed for better idempotency:
548:                    if stashed:
549:                        yield stashed
550:                        stashed = None
551:                    yield (NL, token, spos, (lnum, pos), line)
552:                    continued = 1
553:                else:
554:                    if initial in '([{': parenlev = parenlev + 1
555:                    elif initial in ')]}': parenlev = parenlev - 1
556:                    if stashed:
557:                        yield stashed
558:                        stashed = None
559:                    yield (OP, token, spos, epos, line)
560:            else:
561:                yield (ERRORTOKEN, line[pos],
562:                           (lnum, pos), (lnum, pos+1), line)
563:                pos = pos + 1
564:
565:    if stashed:
566:        yield stashed
567:        stashed = None
568:
569:    for indent in indents[1:]:                 # pop remaining indent levels
570:        yield (DEDENT, '', (lnum, 0), (lnum, 0), '')
571:    yield (ENDMARKER, '', (lnum, 0), (lnum, 0), '')
572:
573:if __name__ == '__main__':                     # testing
574:    import sys
575:    if len(sys.argv) > 1: tokenize(open(sys.argv[1]).readline)
576:    else: tokenize(sys.stdin.readline)

Calling tool: grepTool
test_async_as_identifier
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/coverage_bugsinpy.txt:1:coverage run -m unittest -q tests.test_black.BlackTestCase.test_async_as_identifier
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:506:    def test_async_as_identifier(self) -> None:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/bugsinpy_run_test.sh:1:python -m unittest -q tests.test_black.BlackTestCase.test_async_as_identifier

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py
480:    @patch("black.dump_to_file", dump_to_stderr)
481:    def test_python2_print_function(self) -> None:
482:        source, expected = read_data("python2_print_function")
483:        mode = black.FileMode(target_versions={TargetVersion.PY27})
484:        actual = fs(source, mode=mode)
485:        self.assertFormatEqual(expected, actual)
486:        black.assert_equivalent(source, actual)
487:        black.assert_stable(source, actual, mode)
488:
489:    @patch("black.dump_to_file", dump_to_stderr)
490:    def test_python2_unicode_literals(self) -> None:
491:        source, expected = read_data("python2_unicode_literals")
492:        actual = fs(source)
493:        self.assertFormatEqual(expected, actual)
494:        black.assert_equivalent(source, actual)
495:        black.assert_stable(source, actual, black.FileMode())
496:
497:    @patch("black.dump_to_file", dump_to_stderr)
498:    def test_stub(self) -> None:
499:        mode = black.FileMode(is_pyi=True)
500:        source, expected = read_data("stub.pyi")
501:        actual = fs(source, mode=mode)
502:        self.assertFormatEqual(expected, actual)
503:        black.assert_stable(source, actual, mode)
504:
505:    @patch("black.dump_to_file", dump_to_stderr)
506:    def test_async_as_identifier(self) -> None:
507:        source_path = (THIS_DIR / "data" / "async_as_identifier.py").resolve()
508:        source, expected = read_data("async_as_identifier")
509:        actual = fs(source)
510:        self.assertFormatEqual(expected, actual)
511:        major, minor = sys.version_info[:2]
512:        if major < 3 or (major <= 3 and minor < 7):
513:            black.assert_equivalent(source, actual)
514:        black.assert_stable(source, actual, black.FileMode())
515:        # ensure black can parse this when the target is 3.6
516:        self.invokeBlack([str(source_path), "--target-version", "py36"])
517:        # but not on 3.7, because async/await is no longer an identifier
518:        self.invokeBlack([str(source_path), "--target-version", "py37"], exit_code=123)
519:
520:    @patch("black.dump_to_file", dump_to_stderr)
521:    def test_python37(self) -> None:
522:        source_path = (THIS_DIR / "data" / "python37.py").resolve()
523:        source, expected = read_data("python37")
524:        actual = fs(source)
525:        self.assertFormatEqual(expected, actual)
526:        major, minor = sys.version_info[:2]
527:        if major > 3 or (major == 3 and minor >= 7):
528:            black.assert_equivalent(source, actual)
529:        black.assert_stable(source, actual, black.FileMode())
530:        # ensure black can parse this when the target is 3.7
531:        self.invokeBlack([str(source_path), "--target-version", "py37"])
532:        # but not on 3.6, because we use async as a reserved keyword
533:        self.invokeBlack([str(source_path), "--target-version", "py36"], exit_code=123)
534:
535:    @patch("black.dump_to_file", dump_to_stderr)
536:    def test_fmtonoff(self) -> None:
537:        source, expected = read_data("fmtonoff")
538:        actual = fs(source)
539:        self.assertFormatEqual(expected, actual)
540:        black.assert_equivalent(source, actual)
541:        black.assert_stable(source, actual, black.FileMode())
542:
543:    @patch("black.dump_to_file", dump_to_stderr)
544:    def test_fmtonoff2(self) -> None:
545:        source, expected = read_data("fmtonoff2")
546:        actual = fs(source)
547:        self.assertFormatEqual(expected, actual)
548:        black.assert_equivalent(source, actual)
549:        black.assert_stable(source, actual, black.FileMode())
550:
551:    @patch("black.dump_to_file", dump_to_stderr)
552:    def test_remove_empty_parentheses_after_class(self) -> None:
553:        source, expected = read_data("class_blank_parentheses")
554:        actual = fs(source)
555:        self.assertFormatEqual(expected, actual)
556:        black.assert_equivalent(source, actual)
557:        black.assert_stable(source, actual, black.FileMode())
558:
559:    @patch("black.dump_to_file", dump_to_stderr)
560:    def test_new_line_between_class_and_code(self) -> None:
561:        source, expected = read_data("class_methods_new_line")
562:        actual = fs(source)
563:        self.assertFormatEqual(expected, actual)
564:        black.assert_equivalent(source, actual)
565:        black.assert_stable(source, actual, black.FileMode())
566:
567:    @patch("black.dump_to_file", dump_to_stderr)
568:    def test_bracket_match(self) -> None:
569:        source, expected = read_data("bracketmatch")
570:        actual = fs(source)
571:        self.assertFormatEqual(expected, actual)
572:        black.assert_equivalent(source, actual)
573:        black.assert_stable(source, actual, black.FileMode())
574:
575:    @patch("black.dump_to_file", dump_to_stderr)
576:    def test_tuple_assign(self) -> None:
577:        source, expected = read_data("tupleassign")
578:        actual = fs(source)
579:        self.assertFormatEqual(expected, actual)
580:        black.assert_equivalent(source, actual)
581:        black.assert_stable(source, actual, black.FileMode())
582:
583:    def test_tab_comment_indentation(self) -> None:
584:        contents_tab = "if 1:\n\tif 2:\n\t\tpass\n\t# comment\n\tpass\n"
585:        contents_spc = "if 1:\n    if 2:\n        pass\n    # comment\n    pass\n"
586:        self.assertFormatEqual(contents_spc, fs(contents_spc))
587:        self.assertFormatEqual(contents_spc, fs(contents_tab))
588:
589:        contents_tab = "if 1:\n\tif 2:\n\t\tpass\n\t\t# comment\n\tpass\n"
590:        contents_spc = "if 1:\n    if 2:\n        pass\n        # comment\n    pass\n"
591:        self.assertFormatEqual(contents_spc, fs(contents_spc))
592:        self.assertFormatEqual(contents_spc, fs(contents_tab))
593:
594:        # mixed tabs and spaces (valid Python 2 code)
595:        contents_tab = "if 1:\n        if 2:\n\t\tpass\n\t# comment\n        pass\n"
596:        contents_spc = "if 1:\n    if 2:\n        pass\n    # comment\n    pass\n"
597:        self.assertFormatEqual(contents_spc, fs(contents_spc))
598:        self.assertFormatEqual(contents_spc, fs(contents_tab))
599:
600:        contents_tab = "if 1:\n        if 2:\n\t\tpass\n\t\t# comment\n        pass\n"
601:        contents_spc = "if 1:\n    if 2:\n        pass\n        # comment\n    pass\n"
602:        self.assertFormatEqual(contents_spc, fs(contents_spc))
603:        self.assertFormatEqual(contents_spc, fs(contents_tab))
604:
605:    def test_report_verbose(self) -> None:
606:        report = black.Report(verbose=True)
607:        out_lines = []
608:        err_lines = []
609:
610:        def out(msg: str, **kwargs: Any) -> None:
611:            out_lines.append(msg)
612:
613:        def err(msg: str, **kwargs: Any) -> None:
614:            err_lines.append(msg)
615:
616:        with patch("black.out", out), patch("black.err", err):
617:            report.done(Path("f1"), black.Changed.NO)
618:            self.assertEqual(len(out_lines), 1)
619:            self.assertEqual(len(err_lines), 0)

Calling tool: grepTool
async_as_identifier.py
No files found. Call grepTool again with a more generic pattern.

Calling tool: globTool
**/tests/data/async_as_identifier*
Matched files:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/data/async_as_identifier.py

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/data/async_as_identifier.py
1:def async():
2:    pass
3:
4:
5:def await():
6:    pass
7:
8:
9:await = lambda: None
10:async = lambda: None
11:async()
12:await()
13:
14:
15:def sync_fn():
16:    await = lambda: None
17:    async = lambda: None
18:    async()
19:    await()
20:
21:
22:async def async_fn():
23:    await async_fn()
24:
25:
26:# output
27:def async():
28:    pass
29:
30:
31:def await():
32:    pass
33:
34:
35:await = lambda: None
36:async = lambda: None
37:async()
38:await()
39:
40:
41:def sync_fn():
42:    await = lambda: None
43:    async = lambda: None
44:    async()
45:    await()
46:
47:
48:async def async_fn():
49:    await async_fn()

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/data/python37.py
1:#!/usr/bin/env python3.7
2:
3:
4:def f():
5:    return (i * 2 async for i in arange(42))
6:
7:
8:def g():
9:    return (
10:        something_long * something_long
11:        async for something_long in async_generator(with_an_argument)
12:    )
13:
14:
15:async def func():
16:    if test:
17:        out_batched = [
18:            i
19:            async for i in aitertools._async_map(
20:                self.async_inc, arange(8), batch_size=3
21:            )
22:        ]
23:
24:
25:def awaited_generator_value(n):
26:    return (await awaitable for awaitable in awaitable_list)
27:
28:
29:def make_arange(n):
30:    return (i * 2 for i in range(n) if await wrap(i))
31:
32:
33:# output
34:
35:
36:#!/usr/bin/env python3.7
37:
38:
39:def f():
40:    return (i * 2 async for i in arange(42))
41:
42:
43:def g():
44:    return (
45:        something_long * something_long
46:        async for something_long in async_generator(with_an_argument)
47:    )
48:
49:
50:async def func():
51:    if test:
52:        out_batched = [
53:            i
54:            async for i in aitertools._async_map(
55:                self.async_inc, arange(8), batch_size=3
56:            )
57:        ]
58:
59:
60:def awaited_generator_value(n):
61:    return (await awaitable for awaitable in awaitable_list)
62:
63:
64:def make_arange(n):
65:    return (i * 2 for i in range(n) if await wrap(i))

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi
1:# Stubs for lib2to3.pgen2.tokenize (Python 3.6)
2:# NOTE: Only elements from __all__ are present.
3:
4:from typing import Callable, Iterable, Iterator, List, Text, Tuple
5:from attr import dataclass
6:from blib2to3.pgen2.token import *  # noqa
7:
8:
9:_Coord = Tuple[int, int]
10:_TokenEater = Callable[[int, Text, _Coord, _Coord, Text], None]
11:_TokenInfo = Tuple[int, Text, _Coord, _Coord, Text]
12:
13:@dataclass(frozen=True)
14:class TokenizerConfig:
15:    async_is_reserved_keyword: bool = False
16:
17:class TokenError(Exception): ...
18:class StopTokenizing(Exception): ...
19:
20:def tokenize(readline: Callable[[], Text], tokeneater: _TokenEater = ...) -> None: ...
21:
22:class Untokenizer:
23:    tokens: List[Text]
24:    prev_row: int
25:    prev_col: int
26:    def __init__(self) -> None: ...
27:    def add_whitespace(self, start: _Coord) -> None: ...
28:    def untokenize(self, iterable: Iterable[_TokenInfo]) -> Text: ...
29:    def compat(self, token: Tuple[int, Text], iterable: Iterable[_TokenInfo]) -> None: ...
30:
31:def untokenize(iterable: Iterable[_TokenInfo]) -> Text: ...
32:def generate_tokens(
33:    readline: Callable[[], Text]
34:) -> Iterator[_TokenInfo]: ...

Calling tool: grepTool
generate_tokens\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:6:generate_tokens(readline) is a generator that breaks a stream of
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:166:    tuples generated by generate_tokens().
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:175:    for token_info in generate_tokens(readline):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:333:        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:336:        t2 = [tok[:2] for tokin generate_tokens(readline)]
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:344:    The generate_tokens() generator requires one argument, readline, which
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py:107:        tokens = tokenize.generate_tokens(stream.readline, config=self.tokenizer_config)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py:121:        tokens = tokenize.generate_tokens(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi:32:def generate_tokens(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/pgen.py:19:        self.generator = tokenize.generate_tokens(stream.readline)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.py:6:generate_tokens(readline) is a generator that breaks a stream of
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.py:161:    tuples generated by generate_tokens().
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.py:170:    for token_info in generate_tokens(readline):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.py:328:        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.py:331:        t2 = [tok[:2] for tokin generate_tokens(readline)]
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.py:337:def generate_tokens(readline):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.py:339:    The generate_tokens() generator requires one argument, readline, which
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/driver.py:100:        tokens = tokenize.generate_tokens(stream.readline)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/driver.py:114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.pyi:28:def generate_tokens(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/pgen.py:19:        self.generator = tokenize.generate_tokens(stream.readline)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/flake8/checker.py:578:        for token in file_processor.generate_tokens():
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/flake8/processor.py:120:                    tokenize.generate_tokens(lambda: next(line_iter))
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/flake8/processor.py:259:    def generate_tokens(self):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/flake8/processor.py:267:            for token in tokenize.generate_tokens(self.next_line):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/sphinx/pycode/parser.py:134:        self.tokens = tokenize.generate_tokens(lambda: next(lines))
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pycodestyle.py:2026:    def generate_tokens(self):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pycodestyle.py:2030:        tokengen = tokenize.generate_tokens(self.readline)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pycodestyle.py:2088:        for token in self.generate_tokens():
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/coverage/parser.py:138:        tokgen = generate_tokens(self.text)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/coverage/phystokens.py:21:    tokenize.generate_tokens() doesn't return a token for the backslash that
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/coverage/phystokens.py:25:    Returns the same values as generate_tokens()
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/coverage/phystokens.py:95:    tokgen = generate_tokens(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/coverage/phystokens.py:142:    def generate_tokens(self, text):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/coverage/phystokens.py:147:            self.last_tokens = list(tokenize.generate_tokens(readline))
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/babel/messages/extract.py:417:    tokens = generate_tokens(next_line)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/2/tokenize.pyi:117:def generate_tokens(readline: Callable[[], str]) -> Generator[_TokenType, None, None]: ...
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/3/tokenize.pyi:46:def generate_tokens(readline: Callable[[], str]) -> Generator[TokenInfo, None, None]: ...  # undocumented
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/2and3/lib2to3/pgen2/tokenize.pyi:28:def generate_tokens(

Calling tool: grepTool
get_grammars\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/black.py:751:def get_grammars(target_versions: Set[TargetVersion]) -> List[Grammar]:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/black.py:772:    for grammar in get_grammars(set(target_versions)):

Calling tool: grepTool
get_parser_configs\(|lib2to3_parse\(
Showing first 50 results from grep. Consider using a more specific pattern or path.
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:711:    src_node = lib2to3_parse(src_contents.lstrip(), mode.target_versions)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:821:def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:826:    for parser_config in get_parser_configs(set(target_versions)):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:919:            code = lib2to3_parse(code)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/data/debug_visitor.py:32:        list(v.visit(lib2to3_parse(code)))
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:147:                exp_node = black.lib2to3_parse(expected)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:154:                exp_node = black.lib2to3_parse(actual)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:877:    def test_lib2to3_parse(self) -> None:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:879:            black.lib2to3_parse("invalid syntax")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:882:        black.lib2to3_parse(straddling)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:883:        black.lib2to3_parse(straddling, {TargetVersion.PY27})
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:884:        black.lib2to3_parse(straddling, {TargetVersion.PY36})
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:885:        black.lib2to3_parse(straddling, {TargetVersion.PY27, TargetVersion.PY36})
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:888:        black.lib2to3_parse(py2_only)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:889:        black.lib2to3_parse(py2_only, {TargetVersion.PY27})
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:891:            black.lib2to3_parse(py2_only, {TargetVersion.PY36})
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:893:            black.lib2to3_parse(py2_only, {TargetVersion.PY27, TargetVersion.PY36})
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:896:        black.lib2to3_parse(py3_only)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:898:            black.lib2to3_parse(py3_only, {TargetVersion.PY27})
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:899:        black.lib2to3_parse(py3_only, {TargetVersion.PY36})
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:900:        black.lib2to3_parse(py3_only, {TargetVersion.PY27, TargetVersion.PY36})
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:903:        node = black.lib2to3_parse("def f(*, arg): ...\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:905:        node = black.lib2to3_parse("def f(*, arg,): ...\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:907:        node = black.lib2to3_parse("f(*arg,)\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:911:        node = black.lib2to3_parse("def f(*, arg): f'string'\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:913:        node = black.lib2to3_parse("123_456\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:915:        node = black.lib2to3_parse("123456\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:918:        node = black.lib2to3_parse(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:925:        node = black.lib2to3_parse(expected)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:928:        node = black.lib2to3_parse(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:930:        node = black.lib2to3_parse(expected)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:934:        node = black.lib2to3_parse("\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:936:        node = black.lib2to3_parse("from __future__ import black\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:938:        node = black.lib2to3_parse("from __future__ import multiple, imports\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:940:        node = black.lib2to3_parse("from __future__ import (parenthesized, imports)\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:942:        node = black.lib2to3_parse(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:946:        node = black.lib2to3_parse("# comment\nfrom __future__ import black\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:948:        node = black.lib2to3_parse('"""docstring"""\nfrom __future__ import black\n')
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:950:        node = black.lib2to3_parse("some(other, code)\nfrom __future__ import black\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:952:        node = black.lib2to3_parse("from some.module import black\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:954:        node = black.lib2to3_parse(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:958:        node = black.lib2to3_parse(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:1008:        n = black.lib2to3_parse("\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/tests/data/debug_visitor.py:32:        list(v.visit(lib2to3_parse(code)))
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/tests/test_black.py:147:                exp_node = black.lib2to3_parse(expected)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/tests/test_black.py:154:                exp_node = black.lib2to3_parse(actual)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/tests/test_black.py:857:    def test_lib2to3_parse(self) -> None:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/tests/test_black.py:859:            black.lib2to3_parse("invalid syntax")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/tests/test_black.py:862:        black.lib2to3_parse(straddling)

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py
860:            self.assertEqual(report.return_code, 123)
861:            report.done(Path("f4"), black.Changed.NO)
862:            self.assertEqual(len(out_lines), 2)
863:            self.assertEqual(len(err_lines), 2)
864:            self.assertEqual(
865:                unstyle(str(report)),
866:                "2 files reformatted, 3 files left unchanged, "
867:                "2 files failed to reformat.",
868:            )
869:            self.assertEqual(report.return_code, 123)
870:            report.check = True
871:            self.assertEqual(
872:                unstyle(str(report)),
873:                "2 files would be reformatted, 3 files would be left unchanged, "
874:                "2 files would fail to reformat.",
875:            )
876:
877:    def test_lib2to3_parse(self) -> None:
878:        with self.assertRaises(black.InvalidInput):
879:            black.lib2to3_parse("invalid syntax")
880:
881:        straddling = "x + y"
882:        black.lib2to3_parse(straddling)
883:        black.lib2to3_parse(straddling, {TargetVersion.PY27})
884:        black.lib2to3_parse(straddling, {TargetVersion.PY36})
885:        black.lib2to3_parse(straddling, {TargetVersion.PY27, TargetVersion.PY36})
886:
887:        py2_only = "print x"
888:        black.lib2to3_parse(py2_only)
889:        black.lib2to3_parse(py2_only, {TargetVersion.PY27})
890:        with self.assertRaises(black.InvalidInput):
891:            black.lib2to3_parse(py2_only, {TargetVersion.PY36})
892:        with self.assertRaises(black.InvalidInput):
893:            black.lib2to3_parse(py2_only, {TargetVersion.PY27, TargetVersion.PY36})
894:
895:        py3_only = "exec(x, end=y)"
896:        black.lib2to3_parse(py3_only)
897:        with self.assertRaises(black.InvalidInput):
898:            black.lib2to3_parse(py3_only, {TargetVersion.PY27})
899:        black.lib2to3_parse(py3_only, {TargetVersion.PY36})
900:        black.lib2to3_parse(py3_only, {TargetVersion.PY27, TargetVersion.PY36})
901:
902:    def test_get_features_used(self) -> None:
903:        node = black.lib2to3_parse("def f(*, arg): ...\n")
904:        self.assertEqual(black.get_features_used(node), set())
905:        node = black.lib2to3_parse("def f(*, arg,): ...\n")
906:        self.assertEqual(black.get_features_used(node), {Feature.TRAILING_COMMA_IN_DEF})
907:        node = black.lib2to3_parse("f(*arg,)\n")
908:        self.assertEqual(
909:            black.get_features_used(node), {Feature.TRAILING_COMMA_IN_CALL}
910:        )
911:        node = black.lib2to3_parse("def f(*, arg): f'string'\n")
912:        self.assertEqual(black.get_features_used(node), {Feature.F_STRINGS})
913:        node = black.lib2to3_parse("123_456\n")
914:        self.assertEqual(black.get_features_used(node), {Feature.NUMERIC_UNDERSCORES})
915:        node = black.lib2to3_parse("123456\n")
916:        self.assertEqual(black.get_features_used(node), set())
917:        source, expected = read_data("function")
918:        node = black.lib2to3_parse(source)
919:        expected_features = {
920:            Feature.TRAILING_COMMA_IN_CALL,
921:            Feature.TRAILING_COMMA_IN_DEF,
922:            Feature.F_STRINGS,
923:        }
924:        self.assertEqual(black.get_features_used(node), expected_features)
925:        node = black.lib2to3_parse(expected)
926:        self.assertEqual(black.get_features_used(node), expected_features)
927:        source, expected = read_data("expression")
928:        node = black.lib2to3_parse(source)
929:        self.assertEqual(black.get_features_used(node), set())
930:        node = black.lib2to3_parse(expected)
931:        self.assertEqual(black.get_features_used(node), set())
932:
933:    def test_get_future_imports(self) -> None:
934:        node = black.lib2to3_parse("\n")
935:        self.assertEqual(set(), black.get_future_imports(node))
936:        node = black.lib2to3_parse("from __future__ import black\n")
937:        self.assertEqual({"black"}, black.get_future_imports(node))
938:        node = black.lib2to3_parse("from __future__ import multiple, imports\n")
939:        self.assertEqual({"multiple", "imports"}, black.get_future_imports(node))
940:        node = black.lib2to3_parse("from __future__ import (parenthesized, imports)\n")
941:        self.assertEqual({"parenthesized", "imports"}, black.get_future_imports(node))
942:        node = black.lib2to3_parse(
943:            "from __future__ import multiple\nfrom __future__ import imports\n"
944:        )
945:        self.assertEqual({"multiple", "imports"}, black.get_future_imports(node))
946:        node = black.lib2to3_parse("# comment\nfrom __future__ import black\n")
947:        self.assertEqual({"black"}, black.get_future_imports(node))
948:        node = black.lib2to3_parse('"""docstring"""\nfrom __future__ import black\n')
949:        self.assertEqual({"black"}, black.get_future_imports(node))
950:        node = black.lib2to3_parse("some(other, code)\nfrom __future__ import black\n")
951:        self.assertEqual(set(), black.get_future_imports(node))
952:        node = black.lib2to3_parse("from some.module import black\n")
953:        self.assertEqual(set(), black.get_future_imports(node))
954:        node = black.lib2to3_parse(
955:            "from __future__ import unicode_literals as _unicode_literals"
956:        )
957:        self.assertEqual({"unicode_literals"}, black.get_future_imports(node))
958:        node = black.lib2to3_parse(
959:            "from __future__ import unicode_literals as _lol, print"
960:        )
961:        self.assertEqual({"unicode_literals", "print"}, black.get_future_imports(node))
962:
963:    def test_debug_visitor(self) -> None:
964:        source, _ = read_data("debug_visitor.py")
965:        expected, _ = read_data("debug_visitor.out")
966:        out_lines = []
967:        err_lines = []
968:
969:        def out(msg: str, **kwargs: Any) -> None:
970:            out_lines.append(msg)
971:
972:        def err(msg: str, **kwargs: Any) -> None:
973:            err_lines.append(msg)
974:
975:        with patch("black.out", out), patch("black.err", err):
976:            black.DebugVisitor.show(source)
977:        actual = "\n".join(out_lines) + "\n"
978:        log_name = ""
979:        if expected != actual:
980:            log_name = black.dump_to_file(*out_lines)
981:        self.assertEqual(
982:            expected,
983:            actual,
984:            f"AST print out is different. Actual version dumped to {log_name}",
985:        )
986:
987:    def test_format_file_contents(self) -> None:
988:        empty = ""
989:        mode = black.FileMode()
990:        with self.assertRaises(black.NothingChanged):
991:            black.format_file_contents(empty, mode=mode, fast=False)
992:        just_nl = "\n"
993:        with self.assertRaises(black.NothingChanged):
994:            black.format_file_contents(just_nl, mode=mode, fast=False)
995:        same = "l = [1, 2, 3]\n"
996:        with self.assertRaises(black.NothingChanged):
997:            black.format_file_contents(same, mode=mode, fast=False)
998:        different = "l = [1,2,3]"
999:        expected = same
1000:        actual = black.format_file_contents(different, mode=mode, fast=False)
1001:        self.assertEqual(expected, actual)
1002:        invalid = "return if you can"
1003:        with self.assertRaises(black.InvalidInput) as e:
1004:            black.format_file_contents(invalid, mode=mode, fast=False)
1005:        self.assertEqual(str(e.exception), "Cannot parse: 1:7: return if you can")
1006:
1007:    def test_endmarker(self) -> None:
1008:        n = black.lib2to3_parse("\n")
1009:        self.assertEqual(n.type, black.syms.file_input)
1010:        self.assertEqual(len(n.children), 1)
1011:        self.assertEqual(n.children[0].type, black.token.ENDMARKER)
1012:
1013:    @unittest.skipIf(os.environ.get("SKIP_AST_PRINT"), "user set SKIP_AST_PRINT")
1014:    def test_assertFormatEqual(self) -> None:
1015:        out_lines = []
1016:        err_lines = []
1017:
1018:        def out(msg: str, **kwargs: Any) -> None:
1019:            out_lines.append(msg)
1020:
1021:        def err(msg: str, **kwargs: Any) -> None:
1022:            err_lines.append(msg)
1023:
1024:        with patch("black.out", out), patch("black.err", err):
1025:            with self.assertRaises(AssertionError):
1026:                self.assertFormatEqual("l = [1, 2, 3]", "l = [1, 2, 3,]")
1027:
1028:        out_str = "".join(out_lines)
1029:        self.assertTrue("Expected tree:" in out_str)
1030:        self.assertTrue("Actual tree:" in out_str)
1031:        self.assertEqual("".join(err_lines), "")
1032:
1033:    def test_cache_broken_file(self) -> None:
1034:        mode = black.FileMode()
1035:        with cache_dir() as workspace:
1036:            cache_file = black.get_cache_file(mode)
1037:            with cache_file.open("w") as fobj:
1038:                fobj.write("this is not a pickle")
1039:            self.assertEqual(black.read_cache(mode), {})

Calling tool: createTodo
Todo List Created
Todo List (15 items):
- [ ] Verify Feature enum includes ASYNC_IS_VALID_IDENTIFIER and ASYNC_IS_RESERVED_KEYWORD and that VERSION_TO_FEATURES maps each TargetVersion (PY27, PY33, PY34, PY35, PY36, PY37, PY38) to exactly one of these two flags (mutually exclusive, never both, never neither). (ID: todo-1)
- [ ] Confirm black.ParserConfig dataclass exists with fields: grammar: Grammar and tokenizer_config: TokenizerConfig defaulting to TokenizerConfig(). Ensure TokenizerConfig is imported from blib2to3.pgen2.tokenize. (ID: todo-2)
- [ ] Validate get_parser_configs() logic and order for parsing attempts: (a) when target_versions is empty, returns four ParserConfig entries in this order: Python 3 grammar with TokenizerConfig(async_is_reserved_keyword=True), Python 3 grammar with TokenizerConfig(async_is_reserved_keyword=False), Python 2 no_print grammar, Python 2 grammar; (b) when all target_versions are Python 2, returns only the two Python 2 grammars (no TokenizerConfig overrides); (c) when any Python 3 target_versions, returns Python 3 ParserConfigs depending on supports_feature: include reserved-keyword config if ASYNC_IS_VALID_IDENTIFIER is not supported by all versions; include valid-identifier config if ASYNC_IS_RESERVED_KEYWORD is not supported by all versions, with reserved-first order when both are added. (ID: todo-3)
- [ ] Check supports_feature(target_versions, feature) returns True only if all versions in the set include the feature (i.e., uses all(...)). This ensures mixed targets (e.g., PY36+PY37) will add both parser configs. (ID: todo-4)
- [ ] Confirm lib2to3_parse() iterates over get_parser_configs() results, constructs driver.Driver with parser_config.grammar and tokenizer_config, and passes tokenizer_config through to parsing (no regression in error handling: InvalidInput raised only after all attempts fail and includes correct location info). (ID: todo-5)
- [ ] Verify blib2to3/pgen2/driver.Driver __init__ signature adds tokenizer_config with default tokenize.TokenizerConfig(), stores it on self, and that parse_stream_raw() and parse_string() pass config=self.tokenizer_config to tokenize.generate_tokens(). (ID: todo-6)
- [ ] Ensure blib2to3/pgen2/tokenize.TokenizerConfig dataclass exists, is frozen, and defines bool field async_is_reserved_keyword defaulting to False. (ID: todo-7)
- [ ] Validate blib2to3/pgen2/tokenize.generate_tokens() signature now accepts optional config: TokenizerConfig = TokenizerConfig(), and inside sets async_is_reserved_keyword = config.async_is_reserved_keyword. (ID: todo-8)
- [ ] Inspect generate_tokens() tokenization rule for identifiers: when token in ('async','await') and (async_is_reserved_keyword or async_def) is True, yield ASYNC/await tokens; otherwise treat them as NAME. This should enable always-keyword behavior for Python 3.7+ mode while preserving legacy behavior for earlier versions. (ID: todo-9)
- [ ] Verify Untokenizer.compat() includes ASYNC and AWAIT tokens in the set that receive a trailing space (toknum in (NAME, NUMBER, ASYNC, AWAIT)). (ID: todo-10)
- [ ] Confirm all internal call sites to tokenize.generate_tokens either pass the new config (driver) or rely on the default (e.g., pgen2/pgen.py). Ensure no remaining internal call sites break due to signature change. (ID: todo-11)
- [ ] Check blib2to3/pgen2/tokenize.pyi is consistent with implementation: it defines TokenizerConfig dataclass (frozen) with async_is_reserved_keyword, but its generate_tokens stub currently lacks the config parameter. Flag this inconsistency and require updating the stub to include the optional config parameter to match runtime signature. (ID: todo-12)
- [ ] Verify tests: tests/test_black.py::test_async_as_identifier uses tests/data/async_as_identifier.py and asserts that formatting is stable; additionally, it runs Black with --target-version py36 (should succeed) and with --target-version py37 expecting nonzero exit code. Ensure the test clearly exercises the intended behavior change. (ID: todo-13)
- [ ] Verify tests: tests/test_black.py::test_python37 uses tests/data/python37.py and checks that Black can parse with --target-version py37 and fails with --target-version py36. Confirm tests/data/python37.py contains examples requiring 'await' as a keyword outside async def (e.g., await in generator expressions) and 'async for' constructs. (ID: todo-14)
- [ ] Confirm no remaining references to the removed get_grammars() function in the active code (outside vendored env/ mirrors). Ensure lib2to3_parse uses get_parser_configs exclusively. (ID: todo-15)


=== PHASE 1 COMPLETE ===

=== PHASE 2: VALIDATION ===

Validating todo item 1 of 15
description: 'Verify Feature enum includes ASYNC_IS_VALID_IDENTIFIER and ASYNC_IS_RESERVED_KEYWORD and that VERSION_TO_FEATURES maps each TargetVersion (PY27, PY33, PY34, PY35, PY36, PY37, PY38) to exactly one of these two flags (mutually exclusive, never both, never neither).'
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "class Feature\\(", "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black" ]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "VERSION_TO_FEATURES",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "test_async_as_identifier",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black"
]
Calling tool: grepTool
class Feature\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:133:class Feature(Enum):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/black.py:132:class Feature(Enum):

Calling tool: grepTool
VERSION_TO_FEATURES
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:146:VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:207:    return all(feature in VERSION_TO_FEATURES[version] for version in target_versions)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:3187:        version for version in TargetVersion if features <= VERSION_TO_FEATURES[version]
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/black.py:141:VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/black.py:195:    return all(feature in VERSION_TO_FEATURES[version] for version in target_versions)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/black.py:3129:        version for version in TargetVersion if features <= VERSION_TO_FEATURES[version]

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
120:    PY34 = 4
121:    PY35 = 5
122:    PY36 = 6
123:    PY37 = 7
124:    PY38 = 8
125:
126:    def is_python2(self) -> bool:
127:        return self is TargetVersion.PY27
128:
129:
130:PY36_VERSIONS = {TargetVersion.PY36, TargetVersion.PY37, TargetVersion.PY38}
131:
132:
133:class Feature(Enum):
134:    # All string literals are unicode
135:    UNICODE_LITERALS = 1
136:    F_STRINGS = 2
137:    NUMERIC_UNDERSCORES = 3
138:    TRAILING_COMMA_IN_CALL = 4
139:    TRAILING_COMMA_IN_DEF = 5
140:    # The following two feature-flags are mutually exclusive, and exactly one should be
141:    # set for every version of python.
142:    ASYNC_IS_VALID_IDENTIFIER = 6
143:    ASYNC_IS_RESERVED_KEYWORD = 7
144:
145:
146:VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {
147:    TargetVersion.PY27: {Feature.ASYNC_IS_VALID_IDENTIFIER},
148:    TargetVersion.PY33: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
149:    TargetVersion.PY34: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
150:    TargetVersion.PY35: {
151:        Feature.UNICODE_LITERALS,
152:        Feature.TRAILING_COMMA_IN_CALL,
153:        Feature.ASYNC_IS_VALID_IDENTIFIER,
154:    },
155:    TargetVersion.PY36: {
156:        Feature.UNICODE_LITERALS,
157:        Feature.F_STRINGS,
158:        Feature.NUMERIC_UNDERSCORES,
159:        Feature.TRAILING_COMMA_IN_CALL,
160:        Feature.TRAILING_COMMA_IN_DEF,
161:        Feature.ASYNC_IS_VALID_IDENTIFIER,
162:    },
163:    TargetVersion.PY37: {
164:        Feature.UNICODE_LITERALS,
165:        Feature.F_STRINGS,
166:        Feature.NUMERIC_UNDERSCORES,
167:        Feature.TRAILING_COMMA_IN_CALL,
168:        Feature.TRAILING_COMMA_IN_DEF,
169:        Feature.ASYNC_IS_RESERVED_KEYWORD,
170:    },
171:    TargetVersion.PY38: {
172:        Feature.UNICODE_LITERALS,
173:        Feature.F_STRINGS,
174:        Feature.NUMERIC_UNDERSCORES,
175:        Feature.TRAILING_COMMA_IN_CALL,
176:        Feature.TRAILING_COMMA_IN_DEF,
177:        Feature.ASYNC_IS_RESERVED_KEYWORD,
178:    },
179:}
180:
181:
182:@dataclass
183:class FileMode:
184:    target_versions: Set[TargetVersion] = Factory(set)
185:    line_length: int = DEFAULT_LINE_LENGTH
186:    string_normalization: bool = True
187:    is_pyi: bool = False
188:
189:    def get_cache_key(self) -> str:
190:        if self.target_versions:
191:            version_str = ",".join(
192:                str(version.value)
193:                for version in sorted(self.target_versions, key=lambda v: v.value)
194:            )
195:        else:
196:            version_str = "-"
197:        parts = [
198:            version_str,
199:            str(self.line_length),
200:            str(int(self.string_normalization)),
201:            str(int(self.is_pyi)),
202:        ]
203:        return ".".join(parts)
204:
205:
206:def supports_feature(target_versions: Set[TargetVersion], feature: Feature) -> bool:
207:    return all(feature in VERSION_TO_FEATURES[version] for version in target_versions)
208:
209:
210:def read_pyproject_toml(
211:    ctx: click.Context, param: click.Parameter, value: Union[str, int, bool, None]
212:) -> Optional[str]:
213:    """Inject Black configuration from "pyproject.toml" into defaults in `ctx`.
214:
215:    Returns the path to a successfully found and read configuration file, None
216:    otherwise.
217:    """
218:    assert not isinstance(value, (int, bool)), "Invalid parameter type passed"
219:    if not value:
220:        root = find_project_root(ctx.params.get("src", ()))
221:        path = root / "pyproject.toml"
222:        if path.is_file():
223:            value = str(path)
224:        else:
225:            return None
226:
227:    try:
228:        pyproject_toml = toml.load(value)
229:        config = pyproject_toml.get("tool", {}).get("black", {})
230:    except (toml.TomlDecodeError, OSError) as e:
231:        raise click.FileError(
232:            filename=value, hint=f"Error reading configuration file: {e}"
233:        )
234:
235:    if not config:
236:        return None
237:
238:    if ctx.default_map is None:
239:        ctx.default_map = {}

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/black.py
120:    PY35 = 5
121:    PY36 = 6
122:    PY37 = 7
123:    PY38 = 8
124:
125:    def is_python2(self) -> bool:
126:        return self is TargetVersion.PY27
127:
128:
129:PY36_VERSIONS = {TargetVersion.PY36, TargetVersion.PY37, TargetVersion.PY38}
130:
131:
132:class Feature(Enum):
133:    # All string literals are unicode
134:    UNICODE_LITERALS = 1
135:    F_STRINGS = 2
136:    NUMERIC_UNDERSCORES = 3
137:    TRAILING_COMMA_IN_CALL = 4
138:    TRAILING_COMMA_IN_DEF = 5
139:
140:
141:VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {
142:    TargetVersion.PY27: set(),
143:    TargetVersion.PY33: {Feature.UNICODE_LITERALS},
144:    TargetVersion.PY34: {Feature.UNICODE_LITERALS},
145:    TargetVersion.PY35: {Feature.UNICODE_LITERALS, Feature.TRAILING_COMMA_IN_CALL},
146:    TargetVersion.PY36: {
147:        Feature.UNICODE_LITERALS,
148:        Feature.F_STRINGS,
149:        Feature.NUMERIC_UNDERSCORES,
150:        Feature.TRAILING_COMMA_IN_CALL,
151:        Feature.TRAILING_COMMA_IN_DEF,
152:    },
153:    TargetVersion.PY37: {
154:        Feature.UNICODE_LITERALS,
155:        Feature.F_STRINGS,
156:        Feature.NUMERIC_UNDERSCORES,
157:        Feature.TRAILING_COMMA_IN_CALL,
158:        Feature.TRAILING_COMMA_IN_DEF,
159:    },
160:    TargetVersion.PY38: {
161:        Feature.UNICODE_LITERALS,
162:        Feature.F_STRINGS,
163:        Feature.NUMERIC_UNDERSCORES,
164:        Feature.TRAILING_COMMA_IN_CALL,
165:        Feature.TRAILING_COMMA_IN_DEF,
166:    },
167:}
168:
169:
170:@dataclass
171:class FileMode:
172:    target_versions: Set[TargetVersion] = Factory(set)
173:    line_length: int = DEFAULT_LINE_LENGTH
174:    string_normalization: bool = True
175:    is_pyi: bool = False
176:
177:    def get_cache_key(self) -> str:
178:        if self.target_versions:
179:            version_str = ",".join(
180:                str(version.value)
181:                for version in sorted(self.target_versions, key=lambda v: v.value)
182:            )
183:        else:
184:            version_str = "-"
185:        parts = [
186:            version_str,
187:            str(self.line_length),
188:            str(int(self.string_normalization)),
189:            str(int(self.is_pyi)),
190:        ]
191:        return ".".join(parts)
192:
193:
194:def supports_feature(target_versions: Set[TargetVersion], feature: Feature) -> bool:
195:    return all(feature in VERSION_TO_FEATURES[version] for version in target_versions)
196:
197:
198:def read_pyproject_toml(
199:    ctx: click.Context, param: click.Parameter, value: Union[str, int, bool, None]
200:) -> Optional[str]:
201:    """Inject Black configuration from "pyproject.toml" into defaults in `ctx`.
202:
203:    Returns the path to a successfully found and read configuration file, None
204:    otherwise.
205:    """
206:    assert not isinstance(value, (int, bool)), "Invalid parameter type passed"
207:    if not value:
208:        root = find_project_root(ctx.params.get("src", ()))
209:        path = root / "pyproject.toml"
210:        if path.is_file():
211:            value = str(path)
212:        else:
213:            return None
214:
215:    try:
216:        pyproject_toml = toml.load(value)
217:        config = pyproject_toml.get("tool", {}).get("black", {})
218:    except (toml.TomlDecodeError, OSError) as e:
219:        raise click.FileError(
220:            filename=value, hint=f"Error reading configuration file: {e}"
221:        )
222:
223:    if not config:
224:        return None
225:
226:    if ctx.default_map is None:
227:        ctx.default_map = {}
228:    ctx.default_map.update(  # type: ignore  # bad types in .pyi
229:        {k.replace("--", "").replace("-", "_"): v for k, v in config.items()}
230:    )
231:    return value
232:
233:
234:@click.command(context_settings=dict(help_option_names=["-h", "--help"]))
235:@click.option("-c", "--code", type=str, help="Format the code passed in as a string.")
236:@click.option(
237:    "-l",
238:    "--line-length",
239:    type=int,

Calling tool: globTool
**/env/src/black/black.py
Matched files:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/black.py

Calling tool: grepTool
test_async_as_identifier
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/coverage_bugsinpy.txt:1:coverage run -m unittest -q tests.test_black.BlackTestCase.test_async_as_identifier
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:506:    def test_async_as_identifier(self) -> None:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/bugsinpy_run_test.sh:1:python -m unittest -q tests.test_black.BlackTestCase.test_async_as_identifier

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py
480:    @patch("black.dump_to_file", dump_to_stderr)
481:    def test_python2_print_function(self) -> None:
482:        source, expected = read_data("python2_print_function")
483:        mode = black.FileMode(target_versions={TargetVersion.PY27})
484:        actual = fs(source, mode=mode)
485:        self.assertFormatEqual(expected, actual)
486:        black.assert_equivalent(source, actual)
487:        black.assert_stable(source, actual, mode)
488:
489:    @patch("black.dump_to_file", dump_to_stderr)
490:    def test_python2_unicode_literals(self) -> None:
491:        source, expected = read_data("python2_unicode_literals")
492:        actual = fs(source)
493:        self.assertFormatEqual(expected, actual)
494:        black.assert_equivalent(source, actual)
495:        black.assert_stable(source, actual, black.FileMode())
496:
497:    @patch("black.dump_to_file", dump_to_stderr)
498:    def test_stub(self) -> None:
499:        mode = black.FileMode(is_pyi=True)
500:        source, expected = read_data("stub.pyi")
501:        actual = fs(source, mode=mode)
502:        self.assertFormatEqual(expected, actual)
503:        black.assert_stable(source, actual, mode)
504:
505:    @patch("black.dump_to_file", dump_to_stderr)
506:    def test_async_as_identifier(self) -> None:
507:        source_path = (THIS_DIR / "data" / "async_as_identifier.py").resolve()
508:        source, expected = read_data("async_as_identifier")
509:        actual = fs(source)
510:        self.assertFormatEqual(expected, actual)
511:        major, minor = sys.version_info[:2]
512:        if major < 3 or (major <= 3 and minor < 7):
513:            black.assert_equivalent(source, actual)
514:        black.assert_stable(source, actual, black.FileMode())
515:        # ensure black can parse this when the target is 3.6
516:        self.invokeBlack([str(source_path), "--target-version", "py36"])
517:        # but not on 3.7, because async/await is no longer an identifier
518:        self.invokeBlack([str(source_path), "--target-version", "py37"], exit_code=123)
519:
520:    @patch("black.dump_to_file", dump_to_stderr)
521:    def test_python37(self) -> None:
522:        source_path = (THIS_DIR / "data" / "python37.py").resolve()
523:        source, expected = read_data("python37")
524:        actual = fs(source)
525:        self.assertFormatEqual(expected, actual)
526:        major, minor = sys.version_info[:2]
527:        if major > 3 or (major == 3 and minor >= 7):
528:            black.assert_equivalent(source, actual)
529:        black.assert_stable(source, actual, black.FileMode())
530:        # ensure black can parse this when the target is 3.7
531:        self.invokeBlack([str(source_path), "--target-version", "py37"])
532:        # but not on 3.6, because we use async as a reserved keyword
533:        self.invokeBlack([str(source_path), "--target-version", "py36"], exit_code=123)
534:
535:    @patch("black.dump_to_file", dump_to_stderr)
536:    def test_fmtonoff(self) -> None:
537:        source, expected = read_data("fmtonoff")
538:        actual = fs(source)
539:        self.assertFormatEqual(expected, actual)
540:        black.assert_equivalent(source, actual)
541:        black.assert_stable(source, actual, black.FileMode())
542:
543:    @patch("black.dump_to_file", dump_to_stderr)
544:    def test_fmtonoff2(self) -> None:
545:        source, expected = read_data("fmtonoff2")
546:        actual = fs(source)
547:        self.assertFormatEqual(expected, actual)
548:        black.assert_equivalent(source, actual)
549:        black.assert_stable(source, actual, black.FileMode())
550:
551:    @patch("black.dump_to_file", dump_to_stderr)
552:    def test_remove_empty_parentheses_after_class(self) -> None:
553:        source, expected = read_data("class_blank_parentheses")
554:        actual = fs(source)
555:        self.assertFormatEqual(expected, actual)
556:        black.assert_equivalent(source, actual)
557:        black.assert_stable(source, actual, black.FileMode())
558:
559:    @patch("black.dump_to_file", dump_to_stderr)

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py
1:#!/usr/bin/env python3
2:import asyncio
3:import logging
4:from concurrent.futures import ThreadPoolExecutor
5:from contextlib import contextmanager
6:from functools import partial, wraps
7:from io import BytesIO, TextIOWrapper
8:import os
9:from pathlib import Path
10:import re
11:import sys
12:from tempfile import TemporaryDirectory
13:from typing import (
14:    Any,
15:    BinaryIO,
16:    Callable,
17:    Coroutine,
18:    Generator,
19:    List,
20:    Tuple,
21:    Iterator,
22:    TypeVar,
23:)
24:import unittest
25:from unittest.mock import patch, MagicMock
26:
27:from click import unstyle
28:from click.testing import CliRunner
29:
30:import black
31:from black import Feature, TargetVersion
32:
33:try:
34:    import blackd
35:    from aiohttp.test_utils import TestClient, TestServer
36:except ImportError:
37:    has_blackd_deps = False
38:else:
39:    has_blackd_deps = True
40:

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: In black.py, Feature enum includes ASYNC_IS_VALID_IDENTIFIER and ASYNC_IS_RESERVED_KEYWORD. VERSION_TO_FEATURES maps PY27PY36 to ASYNC_IS_VALID_IDENTIFIER and PY37PY38 to ASYNC_IS_RESERVED_KEYWORD, ensuring each version has exactly one of the two flags (mutually exclusive).


Todo item 1 of 15 is correct
description: 'Verify Feature enum includes ASYNC_IS_VALID_IDENTIFIER and ASYNC_IS_RESERVED_KEYWORD and that VERSION_TO_FEATURES maps each TargetVersion (PY27, PY33, PY34, PY35, PY36, PY37, PY38) to exactly one of these two flags (mutually exclusive, never both, never neither).'
Validating todo item 2 of 15
description: 'Confirm black.ParserConfig dataclass exists with fields: grammar: Grammar and tokenizer_config: TokenizerConfig defaulting to TokenizerConfig(). Ensure TokenizerConfig is imported from blib2to3.pgen2.tokenize.'
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "class ParserConfig|TokenizerConfig",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "from dataclasses import dataclass",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "dataclass", "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py" ]
Calling tool: grepTool
class ParserConfig|TokenizerConfig
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:142:class TokenizerConfig:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py:37:        tokenizer_config=tokenize.TokenizerConfig(),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.pyi:11:from blib2to3.pgen2.tokenize import TokenizerConfig
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.pyi:23:        tokenizer_config: TokenizerConfig = ...
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi:14:class TokenizerConfig:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:51:from blib2to3.pgen2.tokenize import TokenizerConfig
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:764:class ParserConfig:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:766:    tokenizer_config: TokenizerConfig = TokenizerConfig()
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:776:                TokenizerConfig(async_is_reserved_keyword=True),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:781:                TokenizerConfig(async_is_reserved_keyword=False),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:805:                    TokenizerConfig(async_is_reserved_keyword=True),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:813:                    TokenizerConfig(async_is_reserved_keyword=False),

Calling tool: grepTool
from dataclasses import dataclass
No files found. Call grepTool again with a more generic pattern.

Calling tool: grepTool
dataclass
Matched content:
40:from attr import dataclass, evolve, Factory
182:@dataclass
763:@dataclass(frozen=True)
887:@dataclass
1034:@dataclass
1166:@dataclass
1460:@dataclass
1571:@dataclass
2180:@dataclass
3357:@dataclass

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
30:    Pattern,
31:    Sequence,
32:    Set,
33:    Tuple,
34:    TypeVar,
35:    Union,
36:    cast,
37:)
38:
39:from appdirs import user_cache_dir
40:from attr import dataclass, evolve, Factory
41:import click
42:import toml
43:from typed_ast import ast3, ast27
44:
45:# lib2to3 fork
46:from blib2to3.pytree import Node, Leaf, type_repr
47:from blib2to3 import pygram, pytree
48:from blib2to3.pgen2 import driver, token
49:from blib2to3.pgen2.grammar import Grammar
50:from blib2to3.pgen2.parse import ParseError
51:from blib2to3.pgen2.tokenize import TokenizerConfig
52:
53:
54:__version__ = "19.3b0"
55:DEFAULT_LINE_LENGTH = 88
56:DEFAULT_EXCLUDES = (
57:    r"/(\.eggs|\.git|\.hg|\.mypy_cache|\.nox|\.tox|\.venv|_build|buck-out|build|dist)/"
58:)
59:DEFAULT_INCLUDES = r"\.pyi?$"
60:CACHE_DIR = Path(user_cache_dir("black", version=__version__))
61:
62:
63:# types
64:FileContent = str
65:Encoding = str
66:NewLine = str
67:Depth = int
68:NodeType = int
69:LeafID = int

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
740:            current_line, line_length=mode.line_length, features=split_line_features
741:        ):
742:            dst_contents += str(line)
743:    return dst_contents
744:
745:
746:def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:
747:    """Return a tuple of (decoded_contents, encoding, newline).
748:
749:    `newline` is either CRLF or LF but `decoded_contents` is decoded with
750:    universal newlines (i.e. only contains LF).
751:    """
752:    srcbuf = io.BytesIO(src)
753:    encoding, lines = tokenize.detect_encoding(srcbuf.readline)
754:    if not lines:
755:        return "", encoding, "\n"
756:
757:    newline = "\r\n" if b"\r\n" == lines[0][-2:] else "\n"
758:    srcbuf.seek(0)
759:    with io.TextIOWrapper(srcbuf, encoding) as tiow:
760:        return tiow.read(), encoding, newline
761:
762:
763:@dataclass(frozen=True)
764:class ParserConfig:
765:    grammar: Grammar
766:    tokenizer_config: TokenizerConfig = TokenizerConfig()
767:
768:
769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:
770:    if not target_versions:
771:        # No target_version specified, so try all grammars.
772:        return [
773:            # Python 3.7+
774:            ParserConfig(
775:                pygram.python_grammar_no_print_statement_no_exec_statement,
776:                TokenizerConfig(async_is_reserved_keyword=True),
777:            ),
778:            # Python 3.0-3.6
779:            ParserConfig(
780:                pygram.python_grammar_no_print_statement_no_exec_statement,
781:                TokenizerConfig(async_is_reserved_keyword=False),
782:            ),
783:            # Python 2.7 with future print_function import
784:            ParserConfig(pygram.python_grammar_no_print_statement),
785:            # Python 2.7
786:            ParserConfig(pygram.python_grammar),
787:        ]
788:    elif all(version.is_python2() for version in target_versions):
789:        # Python 2-only code, so try Python 2 grammars.

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
130:    {f"{prefix}'''" for prefix in _strprefixes} |
131:    {f'{prefix}"""' for prefix in _strprefixes}
132:)
133:single_quoted = (
134:    {"'", '"'} |
135:    {f"{prefix}'" for prefix in _strprefixes} |
136:    {f'{prefix}"' for prefix in _strprefixes}
137:)
138:
139:tabsize = 8
140:
141:@dataclass(frozen=True)
142:class TokenizerConfig:
143:    async_is_reserved_keyword: bool = False
144:
145:class TokenError(Exception): pass
146:
147:class StopTokenizing(Exception): pass
148:
149:def printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line): # for testing
150:    (srow, scol) = xxx_todo_changeme
151:    (erow, ecol) = xxx_todo_changeme1
152:    print("%d,%d-%d,%d:\t%s\t%s" % \
153:        (srow, scol, erow, ecol, tok_name[type], repr(token)))

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: Verified in black.py: ParserConfig dataclass exists (lines 763-767) with fields grammar: Grammar and tokenizer_config: TokenizerConfig = TokenizerConfig(). Also confirmed import: from blib2to3.pgen2.tokenize import TokenizerConfig (line 51).


Todo item 2 of 15 is correct
description: 'Confirm black.ParserConfig dataclass exists with fields: grammar: Grammar and tokenizer_config: TokenizerConfig defaulting to TokenizerConfig(). Ensure TokenizerConfig is imported from blib2to3.pgen2.tokenize.'
Validating todo item 3 of 15
description: 'Validate get_parser_configs() logic and order for parsing attempts: (a) when target_versions is empty, returns four ParserConfig entries in this order: Python 3 grammar with TokenizerConfig(async_is_reserved_keyword=True), Python 3 grammar with TokenizerConfig(async_is_reserved_keyword=False), Python 2 no_print grammar, Python 2 grammar; (b) when all target_versions are Python 2, returns only the two Python 2 grammars (no TokenizerConfig overrides); (c) when any Python 3 target_versions, returns Python 3 ParserConfigs depending on supports_feature: include reserved-keyword config if ASYNC_IS_VALID_IDENTIFIER is not supported by all versions; include valid-identifier config if ASYNC_IS_RESERVED_KEYWORD is not supported by all versions, with reserved-first order when both are added.'
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "def supports_feature\\(",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/{{FOLDER_PATH}}"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "get_parser_configs\\(",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "def supports_feature\\(",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "from attr import dataclass",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "class Driver\\(", "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black" ]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "class TokenizerConfig",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "def generate_tokens\\(",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "class Feature\\(", "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black" ]
Calling tool: grepTool
def supports_feature\(
Grep command failed with exit code 2: rg: .rgignore: No such file or directory (os error 2)
rg: /Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/{{FOLDER_PATH}}: IO error for operation on /Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/{{FOLDER_PATH}}: No such file or directory (os error 2)


Calling tool: grepTool
get_parser_configs\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:826:    for parser_config in get_parser_configs(set(target_versions)):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
730:        for feature in {Feature.TRAILING_COMMA_IN_CALL, Feature.TRAILING_COMMA_IN_DEF}
731:        if supports_feature(versions, feature)
732:    }
733:    for current_line in lines.visit(src_node):
734:        for _ in range(after):
735:            dst_contents += str(empty_line)
736:        before, after = elt.maybe_empty_lines(current_line)
737:        for _ in range(before):
738:            dst_contents += str(empty_line)
739:        for line in split_line(
740:            current_line, line_length=mode.line_length, features=split_line_features
741:        ):
742:            dst_contents += str(line)
743:    return dst_contents
744:
745:
746:def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:
747:    """Return a tuple of (decoded_contents, encoding, newline).
748:
749:    `newline` is either CRLF or LF but `decoded_contents` is decoded with
750:    universal newlines (i.e. only contains LF).
751:    """
752:    srcbuf = io.BytesIO(src)
753:    encoding, lines = tokenize.detect_encoding(srcbuf.readline)
754:    if not lines:
755:        return "", encoding, "\n"
756:
757:    newline = "\r\n" if b"\r\n" == lines[0][-2:] else "\n"
758:    srcbuf.seek(0)
759:    with io.TextIOWrapper(srcbuf, encoding) as tiow:
760:        return tiow.read(), encoding, newline
761:
762:
763:@dataclass(frozen=True)
764:class ParserConfig:
765:    grammar: Grammar
766:    tokenizer_config: TokenizerConfig = TokenizerConfig()
767:
768:
769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:
770:    if not target_versions:
771:        # No target_version specified, so try all grammars.
772:        return [
773:            # Python 3.7+
774:            ParserConfig(
775:                pygram.python_grammar_no_print_statement_no_exec_statement,
776:                TokenizerConfig(async_is_reserved_keyword=True),
777:            ),
778:            # Python 3.0-3.6
779:            ParserConfig(
780:                pygram.python_grammar_no_print_statement_no_exec_statement,
781:                TokenizerConfig(async_is_reserved_keyword=False),
782:            ),
783:            # Python 2.7 with future print_function import
784:            ParserConfig(pygram.python_grammar_no_print_statement),
785:            # Python 2.7
786:            ParserConfig(pygram.python_grammar),
787:        ]
788:    elif all(version.is_python2() for version in target_versions):
789:        # Python 2-only code, so try Python 2 grammars.
790:        return [
791:            # Python 2.7 with future print_function import
792:            ParserConfig(pygram.python_grammar_no_print_statement),
793:            # Python 2.7
794:            ParserConfig(pygram.python_grammar),
795:        ]
796:    else:
797:        # Python 3-compatible code, so only try Python 3 grammar.
798:        configs = []
799:        # If we have to parse both, try to parse async as a keyword first
800:        if not supports_feature(target_versions, Feature.ASYNC_IS_VALID_IDENTIFIER):
801:            # Python 3.7+
802:            configs.append(
803:                ParserConfig(
804:                    pygram.python_grammar_no_print_statement_no_exec_statement,
805:                    TokenizerConfig(async_is_reserved_keyword=True),
806:                )
807:            )
808:        if not supports_feature(target_versions, Feature.ASYNC_IS_RESERVED_KEYWORD):
809:            # Python 3.0-3.6
810:            configs.append(
811:                ParserConfig(
812:                    pygram.python_grammar_no_print_statement_no_exec_statement,
813:                    TokenizerConfig(async_is_reserved_keyword=False),
814:                )
815:            )
816:        # At least one of the above branches must have been taken, because every Python
817:        # version has exactly one of the two 'ASYNC_IS_*' flags
818:        return configs
819:
820:
821:def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:
822:    """Given a string with source, return the lib2to3 Node."""
823:    if src_txt[-1:] != "\n":
824:        src_txt += "\n"
825:
826:    for parser_config in get_parser_configs(set(target_versions)):
827:        drv = driver.Driver(
828:            parser_config.grammar,
829:            pytree.convert,
830:            tokenizer_config=parser_config.tokenizer_config,
831:        )
832:        try:
833:            result = drv.parse_string(src_txt, True)
834:            break
835:
836:        except ParseError as pe:
837:            lineno, column = pe.context[1]
838:            lines = src_txt.splitlines()
839:            try:
840:                faulty_line = lines[lineno - 1]
841:            except IndexError:
842:                faulty_line = "<line number missing in source>"
843:            exc = InvalidInput(f"Cannot parse: {lineno}:{column}: {faulty_line}")
844:    else:
845:        raise exc from None
846:
847:    if isinstance(result, Leaf):
848:        result = Node(syms.file_input, [result])
849:    return result
850:
851:
852:def lib2to3_unparse(node: Node) -> str:
853:    """Given a lib2to3 node, return its string representation."""
854:    code = str(node)
855:    return code
856:
857:
858:T = TypeVar("T")
859:
860:
861:class Visitor(Generic[T]):
862:    """Basic lib2to3 visitor that yields things of type `T` on `visit()`."""
863:
864:    def visit(self, node: LN) -> Iterator[T]:
865:        """Main method to visit `node` and its children.
866:
867:        It tries to find a `visit_*()` method for the given `node.type`, like
868:        `visit_simple_stmt` for Node objects or `visit_INDENT` for Leaf objects.
869:        If no dedicated `visit_*()` method is found, chooses `visit_default()`

Calling tool: grepTool
def supports_feature\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:206:def supports_feature(target_versions: Set[TargetVersion], feature: Feature) -> bool:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/black.py:194:def supports_feature(target_versions: Set[TargetVersion], feature: Feature) -> bool:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
180:
181:
182:@dataclass
183:class FileMode:
184:    target_versions: Set[TargetVersion] = Factory(set)
185:    line_length: int = DEFAULT_LINE_LENGTH
186:    string_normalization: bool = True
187:    is_pyi: bool = False
188:
189:    def get_cache_key(self) -> str:
190:        if self.target_versions:
191:            version_str = ",".join(
192:                str(version.value)
193:                for version in sorted(self.target_versions, key=lambda v: v.value)
194:            )
195:        else:
196:            version_str = "-"
197:        parts = [
198:            version_str,
199:            str(self.line_length),
200:            str(int(self.string_normalization)),
201:            str(int(self.is_pyi)),
202:        ]
203:        return ".".join(parts)
204:
205:
206:def supports_feature(target_versions: Set[TargetVersion], feature: Feature) -> bool:
207:    return all(feature in VERSION_TO_FEATURES[version] for version in target_versions)
208:
209:
210:def read_pyproject_toml(
211:    ctx: click.Context, param: click.Parameter, value: Union[str, int, bool, None]
212:) -> Optional[str]:
213:    """Inject Black configuration from "pyproject.toml" into defaults in `ctx`.
214:
215:    Returns the path to a successfully found and read configuration file, None
216:    otherwise.
217:    """
218:    assert not isinstance(value, (int, bool)), "Invalid parameter type passed"
219:    if not value:
220:        root = find_project_root(ctx.params.get("src", ()))
221:        path = root / "pyproject.toml"
222:        if path.is_file():
223:            value = str(path)
224:        else:
225:            return None
226:
227:    try:
228:        pyproject_toml = toml.load(value)
229:        config = pyproject_toml.get("tool", {}).get("black", {})

Calling tool: grepTool
from attr import dataclass
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:34:from attr import dataclass
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi:5:from attr import dataclass
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:40:from attr import dataclass, evolve, Factory
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/black.py:40:from attr import dataclass, evolve, Factory

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
1:import asyncio
2:from asyncio.base_events import BaseEventLoop
3:from concurrent.futures import Executor, ProcessPoolExecutor
4:from datetime import datetime
5:from enum import Enum
6:from functools import lru_cache, partial, wraps
7:import io
8:import itertools
9:import logging
10:from multiprocessing import Manager, freeze_support
11:import os
12:from pathlib import Path
13:import pickle
14:import re
15:import signal
16:import sys
17:import tempfile
18:import tokenize
19:from typing import (
20:    Any,
21:    Callable,
22:    Collection,
23:    Dict,
24:    Generator,
25:    Generic,
26:    Iterable,
27:    Iterator,
28:    List,
29:    Optional,
30:    Pattern,
31:    Sequence,
32:    Set,
33:    Tuple,
34:    TypeVar,
35:    Union,
36:    cast,
37:)
38:
39:from appdirs import user_cache_dir
40:from attr import dataclass, evolve, Factory
41:import click
42:import toml
43:from typed_ast import ast3, ast27
44:
45:# lib2to3 fork
46:from blib2to3.pytree import Node, Leaf, type_repr
47:from blib2to3 import pygram, pytree
48:from blib2to3.pgen2 import driver, token
49:from blib2to3.pgen2.grammar import Grammar
50:from blib2to3.pgen2.parse import ParseError
51:from blib2to3.pgen2.tokenize import TokenizerConfig
52:
53:
54:__version__ = "19.3b0"
55:DEFAULT_LINE_LENGTH = 88
56:DEFAULT_EXCLUDES = (
57:    r"/(\.eggs|\.git|\.hg|\.mypy_cache|\.nox|\.tox|\.venv|_build|buck-out|build|dist)/"
58:)
59:DEFAULT_INCLUDES = r"\.pyi?$"
60:CACHE_DIR = Path(user_cache_dir("black", version=__version__))
61:
62:
63:# types
64:FileContent = str
65:Encoding = str
66:NewLine = str
67:Depth = int
68:NodeType = int
69:LeafID = int
70:Priority = int
71:Index = int
72:LN = Union[Leaf, Node]
73:SplitFunc = Callable[["Line", Collection["Feature"]], Iterator["Line"]]
74:Timestamp = float
75:FileSize = int
76:CacheInfo = Tuple[Timestamp, FileSize]
77:Cache = Dict[Path, CacheInfo]
78:out = partial(click.secho, bold=True, err=True)
79:err = partial(click.secho, fg="red", err=True)
80:

Calling tool: grepTool
class Driver\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py:30:class Driver(object):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/driver.py:30:class Driver(object):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Modifications:
5:# Copyright 2006 Google, Inc. All Rights Reserved.
6:# Licensed to PSF under a Contributor Agreement.
7:
8:"""Parser driver.
9:
10:This provides a high-level interface to parse a file into a syntax tree.
11:
12:"""
13:
14:__author__ = "Guido van Rossum <guido@python.org>"
15:
16:__all__ = ["Driver", "load_grammar"]
17:
18:# Python imports
19:import codecs
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(
33:        self,
34:        grammar,
35:        convert=None,
36:        logger=None,
37:        tokenizer_config=tokenize.TokenizerConfig(),
38:    ):
39:        self.grammar = grammar
40:        if logger is None:
41:            logger = logging.getLogger(__name__)
42:        self.logger = logger
43:        self.convert = convert
44:        self.tokenizer_config = tokenizer_config
45:
46:    def parse_tokens(self, tokens, debug=False):
47:        """Parse a series of tokens and return the syntax tree."""
48:        # XXX Move the prefix computation into a wrapper around tokenize.
49:        p = parse.Parser(self.grammar, self.convert)
50:        p.setup()
51:        lineno = 1
52:        column = 0
53:        indent_columns = []
54:        type = value = start = end = line_text = None
55:        prefix = ""
56:        for quintuple in tokens:
57:            type, value, start, end, line_text = quintuple
58:            if start != (lineno, column):
59:                assert (lineno, column) <= start, ((lineno, column), start)
60:                s_lineno, s_column = start
61:                if lineno < s_lineno:
62:                    prefix += "\n" * (s_lineno - lineno)
63:                    lineno = s_lineno
64:                    column = 0
65:                if column < s_column:
66:                    prefix += line_text[column:s_column]
67:                    column = s_column
68:            if type in (tokenize.COMMENT, tokenize.NL):
69:                prefix += value
70:                lineno, column = end
71:                if value.endswith("\n"):
72:                    lineno += 1
73:                    column = 0
74:                continue
75:            if type == token.OP:
76:                type = grammar.opmap[value]
77:            if debug:
78:                self.logger.debug("%s %r (prefix=%r)",
79:                                  token.tok_name[type], value, prefix)
80:            if type == token.INDENT:
81:                indent_columns.append(len(value))
82:                _prefix = prefix + value
83:                prefix = ""
84:                value = ""
85:            elif type == token.DEDENT:
86:                _indent_col = indent_columns.pop()
87:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
88:            if p.addtoken(type, value, (prefix, start)):
89:                if debug:
90:                    self.logger.debug("Stop.")
91:                break
92:            prefix = ""
93:            if type in {token.INDENT, token.DEDENT}:
94:                prefix = _prefix
95:            lineno, column = end
96:            if value.endswith("\n"):
97:                lineno += 1
98:                column = 0
99:        else:
100:            # We never broke out -- EOF is too soon (how can this happen???)
101:            raise parse.ParseError("incomplete input",
102:                                   type, value, (prefix, start))
103:        return p.rootnode
104:
105:    def parse_stream_raw(self, stream, debug=False):
106:        """Parse a stream and return the syntax tree."""
107:        tokens = tokenize.generate_tokens(stream.readline, config=self.tokenizer_config)
108:        return self.parse_tokens(tokens, debug)
109:
110:    def parse_stream(self, stream, debug=False):
111:        """Parse a stream and return the syntax tree."""
112:        return self.parse_stream_raw(stream, debug)
113:
114:    def parse_file(self, filename, encoding=None, debug=False):
115:        """Parse a file and return the syntax tree."""
116:        with io.open(filename, "r", encoding=encoding) as stream:
117:            return self.parse_stream(stream, debug)
118:
119:    def parse_string(self, text, debug=False):
120:        """Parse a string and return the syntax tree."""
121:        tokens = tokenize.generate_tokens(
122:            io.StringIO(text).readline,
123:            config=self.tokenizer_config,
124:        )
125:        return self.parse_tokens(tokens, debug)
126:
127:    def _partially_consume_prefix(self, prefix, column):
128:        lines = []
129:        current_line = ""
130:        current_column = 0
131:        wait_for_nl = False
132:        for char in prefix:
133:            current_line += char
134:            if wait_for_nl:
135:                if char == '\n':
136:                    if current_line.strip() and current_column < column:
137:                        res = ''.join(lines)
138:                        return res, prefix[len(res):]
139:
140:                    lines.append(current_line)

Calling tool: grepTool
class TokenizerConfig
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:142:class TokenizerConfig:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi:14:class TokenizerConfig:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
120:)
121:
122:endprogs = {"'": re.compile(Single), '"': re.compile(Double),
123:            "'''": single3prog, '"""': double3prog,
124:            **{f"{prefix}'''": single3prog for prefix in _strprefixes},
125:            **{f'{prefix}"""': double3prog for prefix in _strprefixes},
126:            **{prefix: None for prefix in _strprefixes}}
127:
128:triple_quoted = (
129:    {"'''", '"""'} |
130:    {f"{prefix}'''" for prefix in _strprefixes} |
131:    {f'{prefix}"""' for prefix in _strprefixes}
132:)
133:single_quoted = (
134:    {"'", '"'} |
135:    {f"{prefix}'" for prefix in _strprefixes} |
136:    {f'{prefix}"' for prefix in _strprefixes}
137:)
138:
139:tabsize = 8
140:
141:@dataclass(frozen=True)
142:class TokenizerConfig:
143:    async_is_reserved_keyword: bool = False
144:
145:class TokenError(Exception): pass
146:
147:class StopTokenizing(Exception): pass
148:
149:def printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line): # for testing
150:    (srow, scol) = xxx_todo_changeme
151:    (erow, ecol) = xxx_todo_changeme1
152:    print("%d,%d-%d,%d:\t%s\t%s" % \
153:        (srow, scol, erow, ecol, tok_name[type], repr(token)))
154:
155:def tokenize(readline, tokeneater=printtoken):
156:    """
157:    The tokenize() function accepts two parameters: one representing the
158:    input stream, and one providing an output mechanism for tokenize().
159:
160:    The first parameter, readline, must be a callable object which provides
161:    the same interface as the readline() method of built-in file objects.
162:    Each call to the function should return one line of input as a string.
163:
164:    The second parameter, tokeneater, must also be a callable object. It is
165:    called once for each token, with five arguments, corresponding to the
166:    tuples generated by generate_tokens().
167:    """
168:    try:
169:        tokenize_loop(readline, tokeneater)
170:    except StopTokenizing:
171:        pass
172:
173:# backwards compatible interface
174:def tokenize_loop(readline, tokeneater):
175:    for token_info in generate_tokens(readline):
176:        tokeneater(*token_info)
177:
178:class Untokenizer:
179:
180:    def __init__(self):
181:        self.tokens = []
182:        self.prev_row = 1
183:        self.prev_col = 0
184:
185:    def add_whitespace(self, start):
186:        row, col = start
187:        assert row <= self.prev_row
188:        col_offset = col - self.prev_col
189:        if col_offset:
190:            self.tokens.append(" " * col_offset)
191:
192:    def untokenize(self, iterable):
193:        for t in iterable:
194:            if len(t) == 2:
195:                self.compat(t, iterable)
196:                break
197:            tok_type, token, start, end, line = t
198:            self.add_whitespace(start)
199:            self.tokens.append(token)
200:            self.prev_row, self.prev_col = end
201:            if tok_type in (NEWLINE, NL):
202:                self.prev_row += 1
203:                self.prev_col = 0
204:        return "".join(self.tokens)
205:
206:    def compat(self, token, iterable):
207:        startline = False
208:        indents = []
209:        toks_append = self.tokens.append
210:        toknum, tokval = token
211:        if toknum in (NAME, NUMBER):
212:            tokval += ' '
213:        if toknum in (NEWLINE, NL):
214:            startline = True
215:        for tok in iterable:
216:            toknum, tokval = tok[:2]
217:
218:            if toknum in (NAME, NUMBER, ASYNC, AWAIT):
219:                tokval += ' '

Calling tool: grepTool
def generate_tokens\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi:32:def generate_tokens(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.py:337:def generate_tokens(readline):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.pyi:28:def generate_tokens(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/flake8/processor.py:259:    def generate_tokens(self):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/coverage/phystokens.py:142:    def generate_tokens(self, text):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pycodestyle.py:2026:    def generate_tokens(self):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/2/tokenize.pyi:117:def generate_tokens(readline: Callable[[], str]) -> Generator[_TokenType, None, None]: ...
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/3/tokenize.pyi:46:def generate_tokens(readline: Callable[[], str]) -> Generator[TokenInfo, None, None]: ...  # undocumented
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/2and3/lib2to3/pgen2/tokenize.pyi:28:def generate_tokens(

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
330:
331:    Round-trip invariant for limited intput:
332:        # Output text will tokenize the back to the input
333:        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
334:        newcode = untokenize(t1)
335:        readline = iter(newcode.splitlines(1)).next
336:        t2 = [tok[:2] for tokin generate_tokens(readline)]
337:        assert t1 == t2
338:    """
339:    ut = Untokenizer()
340:    return ut.untokenize(iterable)
341:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
343:    """
344:    The generate_tokens() generator requires one argument, readline, which
345:    must be a callable object which provides the same interface as the
346:    readline() method of built-in file objects. Each call to the function
347:    should return one line of input as a string.  Alternately, readline
348:    can be a callable function terminating with StopIteration:
349:        readline = open(myfile).next    # Example of alternate readline
350:
351:    The generator produces 5-tuples with these members: the token type; the
352:    token string; a 2-tuple (srow, scol) of ints specifying the row and
353:    column where the token begins in the source; a 2-tuple (erow, ecol) of
354:    ints specifying the row and column where the token ends in the source;
355:    and the line on which the token was found. The line passed is the
356:    logical line; continuation lines are included.
357:    """
358:    lnum = parenlev = continued = 0
359:    numchars = '0123456789'
360:    contstr, needcont = '', 0
361:    contline = None
362:    indents = [0]
363:
364:    # If we know we're parsing 3.7+, we can unconditionally parse `async` and
365:    # `await` as keywords.
366:    async_is_reserved_keyword = config.async_is_reserved_keyword
367:    # 'stashed' and 'async_*' are used for async/await parsing
368:    stashed = None
369:    async_def = False
370:    async_def_indent = 0
371:    async_def_nl = False
372:
373:    while 1:                                   # loop over lines in stream
374:        try:
375:            line = readline()
376:        except StopIteration:
377:            line = ''
378:        lnum = lnum + 1
379:        pos, max = 0, len(line)
380:
381:        if contstr:                            # continued string
382:            if not line:
383:                raise TokenError("EOF in multi-line string", strstart)
384:            endmatch = endprog.match(line)
385:            if endmatch:
386:                pos = end = endmatch.end(0)
387:                yield (STRING, contstr + line[:end],
388:                       strstart, (lnum, end), contline + line)
389:                contstr, needcont = '', 0
390:                contline = None
391:            elif needcont and line[-2:] != '\\\n' and line[-3:] != '\\\r\n':
392:                yield (ERRORTOKEN, contstr + line,
393:                           strstart, (lnum, len(line)), contline)
394:                contstr = ''
395:                contline = None
396:                continue
397:            else:
398:                contstr = contstr + line
399:                contline = contline + line
400:                continue
401:
402:        elif parenlev == 0 and not continued:  # new statement
403:            if not line: break
404:            column = 0
405:            while pos < max:                   # measure leading whitespace
406:                if line[pos] == ' ': column = column + 1
407:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize
408:                elif line[pos] == '\f': column = 0
409:                else: break
410:                pos = pos + 1
411:            if pos == max: break
412:
413:            if stashed:
414:                yield stashed
415:                stashed = None
416:
417:            if line[pos] in '\r\n':            # skip blank lines
418:                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)
419:                continue
420:
421:            if line[pos] == '#':               # skip comments
422:                comment_token = line[pos:].rstrip('\r\n')
423:                nl_pos = pos + len(comment_token)
424:                yield (COMMENT, comment_token,
425:                        (lnum, pos), (lnum, pos + len(comment_token)), line)
426:                yield (NL, line[nl_pos:],
427:                        (lnum, nl_pos), (lnum, len(line)), line)
428:                continue
429:
430:            if column > indents[-1]:           # count indents
431:                indents.append(column)
432:                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)
433:
434:            while column < indents[-1]:        # count dedents
435:                if column not in indents:
436:                    raise IndentationError(
437:                        "unindent does not match any outer indentation level",
438:                        ("<tokenize>", lnum, pos, line))
439:                indents = indents[:-1]
440:
441:                if async_def and async_def_indent >= indents[-1]:
442:                    async_def = False
443:                    async_def_nl = False
444:                    async_def_indent = 0
445:
446:                yield (DEDENT, '', (lnum, pos), (lnum, pos), line)
447:
448:            if async_def and async_def_nl and async_def_indent >= indents[-1]:
449:                async_def = False
450:                async_def_nl = False
451:                async_def_indent = 0
452:
453:        else:                                  # continued statement
454:            if not line:
455:                raise TokenError("EOF in multi-line statement", (lnum, 0))
456:            continued = 0
457:
458:        while pos < max:
459:            pseudomatch = pseudoprog.match(line, pos)
460:            if pseudomatch:                                # scan for tokens
461:                start, end = pseudomatch.span(1)
462:                spos, epos, pos = (lnum, start), (lnum, end), end
463:                token, initial = line[start:end], line[start]
464:
465:                if initial in numchars or \
466:                   (initial == '.' and token != '.'):      # ordinary number
467:                    yield (NUMBER, token, spos, epos, line)
468:                elif initial in '\r\n':
469:                    newline = NEWLINE
470:                    if parenlev > 0:
471:                        newline = NL
472:                    elif async_def:
473:                        async_def_nl = True
474:                    if stashed:
475:                        yield stashed
476:                        stashed = None
477:                    yield (newline, token, spos, epos, line)
478:
479:                elif initial == '#':
480:                    assert not token.endswith("\n")
481:                    if stashed:
482:                        yield stashed
483:                        stashed = None
484:                    yield (COMMENT, token, spos, epos, line)
485:                elif token in triple_quoted:
486:                    endprog = endprogs[token]
487:                    endmatch = endprog.match(line, pos)
488:                    if endmatch:                           # all on one line
489:                        pos = endmatch.end(0)
490:                        token = line[start:pos]
491:                        if stashed:
492:                            yield stashed
493:                            stashed = None
494:                        yield (STRING, token, spos, (lnum, pos), line)
495:                    else:
496:                        strstart = (lnum, start)           # multiple lines
497:                        contstr = line[start:]
498:                        contline = line
499:                        break
500:                elif initial in single_quoted or \
501:                    token[:2] in single_quoted or \
502:                    token[:3] in single_quoted:
503:                    if token[-1] == '\n':                  # continued string
504:                        strstart = (lnum, start)
505:                        endprog = (endprogs[initial] or endprogs[token[1]] or
506:                                   endprogs[token[2]])
507:                        contstr, needcont = line[start:], 1
508:                        contline = line
509:                        break
510:                    else:                                  # ordinary string
511:                        if stashed:
512:                            yield stashed
513:                            stashed = None
514:                        yield (STRING, token, spos, epos, line)
515:                elif initial.isidentifier():               # ordinary name
516:                    if token in ('async', 'await'):
517:                        if async_is_reserved_keyword or async_def:
518:                            yield (ASYNC if token == 'async' else AWAIT,
519:                                   token, spos, epos, line)
520:                            continue
521:
522:                    tok = (NAME, token, spos, epos, line)
523:                    if token == 'async' and not stashed:
524:                        stashed = tok
525:                        continue
526:
527:                    if token in ('def', 'for'):
528:                        if (stashed
529:                                and stashed[0] == NAME
530:                                and stashed[1] == 'async'):
531:
532:                            if token == 'def':
533:                                async_def = True
534:                                async_def_indent = indents[-1]
535:
536:                            yield (ASYNC, stashed[1],
537:                                   stashed[2], stashed[3],
538:                                   stashed[4])
539:                            stashed = None
540:
541:                    if stashed:
542:                        yield stashed
543:                        stashed = None
544:
545:                    yield tok
546:                elif initial == '\\':                      # continued stmt
547:                    # This yield is new; needed for better idempotency:
548:                    if stashed:
549:                        yield stashed
550:                        stashed = None
551:                    yield (NL, token, spos, (lnum, pos), line)
552:                    continued = 1
553:                else:
554:                    if initial in '([{': parenlev = parenlev + 1
555:                    elif initial in ')]}': parenlev = parenlev - 1
556:                    if stashed:
557:                        yield stashed
558:                        stashed = None
559:                    yield (OP, token, spos, epos, line)
560:            else:
561:                yield (ERRORTOKEN, line[pos],
562:                           (lnum, pos), (lnum, pos+1), line)
563:                pos = pos + 1
564:
565:    if stashed:
566:        yield stashed
567:        stashed = None
568:
569:    for indent in indents[1:]:                 # pop remaining indent levels
570:        yield (DEDENT, '', (lnum, 0), (lnum, 0), '')
571:    yield (ENDMARKER, '', (lnum, 0), (lnum, 0), '')
572:
573:if __name__ == '__main__':                     # testing
574:    import sys
575:    if len(sys.argv) > 1: tokenize(open(sys.argv[1]).readline)
576:    else: tokenize(sys.stdin.readline)

Calling tool: grepTool
class Feature\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:133:class Feature(Enum):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/black.py:132:class Feature(Enum):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
120:    PY34 = 4
121:    PY35 = 5
122:    PY36 = 6
123:    PY37 = 7
124:    PY38 = 8
125:
126:    def is_python2(self) -> bool:
127:        return self is TargetVersion.PY27
128:
129:
130:PY36_VERSIONS = {TargetVersion.PY36, TargetVersion.PY37, TargetVersion.PY38}
131:
132:
133:class Feature(Enum):
134:    # All string literals are unicode
135:    UNICODE_LITERALS = 1
136:    F_STRINGS = 2
137:    NUMERIC_UNDERSCORES = 3
138:    TRAILING_COMMA_IN_CALL = 4
139:    TRAILING_COMMA_IN_DEF = 5
140:    # The following two feature-flags are mutually exclusive, and exactly one should be
141:    # set for every version of python.
142:    ASYNC_IS_VALID_IDENTIFIER = 6
143:    ASYNC_IS_RESERVED_KEYWORD = 7
144:
145:
146:VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {
147:    TargetVersion.PY27: {Feature.ASYNC_IS_VALID_IDENTIFIER},
148:    TargetVersion.PY33: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
149:    TargetVersion.PY34: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
150:    TargetVersion.PY35: {
151:        Feature.UNICODE_LITERALS,
152:        Feature.TRAILING_COMMA_IN_CALL,
153:        Feature.ASYNC_IS_VALID_IDENTIFIER,
154:    },
155:    TargetVersion.PY36: {
156:        Feature.UNICODE_LITERALS,
157:        Feature.F_STRINGS,
158:        Feature.NUMERIC_UNDERSCORES,
159:        Feature.TRAILING_COMMA_IN_CALL,
160:        Feature.TRAILING_COMMA_IN_DEF,
161:        Feature.ASYNC_IS_VALID_IDENTIFIER,
162:    },
163:    TargetVersion.PY37: {
164:        Feature.UNICODE_LITERALS,
165:        Feature.F_STRINGS,
166:        Feature.NUMERIC_UNDERSCORES,
167:        Feature.TRAILING_COMMA_IN_CALL,
168:        Feature.TRAILING_COMMA_IN_DEF,
169:        Feature.ASYNC_IS_RESERVED_KEYWORD,
170:    },
171:    TargetVersion.PY38: {
172:        Feature.UNICODE_LITERALS,
173:        Feature.F_STRINGS,
174:        Feature.NUMERIC_UNDERSCORES,
175:        Feature.TRAILING_COMMA_IN_CALL,
176:        Feature.TRAILING_COMMA_IN_DEF,
177:        Feature.ASYNC_IS_RESERVED_KEYWORD,
178:    },
179:}
180:
181:
182:@dataclass
183:class FileMode:
184:    target_versions: Set[TargetVersion] = Factory(set)
185:    line_length: int = DEFAULT_LINE_LENGTH
186:    string_normalization: bool = True
187:    is_pyi: bool = False
188:
189:    def get_cache_key(self) -> str:
190:        if self.target_versions:
191:            version_str = ",".join(
192:                str(version.value)
193:                for version in sorted(self.target_versions, key=lambda v: v.value)
194:            )
195:        else:
196:            version_str = "-"
197:        parts = [
198:            version_str,
199:            str(self.line_length),

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: Verified black.py:get_parser_configs() implementation. For empty targets (lines 770-787) returns four ParserConfigs in required order with tokenizer configs True then False, then Python 2 grammars. For Python 2-only (lines 788-795) returns only two Python 2 grammars without tokenizer overrides. For Python 3 targets (lines 796-818) conditionally adds reserved-keyword then valid-identifier configs based on supports_feature (line 206), preserving reserved-first order. Feature flags mapping ensures mutual exclusivity (lines 146-179).


Todo item 3 of 15 is correct
description: 'Validate get_parser_configs() logic and order for parsing attempts: (a) when target_versions is empty, returns four ParserConfig entries in this order: Python 3 grammar with TokenizerConfig(async_is_reserved_keyword=True), Python 3 grammar with TokenizerConfig(async_is_reserved_keyword=False), Python 2 no_print grammar, Python 2 grammar; (b) when all target_versions are Python 2, returns only the two Python 2 grammars (no TokenizerConfig overrides); (c) when any Python 3 target_versions, returns Python 3 ParserConfigs depending on supports_feature: include reserved-keyword config if ASYNC_IS_VALID_IDENTIFIER is not supported by all versions; include valid-identifier config if ASYNC_IS_RESERVED_KEYWORD is not supported by all versions, with reserved-first order when both are added.'
Validating todo item 4 of 15
description: 'Check supports_feature(target_versions, feature) returns True only if all versions in the set include the feature (i.e., uses all(...)). This ensures mixed targets (e.g., PY36+PY37) will add both parser configs.'
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "def supports_feature\\(",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "def get_parser_configs\\(",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "def get_parser_configs\\(",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "test_async_as_identifier",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black"
]
Calling tool: grepTool
def supports_feature\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:206:def supports_feature(target_versions: Set[TargetVersion], feature: Feature) -> bool:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/black.py:194:def supports_feature(target_versions: Set[TargetVersion], feature: Feature) -> bool:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
170:    },
171:    TargetVersion.PY38: {
172:        Feature.UNICODE_LITERALS,
173:        Feature.F_STRINGS,
174:        Feature.NUMERIC_UNDERSCORES,
175:        Feature.TRAILING_COMMA_IN_CALL,
176:        Feature.TRAILING_COMMA_IN_DEF,
177:        Feature.ASYNC_IS_RESERVED_KEYWORD,
178:    },
179:}
180:
181:
182:@dataclass
183:class FileMode:
184:    target_versions: Set[TargetVersion] = Factory(set)
185:    line_length: int = DEFAULT_LINE_LENGTH
186:    string_normalization: bool = True
187:    is_pyi: bool = False
188:
189:    def get_cache_key(self) -> str:
190:        if self.target_versions:
191:            version_str = ",".join(
192:                str(version.value)
193:                for version in sorted(self.target_versions, key=lambda v: v.value)
194:            )
195:        else:
196:            version_str = "-"
197:        parts = [
198:            version_str,
199:            str(self.line_length),
200:            str(int(self.string_normalization)),
201:            str(int(self.is_pyi)),
202:        ]
203:        return ".".join(parts)
204:
205:
206:def supports_feature(target_versions: Set[TargetVersion], feature: Feature) -> bool:
207:    return all(feature in VERSION_TO_FEATURES[version] for version in target_versions)
208:
209:
210:def read_pyproject_toml(
211:    ctx: click.Context, param: click.Parameter, value: Union[str, int, bool, None]
212:) -> Optional[str]:
213:    """Inject Black configuration from "pyproject.toml" into defaults in `ctx`.
214:
215:    Returns the path to a successfully found and read configuration file, None
216:    otherwise.
217:    """
218:    assert not isinstance(value, (int, bool)), "Invalid parameter type passed"
219:    if not value:
220:        root = find_project_root(ctx.params.get("src", ()))
221:        path = root / "pyproject.toml"
222:        if path.is_file():
223:            value = str(path)
224:        else:
225:            return None
226:
227:    try:
228:        pyproject_toml = toml.load(value)
229:        config = pyproject_toml.get("tool", {}).get("black", {})
230:    except (toml.TomlDecodeError, OSError) as e:
231:        raise click.FileError(
232:            filename=value, hint=f"Error reading configuration file: {e}"
233:        )
234:
235:    if not config:
236:        return None
237:
238:    if ctx.default_map is None:
239:        ctx.default_map = {}
240:    ctx.default_map.update(  # type: ignore  # bad types in .pyi
241:        {k.replace("--", "").replace("-", "_"): v for k, v in config.items()}
242:    )
243:    return value
244:
245:
246:@click.command(context_settings=dict(help_option_names=["-h", "--help"]))
247:@click.option("-c", "--code", type=str, help="Format the code passed in as a string.")
248:@click.option(
249:    "-l",

Calling tool: grepTool
def get_parser_configs\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
748:
749:    `newline` is either CRLF or LF but `decoded_contents` is decoded with
750:    universal newlines (i.e. only contains LF).
751:    """
752:    srcbuf = io.BytesIO(src)
753:    encoding, lines = tokenize.detect_encoding(srcbuf.readline)
754:    if not lines:
755:        return "", encoding, "\n"
756:
757:    newline = "\r\n" if b"\r\n" == lines[0][-2:] else "\n"
758:    srcbuf.seek(0)
759:    with io.TextIOWrapper(srcbuf, encoding) as tiow:
760:        return tiow.read(), encoding, newline
761:
762:
763:@dataclass(frozen=True)
764:class ParserConfig:
765:    grammar: Grammar
766:    tokenizer_config: TokenizerConfig = TokenizerConfig()
767:
768:
769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:
770:    if not target_versions:
771:        # No target_version specified, so try all grammars.
772:        return [
773:            # Python 3.7+
774:            ParserConfig(
775:                pygram.python_grammar_no_print_statement_no_exec_statement,
776:                TokenizerConfig(async_is_reserved_keyword=True),
777:            ),
778:            # Python 3.0-3.6
779:            ParserConfig(
780:                pygram.python_grammar_no_print_statement_no_exec_statement,
781:                TokenizerConfig(async_is_reserved_keyword=False),
782:            ),
783:            # Python 2.7 with future print_function import
784:            ParserConfig(pygram.python_grammar_no_print_statement),
785:            # Python 2.7
786:            ParserConfig(pygram.python_grammar),
787:        ]
788:    elif all(version.is_python2() for version in target_versions):
789:        # Python 2-only code, so try Python 2 grammars.
790:        return [
791:            # Python 2.7 with future print_function import
792:            ParserConfig(pygram.python_grammar_no_print_statement),
793:            # Python 2.7
794:            ParserConfig(pygram.python_grammar),
795:        ]
796:    else:
797:        # Python 3-compatible code, so only try Python 3 grammar.
798:        configs = []
799:        # If we have to parse both, try to parse async as a keyword first
800:        if not supports_feature(target_versions, Feature.ASYNC_IS_VALID_IDENTIFIER):
801:            # Python 3.7+
802:            configs.append(
803:                ParserConfig(
804:                    pygram.python_grammar_no_print_statement_no_exec_statement,
805:                    TokenizerConfig(async_is_reserved_keyword=True),
806:                )
807:            )
808:        if not supports_feature(target_versions, Feature.ASYNC_IS_RESERVED_KEYWORD):
809:            # Python 3.0-3.6
810:            configs.append(
811:                ParserConfig(
812:                    pygram.python_grammar_no_print_statement_no_exec_statement,
813:                    TokenizerConfig(async_is_reserved_keyword=False),
814:                )
815:            )
816:        # At least one of the above branches must have been taken, because every Python
817:        # version has exactly one of the two 'ASYNC_IS_*' flags
818:        return configs
819:
820:
821:def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:
822:    """Given a string with source, return the lib2to3 Node."""
823:    if src_txt[-1:] != "\n":
824:        src_txt += "\n"
825:
826:    for parser_config in get_parser_configs(set(target_versions)):
827:        drv = driver.Driver(
828:            parser_config.grammar,
829:            pytree.convert,
830:            tokenizer_config=parser_config.tokenizer_config,
831:        )
832:        try:
833:            result = drv.parse_string(src_txt, True)
834:            break
835:
836:        except ParseError as pe:
837:            lineno, column = pe.context[1]
838:            lines = src_txt.splitlines()
839:            try:
840:                faulty_line = lines[lineno - 1]
841:            except IndexError:
842:                faulty_line = "<line number missing in source>"
843:            exc = InvalidInput(f"Cannot parse: {lineno}:{column}: {faulty_line}")
844:    else:
845:        raise exc from None
846:
847:    if isinstance(result, Leaf):
848:        result = Node(syms.file_input, [result])
849:    return result
850:
851:
852:def lib2to3_unparse(node: Node) -> str:
853:    """Given a lib2to3 node, return its string representation."""
854:    code = str(node)
855:    return code
856:
857:
858:T = TypeVar("T")
859:
860:
861:class Visitor(Generic[T]):
862:    """Basic lib2to3 visitor that yields things of type `T` on `visit()`."""
863:
864:    def visit(self, node: LN) -> Iterator[T]:
865:        """Main method to visit `node` and its children.
866:
867:        It tries to find a `visit_*()` method for the given `node.type`, like

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/black.py
160:    TargetVersion.PY38: {
161:        Feature.UNICODE_LITERALS,
162:        Feature.F_STRINGS,
163:        Feature.NUMERIC_UNDERSCORES,
164:        Feature.TRAILING_COMMA_IN_CALL,
165:        Feature.TRAILING_COMMA_IN_DEF,
166:    },
167:}
168:
169:
170:@dataclass
171:class FileMode:
172:    target_versions: Set[TargetVersion] = Factory(set)
173:    line_length: int = DEFAULT_LINE_LENGTH
174:    string_normalization: bool = True
175:    is_pyi: bool = False
176:
177:    def get_cache_key(self) -> str:
178:        if self.target_versions:
179:            version_str = ",".join(
180:                str(version.value)
181:                for version in sorted(self.target_versions, key=lambda v: v.value)
182:            )
183:        else:
184:            version_str = "-"
185:        parts = [
186:            version_str,
187:            str(self.line_length),
188:            str(int(self.string_normalization)),
189:            str(int(self.is_pyi)),
190:        ]
191:        return ".".join(parts)
192:
193:
194:def supports_feature(target_versions: Set[TargetVersion], feature: Feature) -> bool:
195:    return all(feature in VERSION_TO_FEATURES[version] for version in target_versions)
196:
197:
198:def read_pyproject_toml(
199:    ctx: click.Context, param: click.Parameter, value: Union[str, int, bool, None]
200:) -> Optional[str]:
201:    """Inject Black configuration from "pyproject.toml" into defaults in `ctx`.
202:
203:    Returns the path to a successfully found and read configuration file, None
204:    otherwise.
205:    """
206:    assert not isinstance(value, (int, bool)), "Invalid parameter type passed"
207:    if not value:
208:        root = find_project_root(ctx.params.get("src", ()))
209:        path = root / "pyproject.toml"
210:        if path.is_file():
211:            value = str(path)
212:        else:
213:            return None
214:
215:    try:
216:        pyproject_toml = toml.load(value)
217:        config = pyproject_toml.get("tool", {}).get("black", {})
218:    except (toml.TomlDecodeError, OSError) as e:
219:        raise click.FileError(
220:            filename=value, hint=f"Error reading configuration file: {e}"
221:        )
222:
223:    if not config:
224:        return None
225:
226:    if ctx.default_map is None:
227:        ctx.default_map = {}
228:    ctx.default_map.update(  # type: ignore  # bad types in .pyi
229:        {k.replace("--", "").replace("-", "_"): v for k, v in config.items()}
230:    )
231:    return value
232:
233:
234:@click.command(context_settings=dict(help_option_names=["-h", "--help"]))
235:@click.option("-c", "--code", type=str, help="Format the code passed in as a string.")
236:@click.option(
237:    "-l",
238:    "--line-length",
239:    type=int,

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
120:    PY34 = 4
121:    PY35 = 5
122:    PY36 = 6
123:    PY37 = 7
124:    PY38 = 8
125:
126:    def is_python2(self) -> bool:
127:        return self is TargetVersion.PY27
128:
129:
130:PY36_VERSIONS = {TargetVersion.PY36, TargetVersion.PY37, TargetVersion.PY38}
131:
132:
133:class Feature(Enum):
134:    # All string literals are unicode
135:    UNICODE_LITERALS = 1
136:    F_STRINGS = 2
137:    NUMERIC_UNDERSCORES = 3
138:    TRAILING_COMMA_IN_CALL = 4
139:    TRAILING_COMMA_IN_DEF = 5
140:    # The following two feature-flags are mutually exclusive, and exactly one should be
141:    # set for every version of python.
142:    ASYNC_IS_VALID_IDENTIFIER = 6
143:    ASYNC_IS_RESERVED_KEYWORD = 7
144:
145:
146:VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {
147:    TargetVersion.PY27: {Feature.ASYNC_IS_VALID_IDENTIFIER},
148:    TargetVersion.PY33: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
149:    TargetVersion.PY34: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
150:    TargetVersion.PY35: {
151:        Feature.UNICODE_LITERALS,
152:        Feature.TRAILING_COMMA_IN_CALL,
153:        Feature.ASYNC_IS_VALID_IDENTIFIER,
154:    },
155:    TargetVersion.PY36: {
156:        Feature.UNICODE_LITERALS,
157:        Feature.F_STRINGS,
158:        Feature.NUMERIC_UNDERSCORES,
159:        Feature.TRAILING_COMMA_IN_CALL,
160:        Feature.TRAILING_COMMA_IN_DEF,
161:        Feature.ASYNC_IS_VALID_IDENTIFIER,
162:    },
163:    TargetVersion.PY37: {
164:        Feature.UNICODE_LITERALS,
165:        Feature.F_STRINGS,
166:        Feature.NUMERIC_UNDERSCORES,
167:        Feature.TRAILING_COMMA_IN_CALL,
168:        Feature.TRAILING_COMMA_IN_DEF,
169:        Feature.ASYNC_IS_RESERVED_KEYWORD,
170:    },
171:    TargetVersion.PY38: {
172:        Feature.UNICODE_LITERALS,
173:        Feature.F_STRINGS,
174:        Feature.NUMERIC_UNDERSCORES,
175:        Feature.TRAILING_COMMA_IN_CALL,
176:        Feature.TRAILING_COMMA_IN_DEF,
177:        Feature.ASYNC_IS_RESERVED_KEYWORD,
178:    },
179:}
180:
181:
182:@dataclass
183:class FileMode:
184:    target_versions: Set[TargetVersion] = Factory(set)
185:    line_length: int = DEFAULT_LINE_LENGTH
186:    string_normalization: bool = True
187:    is_pyi: bool = False
188:
189:    def get_cache_key(self) -> str:

Calling tool: grepTool
def get_parser_configs\(
No files found. Call grepTool again with a more generic pattern.

Calling tool: grepTool
test_async_as_identifier
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/coverage_bugsinpy.txt:1:coverage run -m unittest -q tests.test_black.BlackTestCase.test_async_as_identifier
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:506:    def test_async_as_identifier(self) -> None:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/bugsinpy_run_test.sh:1:python -m unittest -q tests.test_black.BlackTestCase.test_async_as_identifier

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py
470:        black.assert_stable(source, actual, black.FileMode())
471:
472:    @patch("black.dump_to_file", dump_to_stderr)
473:    def test_python2(self) -> None:
474:        source, expected = read_data("python2")
475:        actual = fs(source)
476:        self.assertFormatEqual(expected, actual)
477:        black.assert_equivalent(source, actual)
478:        black.assert_stable(source, actual, black.FileMode())
479:
480:    @patch("black.dump_to_file", dump_to_stderr)
481:    def test_python2_print_function(self) -> None:
482:        source, expected = read_data("python2_print_function")
483:        mode = black.FileMode(target_versions={TargetVersion.PY27})
484:        actual = fs(source, mode=mode)
485:        self.assertFormatEqual(expected, actual)
486:        black.assert_equivalent(source, actual)
487:        black.assert_stable(source, actual, mode)
488:
489:    @patch("black.dump_to_file", dump_to_stderr)
490:    def test_python2_unicode_literals(self) -> None:
491:        source, expected = read_data("python2_unicode_literals")
492:        actual = fs(source)
493:        self.assertFormatEqual(expected, actual)
494:        black.assert_equivalent(source, actual)
495:        black.assert_stable(source, actual, black.FileMode())
496:
497:    @patch("black.dump_to_file", dump_to_stderr)
498:    def test_stub(self) -> None:
499:        mode = black.FileMode(is_pyi=True)
500:        source, expected = read_data("stub.pyi")
501:        actual = fs(source, mode=mode)
502:        self.assertFormatEqual(expected, actual)
503:        black.assert_stable(source, actual, mode)
504:
505:    @patch("black.dump_to_file", dump_to_stderr)
506:    def test_async_as_identifier(self) -> None:
507:        source_path = (THIS_DIR / "data" / "async_as_identifier.py").resolve()
508:        source, expected = read_data("async_as_identifier")
509:        actual = fs(source)
510:        self.assertFormatEqual(expected, actual)
511:        major, minor = sys.version_info[:2]
512:        if major < 3 or (major <= 3 and minor < 7):
513:            black.assert_equivalent(source, actual)
514:        black.assert_stable(source, actual, black.FileMode())
515:        # ensure black can parse this when the target is 3.6
516:        self.invokeBlack([str(source_path), "--target-version", "py36"])
517:        # but not on 3.7, because async/await is no longer an identifier
518:        self.invokeBlack([str(source_path), "--target-version", "py37"], exit_code=123)
519:
520:    @patch("black.dump_to_file", dump_to_stderr)
521:    def test_python37(self) -> None:
522:        source_path = (THIS_DIR / "data" / "python37.py").resolve()
523:        source, expected = read_data("python37")
524:        actual = fs(source)
525:        self.assertFormatEqual(expected, actual)
526:        major, minor = sys.version_info[:2]
527:        if major > 3 or (major == 3 and minor >= 7):
528:            black.assert_equivalent(source, actual)
529:        black.assert_stable(source, actual, black.FileMode())
530:        # ensure black can parse this when the target is 3.7
531:        self.invokeBlack([str(source_path), "--target-version", "py37"])
532:        # but not on 3.6, because we use async as a reserved keyword
533:        self.invokeBlack([str(source_path), "--target-version", "py36"], exit_code=123)
534:
535:    @patch("black.dump_to_file", dump_to_stderr)
536:    def test_fmtonoff(self) -> None:
537:        source, expected = read_data("fmtonoff")
538:        actual = fs(source)
539:        self.assertFormatEqual(expected, actual)
540:        black.assert_equivalent(source, actual)
541:        black.assert_stable(source, actual, black.FileMode())
542:
543:    @patch("black.dump_to_file", dump_to_stderr)
544:    def test_fmtonoff2(self) -> None:
545:        source, expected = read_data("fmtonoff2")
546:        actual = fs(source)
547:        self.assertFormatEqual(expected, actual)
548:        black.assert_equivalent(source, actual)
549:        black.assert_stable(source, actual, black.FileMode())

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: supports_feature is implemented as `all(feature in VERSION_TO_FEATURES[version] for version in target_versions)`, ensuring True only if all versions include the feature. Consequently, in mixed targets (e.g., PY36+PY37), both async tokenizer parser configs are added in get_parser_configs.


Todo item 4 of 15 is correct
description: 'Check supports_feature(target_versions, feature) returns True only if all versions in the set include the feature (i.e., uses all(...)). This ensures mixed targets (e.g., PY36+PY37) will add both parser configs.'
Validating todo item 5 of 15
description: 'Confirm lib2to3_parse() iterates over get_parser_configs() results, constructs driver.Driver with parser_config.grammar and tokenizer_config, and passes tokenizer_config through to parsing (no regression in error handling: InvalidInput raised only after all attempts fail and includes correct location info).'
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "def lib2to3_parse\\(",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "class InvalidInput",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "get_grammars\\(", "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black" ]
Calling tool: grepTool
def lib2to3_parse\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:821:def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/black.py:767:def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
730:        for feature in {Feature.TRAILING_COMMA_IN_CALL, Feature.TRAILING_COMMA_IN_DEF}
731:        if supports_feature(versions, feature)
732:    }
733:    for current_line in lines.visit(src_node):
734:        for _ in range(after):
735:            dst_contents += str(empty_line)
736:        before, after = elt.maybe_empty_lines(current_line)
737:        for _ in range(before):
738:            dst_contents += str(empty_line)
739:        for line in split_line(
740:            current_line, line_length=mode.line_length, features=split_line_features
741:        ):
742:            dst_contents += str(line)
743:    return dst_contents
744:
745:
746:def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:
747:    """Return a tuple of (decoded_contents, encoding, newline).
748:
749:    `newline` is either CRLF or LF but `decoded_contents` is decoded with
750:    universal newlines (i.e. only contains LF).
751:    """
752:    srcbuf = io.BytesIO(src)
753:    encoding, lines = tokenize.detect_encoding(srcbuf.readline)
754:    if not lines:
755:        return "", encoding, "\n"
756:
757:    newline = "\r\n" if b"\r\n" == lines[0][-2:] else "\n"
758:    srcbuf.seek(0)
759:    with io.TextIOWrapper(srcbuf, encoding) as tiow:
760:        return tiow.read(), encoding, newline
761:
762:
763:@dataclass(frozen=True)
764:class ParserConfig:
765:    grammar: Grammar
766:    tokenizer_config: TokenizerConfig = TokenizerConfig()
767:
768:
769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:
770:    if not target_versions:
771:        # No target_version specified, so try all grammars.
772:        return [
773:            # Python 3.7+
774:            ParserConfig(
775:                pygram.python_grammar_no_print_statement_no_exec_statement,
776:                TokenizerConfig(async_is_reserved_keyword=True),
777:            ),
778:            # Python 3.0-3.6
779:            ParserConfig(
780:                pygram.python_grammar_no_print_statement_no_exec_statement,
781:                TokenizerConfig(async_is_reserved_keyword=False),
782:            ),
783:            # Python 2.7 with future print_function import
784:            ParserConfig(pygram.python_grammar_no_print_statement),
785:            # Python 2.7
786:            ParserConfig(pygram.python_grammar),
787:        ]
788:    elif all(version.is_python2() for version in target_versions):
789:        # Python 2-only code, so try Python 2 grammars.
790:        return [
791:            # Python 2.7 with future print_function import
792:            ParserConfig(pygram.python_grammar_no_print_statement),
793:            # Python 2.7
794:            ParserConfig(pygram.python_grammar),
795:        ]
796:    else:
797:        # Python 3-compatible code, so only try Python 3 grammar.
798:        configs = []
799:        # If we have to parse both, try to parse async as a keyword first
800:        if not supports_feature(target_versions, Feature.ASYNC_IS_VALID_IDENTIFIER):
801:            # Python 3.7+
802:            configs.append(
803:                ParserConfig(
804:                    pygram.python_grammar_no_print_statement_no_exec_statement,
805:                    TokenizerConfig(async_is_reserved_keyword=True),
806:                )
807:            )
808:        if not supports_feature(target_versions, Feature.ASYNC_IS_RESERVED_KEYWORD):
809:            # Python 3.0-3.6
810:            configs.append(
811:                ParserConfig(
812:                    pygram.python_grammar_no_print_statement_no_exec_statement,
813:                    TokenizerConfig(async_is_reserved_keyword=False),
814:                )
815:            )
816:        # At least one of the above branches must have been taken, because every Python
817:        # version has exactly one of the two 'ASYNC_IS_*' flags
818:        return configs
819:
820:
821:def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:
822:    """Given a string with source, return the lib2to3 Node."""
823:    if src_txt[-1:] != "\n":
824:        src_txt += "\n"
825:
826:    for parser_config in get_parser_configs(set(target_versions)):
827:        drv = driver.Driver(
828:            parser_config.grammar,
829:            pytree.convert,
830:            tokenizer_config=parser_config.tokenizer_config,
831:        )
832:        try:
833:            result = drv.parse_string(src_txt, True)
834:            break
835:
836:        except ParseError as pe:
837:            lineno, column = pe.context[1]
838:            lines = src_txt.splitlines()
839:            try:
840:                faulty_line = lines[lineno - 1]
841:            except IndexError:
842:                faulty_line = "<line number missing in source>"
843:            exc = InvalidInput(f"Cannot parse: {lineno}:{column}: {faulty_line}")
844:    else:
845:        raise exc from None
846:
847:    if isinstance(result, Leaf):
848:        result = Node(syms.file_input, [result])
849:    return result
850:
851:
852:def lib2to3_unparse(node: Node) -> str:
853:    """Given a lib2to3 node, return its string representation."""
854:    code = str(node)
855:    return code
856:
857:
858:T = TypeVar("T")
859:
860:
861:class Visitor(Generic[T]):
862:    """Basic lib2to3 visitor that yields things of type `T` on `visit()`."""
863:
864:    def visit(self, node: LN) -> Iterator[T]:
865:        """Main method to visit `node` and its children.
866:
867:        It tries to find a `visit_*()` method for the given `node.type`, like
868:        `visit_simple_stmt` for Node objects or `visit_INDENT` for Leaf objects.
869:        If no dedicated `visit_*()` method is found, chooses `visit_default()`

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Modifications:
5:# Copyright 2006 Google, Inc. All Rights Reserved.
6:# Licensed to PSF under a Contributor Agreement.
7:
8:"""Parser driver.
9:
10:This provides a high-level interface to parse a file into a syntax tree.
11:
12:"""
13:
14:__author__ = "Guido van Rossum <guido@python.org>"
15:
16:__all__ = ["Driver", "load_grammar"]
17:
18:# Python imports
19:import codecs
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(
33:        self,
34:        grammar,
35:        convert=None,
36:        logger=None,
37:        tokenizer_config=tokenize.TokenizerConfig(),
38:    ):
39:        self.grammar = grammar
40:        if logger is None:
41:            logger = logging.getLogger(__name__)
42:        self.logger = logger
43:        self.convert = convert
44:        self.tokenizer_config = tokenizer_config
45:
46:    def parse_tokens(self, tokens, debug=False):
47:        """Parse a series of tokens and return the syntax tree."""
48:        # XXX Move the prefix computation into a wrapper around tokenize.
49:        p = parse.Parser(self.grammar, self.convert)
50:        p.setup()
51:        lineno = 1
52:        column = 0
53:        indent_columns = []
54:        type = value = start = end = line_text = None
55:        prefix = ""
56:        for quintuple in tokens:
57:            type, value, start, end, line_text = quintuple
58:            if start != (lineno, column):
59:                assert (lineno, column) <= start, ((lineno, column), start)
60:                s_lineno, s_column = start
61:                if lineno < s_lineno:
62:                    prefix += "\n" * (s_lineno - lineno)
63:                    lineno = s_lineno
64:                    column = 0
65:                if column < s_column:
66:                    prefix += line_text[column:s_column]
67:                    column = s_column
68:            if type in (tokenize.COMMENT, tokenize.NL):
69:                prefix += value
70:                lineno, column = end
71:                if value.endswith("\n"):
72:                    lineno += 1
73:                    column = 0
74:                continue
75:            if type == token.OP:
76:                type = grammar.opmap[value]
77:            if debug:
78:                self.logger.debug("%s %r (prefix=%r)",
79:                                  token.tok_name[type], value, prefix)
80:            if type == token.INDENT:
81:                indent_columns.append(len(value))
82:                _prefix = prefix + value
83:                prefix = ""
84:                value = ""
85:            elif type == token.DEDENT:
86:                _indent_col = indent_columns.pop()
87:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
88:            if p.addtoken(type, value, (prefix, start)):
89:                if debug:
90:                    self.logger.debug("Stop.")
91:                break
92:            prefix = ""
93:            if type in {token.INDENT, token.DEDENT}:
94:                prefix = _prefix
95:            lineno, column = end
96:            if value.endswith("\n"):
97:                lineno += 1
98:                column = 0
99:        else:
100:            # We never broke out -- EOF is too soon (how can this happen???)
101:            raise parse.ParseError("incomplete input",
102:                                   type, value, (prefix, start))
103:        return p.rootnode
104:
105:    def parse_stream_raw(self, stream, debug=False):
106:        """Parse a stream and return the syntax tree."""
107:        tokens = tokenize.generate_tokens(stream.readline, config=self.tokenizer_config)
108:        return self.parse_tokens(tokens, debug)
109:
110:    def parse_stream(self, stream, debug=False):
111:        """Parse a stream and return the syntax tree."""
112:        return self.parse_stream_raw(stream, debug)
113:
114:    def parse_file(self, filename, encoding=None, debug=False):
115:        """Parse a file and return the syntax tree."""
116:        with io.open(filename, "r", encoding=encoding) as stream:
117:            return self.parse_stream(stream, debug)
118:
119:    def parse_string(self, text, debug=False):
120:        """Parse a string and return the syntax tree."""
121:        tokens = tokenize.generate_tokens(
122:            io.StringIO(text).readline,
123:            config=self.tokenizer_config,
124:        )
125:        return self.parse_tokens(tokens, debug)
126:
127:    def _partially_consume_prefix(self, prefix, column):
128:        lines = []
129:        current_line = ""
130:        current_column = 0
131:        wait_for_nl = False
132:        for char in prefix:
133:            current_line += char
134:            if wait_for_nl:
135:                if char == '\n':
136:                    if current_line.strip() and current_column < column:
137:                        res = ''.join(lines)
138:                        return res, prefix[len(res):]
139:
140:                    lines.append(current_line)
141:                    current_line = ""
142:                    current_column = 0
143:                    wait_for_nl = False
144:            elif char in ' \t':
145:                current_column += 1
146:            elif char == '\n':
147:                # unexpected empty line
148:                current_column = 0
149:            else:
150:                # indent is finished
151:                wait_for_nl = True
152:        return ''.join(lines), current_line
153:
154:
155:def _generate_pickle_name(gt, cache_dir=None):
156:    head, tail = os.path.splitext(gt)
157:    if tail == ".txt":
158:        tail = ""
159:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
160:    if cache_dir:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
1:# Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006 Python Software Foundation.
2:# All rights reserved.
3:
4:"""Tokenization help for Python programs.
5:
6:generate_tokens(readline) is a generator that breaks a stream of
7:text into Python tokens.  It accepts a readline-like method which is called
8:repeatedly to get the next line of input (or "" for EOF).  It generates
9:5-tuples with these members:
10:
11:    the token type (see token.py)
12:    the token (a string)
13:    the starting (row, column) indices of the token (a 2-tuple of ints)
14:    the ending (row, column) indices of the token (a 2-tuple of ints)
15:    the original line (string)
16:
17:It is designed to match the working of the Python tokenizer exactly, except
18:that it produces COMMENT tokens for comments and gives type OP for all
19:operators
20:
21:Older entry points
22:    tokenize_loop(readline, tokeneater)
23:    tokenize(readline, tokeneater=printtoken)
24:are the same, except instead of generating tokens, tokeneater is a callback
25:function to which the 5 fields described above are passed as 5 arguments,
26:each time a new token is found."""
27:
28:__author__ = 'Ka-Ping Yee <ping@lfw.org>'
29:__credits__ = \
30:    'GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro'
31:
32:import re
33:from codecs import BOM_UTF8, lookup
34:from attr import dataclass
35:from blib2to3.pgen2.token import *
36:
37:from . import token
38:__all__ = [x for x in dir(token) if x[0] != '_'] + ["tokenize",
39:           "generate_tokens", "untokenize"]
40:del token
41:
42:try:
43:    bytes
44:except NameError:
45:    # Support bytes type in Python <= 2.5, so 2to3 turns itself into
46:    # valid Python 3 code.
47:    bytes = str
48:
49:def group(*choices): return '(' + '|'.join(choices) + ')'
50:def any(*choices): return group(*choices) + '*'
51:def maybe(*choices): return group(*choices) + '?'
52:def _combinations(*l):
53:    return set(
54:        x + y for x in l for y in l + ("",) if x.casefold() != y.casefold()
55:    )
56:
57:Whitespace = r'[ \f\t]*'
58:Comment = r'#[^\r\n]*'
59:Ignore = Whitespace + any(r'\\\r?\n' + Whitespace) + maybe(Comment)
60:Name = r'\w+'  # this is invalid but it's fine because Name comes after Number in all groups
61:
62:Binnumber = r'0[bB]_?[01]+(?:_[01]+)*'
63:Hexnumber = r'0[xX]_?[\da-fA-F]+(?:_[\da-fA-F]+)*[lL]?'
64:Octnumber = r'0[oO]?_?[0-7]+(?:_[0-7]+)*[lL]?'
65:Decnumber = group(r'[1-9]\d*(?:_\d+)*[lL]?', '0[lL]?')
66:Intnumber = group(Binnumber, Hexnumber, Octnumber, Decnumber)
67:Exponent = r'[eE][-+]?\d+(?:_\d+)*'
68:Pointfloat = group(r'\d+(?:_\d+)*\.(?:\d+(?:_\d+)*)?', r'\.\d+(?:_\d+)*') + maybe(Exponent)
69:Expfloat = r'\d+(?:_\d+)*' + Exponent
70:Floatnumber = group(Pointfloat, Expfloat)
71:Imagnumber = group(r'\d+(?:_\d+)*[jJ]', Floatnumber + r'[jJ]')
72:Number = group(Imagnumber, Floatnumber, Intnumber)
73:
74:# Tail end of ' string.
75:Single = r"[^'\\]*(?:\\.[^'\\]*)*'"
76:# Tail end of " string.
77:Double = r'[^"\\]*(?:\\.[^"\\]*)*"'
78:# Tail end of ''' string.
79:Single3 = r"[^'\\]*(?:(?:\\.|'(?!''))[^'\\]*)*'''"
80:# Tail end of """ string.
81:Double3 = r'[^"\\]*(?:(?:\\.|"(?!""))[^"\\]*)*"""'
82:_litprefix = r"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?"
83:Triple = group(_litprefix + "'''", _litprefix + '"""')
84:# Single-line ' or " string.
85:String = group(_litprefix + r"'[^\n'\\]*(?:\\.[^\n'\\]*)*'",
86:               _litprefix + r'"[^\n"\\]*(?:\\.[^\n"\\]*)*"')
87:
88:# Because of leftmost-then-longest match semantics, be sure to put the
89:# longest operators first (e.g., if = came before ==, == would get
90:# recognized as two instances of =).
91:Operator = group(r"\*\*=?", r">>=?", r"<<=?", r"<>", r"!=",
92:                 r"//=?", r"->",
93:                 r"[+\-*/%&@|^=<>]=?",
94:                 r"~")
95:
96:Bracket = '[][(){}]'
97:Special = group(r'\r?\n', r'[:;.,`@]')
98:Funny = group(Operator, Bracket, Special)
99:
100:PlainToken = group(Number, Funny, String, Name)
101:Token = Ignore + PlainToken
102:
103:# First (or only) line of ' or " string.
104:ContStr = group(_litprefix + r"'[^\n'\\]*(?:\\.[^\n'\\]*)*" +
105:                group("'", r'\\\r?\n'),
106:                _litprefix + r'"[^\n"\\]*(?:\\.[^\n"\\]*)*' +
107:                group('"', r'\\\r?\n'))
108:PseudoExtras = group(r'\\\r?\n', Comment, Triple)
109:PseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)
110:
111:tokenprog = re.compile(Token, re.UNICODE)
112:pseudoprog = re.compile(PseudoToken, re.UNICODE)
113:single3prog = re.compile(Single3)
114:double3prog = re.compile(Double3)
115:
116:_strprefixes = (
117:    _combinations('r', 'R', 'f', 'F') |
118:    _combinations('r', 'R', 'b', 'B') |
119:    {'u', 'U', 'ur', 'uR', 'Ur', 'UR'}
120:)
121:
122:endprogs = {"'": re.compile(Single), '"': re.compile(Double),
123:            "'''": single3prog, '"""': double3prog,
124:            **{f"{prefix}'''": single3prog for prefix in _strprefixes},
125:            **{f'{prefix}"""': double3prog for prefix in _strprefixes},
126:            **{prefix: None for prefix in _strprefixes}}
127:
128:triple_quoted = (
129:    {"'''", '"""'} |
130:    {f"{prefix}'''" for prefix in _strprefixes} |
131:    {f'{prefix}"""' for prefix in _strprefixes}
132:)
133:single_quoted = (
134:    {"'", '"'} |
135:    {f"{prefix}'" for prefix in _strprefixes} |
136:    {f'{prefix}"' for prefix in _strprefixes}
137:)
138:
139:tabsize = 8
140:
141:@dataclass(frozen=True)
142:class TokenizerConfig:
143:    async_is_reserved_keyword: bool = False
144:
145:class TokenError(Exception): pass
146:
147:class StopTokenizing(Exception): pass
148:
149:def printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line): # for testing
150:    (srow, scol) = xxx_todo_changeme
151:    (erow, ecol) = xxx_todo_changeme1
152:    print("%d,%d-%d,%d:\t%s\t%s" % \
153:        (srow, scol, erow, ecol, tok_name[type], repr(token)))
154:
155:def tokenize(readline, tokeneater=printtoken):
156:    """
157:    The tokenize() function accepts two parameters: one representing the
158:    input stream, and one providing an output mechanism for tokenize().
159:
160:    The first parameter, readline, must be a callable object which provides
161:    the same interface as the readline() method of built-in file objects.
162:    Each call to the function should return one line of input as a string.
163:
164:    The second parameter, tokeneater, must also be a callable object. It is
165:    called once for each token, with five arguments, corresponding to the
166:    tuples generated by generate_tokens().
167:    """
168:    try:
169:        tokenize_loop(readline, tokeneater)
170:    except StopTokenizing:
171:        pass
172:
173:# backwards compatible interface
174:def tokenize_loop(readline, tokeneater):
175:    for token_info in generate_tokens(readline):
176:        tokeneater(*token_info)
177:
178:class Untokenizer:
179:
180:    def __init__(self):
181:        self.tokens = []
182:        self.prev_row = 1
183:        self.prev_col = 0
184:
185:    def add_whitespace(self, start):
186:        row, col = start
187:        assert row <= self.prev_row
188:        col_offset = col - self.prev_col
189:        if col_offset:
190:            self.tokens.append(" " * col_offset)
191:
192:    def untokenize(self, iterable):
193:        for t in iterable:
194:            if len(t) == 2:
195:                self.compat(t, iterable)
196:                break
197:            tok_type, token, start, end, line = t
198:            self.add_whitespace(start)
199:            self.tokens.append(token)
200:            self.prev_row, self.prev_col = end

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
200:            self.prev_row, self.prev_col = end
201:            if tok_type in (NEWLINE, NL):
202:                self.prev_row += 1
203:                self.prev_col = 0
204:        return "".join(self.tokens)
205:
206:    def compat(self, token, iterable):
207:        startline = False
208:        indents = []
209:        toks_append = self.tokens.append
210:        toknum, tokval = token
211:        if toknum in (NAME, NUMBER):
212:            tokval += ' '
213:        if toknum in (NEWLINE, NL):
214:            startline = True
215:        for tok in iterable:
216:            toknum, tokval = tok[:2]
217:
218:            if toknum in (NAME, NUMBER, ASYNC, AWAIT):
219:                tokval += ' '
220:
221:            if toknum == INDENT:
222:                indents.append(tokval)
223:                continue
224:            elif toknum == DEDENT:
225:                indents.pop()
226:                continue
227:            elif toknum in (NEWLINE, NL):
228:                startline = True
229:            elif startline and indents:
230:                toks_append(indents[-1])
231:                startline = False
232:            toks_append(tokval)
233:
234:cookie_re = re.compile(r'^[ \t\f]*#.*?coding[:=][ \t]*([-\w.]+)', re.ASCII)
235:blank_re = re.compile(br'^[ \t\f]*(?:[#\r\n]|$)', re.ASCII)
236:
237:def _get_normal_name(orig_enc):
238:    """Imitates get_normal_name in tokenizer.c."""
239:    # Only care about the first 12 characters.
240:    enc = orig_enc[:12].lower().replace("_", "-")
241:    if enc == "utf-8" or enc.startswith("utf-8-"):
242:        return "utf-8"
243:    if enc in ("latin-1", "iso-8859-1", "iso-latin-1") or \
244:       enc.startswith(("latin-1-", "iso-8859-1-", "iso-latin-1-")):
245:        return "iso-8859-1"
246:    return orig_enc
247:
248:def detect_encoding(readline):
249:    """
250:    The detect_encoding() function is used to detect the encoding that should
251:    be used to decode a Python source file. It requires one argument, readline,
252:    in the same way as the tokenize() generator.
253:
254:    It will call readline a maximum of twice, and return the encoding used
255:    (as a string) and a list of any lines (left as bytes) it has read
256:    in.
257:
258:    It detects the encoding from the presence of a utf-8 bom or an encoding
259:    cookie as specified in pep-0263. If both a bom and a cookie are present, but
260:    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid
261:    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,
262:    'utf-8-sig' is returned.
263:
264:    If no encoding is specified, then the default of 'utf-8' will be returned.
265:    """
266:    bom_found = False
267:    encoding = None
268:    default = 'utf-8'
269:    def read_or_stop():
270:        try:
271:            return readline()
272:        except StopIteration:
273:            return bytes()
274:
275:    def find_cookie(line):
276:        try:
277:            line_string = line.decode('ascii')
278:        except UnicodeDecodeError:
279:            return None
280:        match = cookie_re.match(line_string)
281:        if not match:
282:            return None
283:        encoding = _get_normal_name(match.group(1))
284:        try:
285:            codec = lookup(encoding)
286:        except LookupError:
287:            # This behaviour mimics the Python interpreter
288:            raise SyntaxError("unknown encoding: " + encoding)
289:
290:        if bom_found:
291:            if codec.name != 'utf-8':
292:                # This behaviour mimics the Python interpreter
293:                raise SyntaxError('encoding problem: utf-8')
294:            encoding += '-sig'
295:        return encoding
296:
297:    first = read_or_stop()
298:    if first.startswith(BOM_UTF8):
299:        bom_found = True
300:        first = first[3:]
301:        default = 'utf-8-sig'
302:    if not first:
303:        return default, []
304:
305:    encoding = find_cookie(first)
306:    if encoding:
307:        return encoding, [first]
308:    if not blank_re.match(first):
309:        return default, [first]
310:
311:    second = read_or_stop()
312:    if not second:
313:        return default, [first]
314:
315:    encoding = find_cookie(second)
316:    if encoding:
317:        return encoding, [first, second]
318:
319:    return default, [first, second]
320:
321:def untokenize(iterable):
322:    """Transform tokens back into Python source code.
323:
324:    Each element returned by the iterable must be a token sequence
325:    with at least two elements, a token number and token value.  If
326:    only two tokens are passed, the resulting output is poor.
327:
328:    Round-trip invariant for full input:
329:        Untokenized source will match input source exactly
330:
331:    Round-trip invariant for limited intput:
332:        # Output text will tokenize the back to the input
333:        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
334:        newcode = untokenize(t1)
335:        readline = iter(newcode.splitlines(1)).next
336:        t2 = [tok[:2] for tokin generate_tokens(readline)]
337:        assert t1 == t2
338:    """
339:    ut = Untokenizer()
340:    return ut.untokenize(iterable)
341:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
343:    """
344:    The generate_tokens() generator requires one argument, readline, which
345:    must be a callable object which provides the same interface as the
346:    readline() method of built-in file objects. Each call to the function
347:    should return one line of input as a string.  Alternately, readline
348:    can be a callable function terminating with StopIteration:
349:        readline = open(myfile).next    # Example of alternate readline
350:
351:    The generator produces 5-tuples with these members: the token type; the
352:    token string; a 2-tuple (srow, scol) of ints specifying the row and
353:    column where the token begins in the source; a 2-tuple (erow, ecol) of
354:    ints specifying the row and column where the token ends in the source;
355:    and the line on which the token was found. The line passed is the
356:    logical line; continuation lines are included.
357:    """
358:    lnum = parenlev = continued = 0
359:    numchars = '0123456789'
360:    contstr, needcont = '', 0
361:    contline = None
362:    indents = [0]
363:
364:    # If we know we're parsing 3.7+, we can unconditionally parse `async` and
365:    # `await` as keywords.
366:    async_is_reserved_keyword = config.async_is_reserved_keyword
367:    # 'stashed' and 'async_*' are used for async/await parsing
368:    stashed = None
369:    async_def = False
370:    async_def_indent = 0
371:    async_def_nl = False
372:
373:    while 1:                                   # loop over lines in stream
374:        try:
375:            line = readline()
376:        except StopIteration:
377:            line = ''
378:        lnum = lnum + 1
379:        pos, max = 0, len(line)
380:
381:        if contstr:                            # continued string
382:            if not line:
383:                raise TokenError("EOF in multi-line string", strstart)
384:            endmatch = endprog.match(line)
385:            if endmatch:
386:                pos = end = endmatch.end(0)
387:                yield (STRING, contstr + line[:end],
388:                       strstart, (lnum, end), contline + line)
389:                contstr, needcont = '', 0
390:                contline = None
391:            elif needcont and line[-2:] != '\\\n' and line[-3:] != '\\\r\n':
392:                yield (ERRORTOKEN, contstr + line,
393:                           strstart, (lnum, len(line)), contline)
394:                contstr = ''
395:                contline = None
396:                continue
397:            else:
398:                contstr = contstr + line
399:                contline = contline + line
400:                continue
401:
402:        elif parenlev == 0 and not continued:  # new statement
403:            if not line: break
404:            column = 0
405:            while pos < max:                   # measure leading whitespace
406:                if line[pos] == ' ': column = column + 1
407:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize
408:                elif line[pos] == '\f': column = 0
409:                else: break
410:                pos = pos + 1
411:            if pos == max: break
412:
413:            if stashed:
414:                yield stashed
415:                stashed = None
416:
417:            if line[pos] in '\r\n':            # skip blank lines
418:                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)
419:                continue
420:
421:            if line[pos] == '#':               # skip comments
422:                comment_token = line[pos:].rstrip('\r\n')
423:                nl_pos = pos + len(comment_token)
424:                yield (COMMENT, comment_token,
425:                        (lnum, pos), (lnum, pos + len(comment_token)), line)
426:                yield (NL, line[nl_pos:],
427:                        (lnum, nl_pos), (lnum, len(line)), line)
428:                continue
429:
430:            if column > indents[-1]:           # count indents
431:                indents.append(column)
432:                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)
433:
434:            while column < indents[-1]:        # count dedents
435:                if column not in indents:
436:                    raise IndentationError(
437:                        "unindent does not match any outer indentation level",
438:                        ("<tokenize>", lnum, pos, line))
439:                indents = indents[:-1]
440:
441:                if async_def and async_def_indent >= indents[-1]:
442:                    async_def = False
443:                    async_def_nl = False
444:                    async_def_indent = 0
445:
446:                yield (DEDENT, '', (lnum, pos), (lnum, pos), line)
447:
448:            if async_def and async_def_nl and async_def_indent >= indents[-1]:
449:                async_def = False
450:                async_def_nl = False
451:                async_def_indent = 0
452:
453:        else:                                  # continued statement
454:            if not line:
455:                raise TokenError("EOF in multi-line statement", (lnum, 0))
456:            continued = 0
457:
458:        while pos < max:
459:            pseudomatch = pseudoprog.match(line, pos)
460:            if pseudomatch:                                # scan for tokens
461:                start, end = pseudomatch.span(1)
462:                spos, epos, pos = (lnum, start), (lnum, end), end
463:                token, initial = line[start:end], line[start]
464:
465:                if initial in numchars or \
466:                   (initial == '.' and token != '.'):      # ordinary number
467:                    yield (NUMBER, token, spos, epos, line)
468:                elif initial in '\r\n':
469:                    newline = NEWLINE
470:                    if parenlev > 0:
471:                        newline = NL
472:                    elif async_def:
473:                        async_def_nl = True
474:                    if stashed:
475:                        yield stashed
476:                        stashed = None
477:                    yield (newline, token, spos, epos, line)
478:
479:                elif initial == '#':
480:                    assert not token.endswith("\n")
481:                    if stashed:
482:                        yield stashed
483:                        stashed = None
484:                    yield (COMMENT, token, spos, epos, line)
485:                elif token in triple_quoted:
486:                    endprog = endprogs[token]
487:                    endmatch = endprog.match(line, pos)
488:                    if endmatch:                           # all on one line
489:                        pos = endmatch.end(0)
490:                        token = line[start:pos]
491:                        if stashed:
492:                            yield stashed
493:                            stashed = None
494:                        yield (STRING, token, spos, (lnum, pos), line)
495:                    else:
496:                        strstart = (lnum, start)           # multiple lines
497:                        contstr = line[start:]
498:                        contline = line
499:                        break
500:                elif initial in single_quoted or \
501:                    token[:2] in single_quoted or \
502:                    token[:3] in single_quoted:
503:                    if token[-1] == '\n':                  # continued string
504:                        strstart = (lnum, start)
505:                        endprog = (endprogs[initial] or endprogs[token[1]] or
506:                                   endprogs[token[2]])
507:                        contstr, needcont = line[start:], 1
508:                        contline = line
509:                        break
510:                    else:                                  # ordinary string
511:                        if stashed:
512:                            yield stashed
513:                            stashed = None
514:                        yield (STRING, token, spos, epos, line)
515:                elif initial.isidentifier():               # ordinary name
516:                    if token in ('async', 'await'):
517:                        if async_is_reserved_keyword or async_def:
518:                            yield (ASYNC if token == 'async' else AWAIT,
519:                                   token, spos, epos, line)
520:                            continue
521:
522:                    tok = (NAME, token, spos, epos, line)
523:                    if token == 'async' and not stashed:
524:                        stashed = tok
525:                        continue
526:
527:                    if token in ('def', 'for'):
528:                        if (stashed
529:                                and stashed[0] == NAME
530:                                and stashed[1] == 'async'):
531:
532:                            if token == 'def':
533:                                async_def = True
534:                                async_def_indent = indents[-1]
535:
536:                            yield (ASYNC, stashed[1],
537:                                   stashed[2], stashed[3],
538:                                   stashed[4])
539:                            stashed = None
540:
541:                    if stashed:
542:                        yield stashed
543:                        stashed = None
544:
545:                    yield tok
546:                elif initial == '\\':                      # continued stmt
547:                    # This yield is new; needed for better idempotency:
548:                    if stashed:
549:                        yield stashed
550:                        stashed = None
551:                    yield (NL, token, spos, (lnum, pos), line)
552:                    continued = 1
553:                else:
554:                    if initial in '([{': parenlev = parenlev + 1
555:                    elif initial in ')]}': parenlev = parenlev - 1
556:                    if stashed:
557:                        yield stashed
558:                        stashed = None
559:                    yield (OP, token, spos, epos, line)
560:            else:
561:                yield (ERRORTOKEN, line[pos],
562:                           (lnum, pos), (lnum, pos+1), line)
563:                pos = pos + 1
564:
565:    if stashed:
566:        yield stashed
567:        stashed = None
568:
569:    for indent in indents[1:]:                 # pop remaining indent levels
570:        yield (DEDENT, '', (lnum, 0), (lnum, 0), '')
571:    yield (ENDMARKER, '', (lnum, 0), (lnum, 0), '')
572:
573:if __name__ == '__main__':                     # testing
574:    import sys
575:    if len(sys.argv) > 1: tokenize(open(sys.argv[1]).readline)
576:    else: tokenize(sys.stdin.readline)

Calling tool: grepTool
class InvalidInput
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:93:class InvalidInput(ValueError):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/black.py:92:class InvalidInput(ValueError):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
1:import asyncio
2:from asyncio.base_events import BaseEventLoop
3:from concurrent.futures import Executor, ProcessPoolExecutor
4:from datetime import datetime
5:from enum import Enum
6:from functools import lru_cache, partial, wraps
7:import io
8:import itertools
9:import logging
10:from multiprocessing import Manager, freeze_support
11:import os
12:from pathlib import Path
13:import pickle
14:import re
15:import signal
16:import sys
17:import tempfile
18:import tokenize
19:from typing import (
20:    Any,
21:    Callable,
22:    Collection,
23:    Dict,
24:    Generator,
25:    Generic,
26:    Iterable,
27:    Iterator,
28:    List,
29:    Optional,
30:    Pattern,
31:    Sequence,
32:    Set,
33:    Tuple,
34:    TypeVar,
35:    Union,
36:    cast,
37:)
38:
39:from appdirs import user_cache_dir
40:from attr import dataclass, evolve, Factory
41:import click
42:import toml
43:from typed_ast import ast3, ast27
44:
45:# lib2to3 fork
46:from blib2to3.pytree import Node, Leaf, type_repr
47:from blib2to3 import pygram, pytree
48:from blib2to3.pgen2 import driver, token
49:from blib2to3.pgen2.grammar import Grammar
50:from blib2to3.pgen2.parse import ParseError
51:from blib2to3.pgen2.tokenize import TokenizerConfig
52:
53:
54:__version__ = "19.3b0"
55:DEFAULT_LINE_LENGTH = 88
56:DEFAULT_EXCLUDES = (
57:    r"/(\.eggs|\.git|\.hg|\.mypy_cache|\.nox|\.tox|\.venv|_build|buck-out|build|dist)/"
58:)
59:DEFAULT_INCLUDES = r"\.pyi?$"
60:CACHE_DIR = Path(user_cache_dir("black", version=__version__))
61:
62:
63:# types
64:FileContent = str
65:Encoding = str
66:NewLine = str
67:Depth = int
68:NodeType = int
69:LeafID = int
70:Priority = int
71:Index = int
72:LN = Union[Leaf, Node]
73:SplitFunc = Callable[["Line", Collection["Feature"]], Iterator["Line"]]
74:Timestamp = float
75:FileSize = int
76:CacheInfo = Tuple[Timestamp, FileSize]
77:Cache = Dict[Path, CacheInfo]
78:out = partial(click.secho, bold=True, err=True)
79:err = partial(click.secho, fg="red", err=True)
80:
81:pygram.initialize(CACHE_DIR)
82:syms = pygram.python_symbols
83:
84:
85:class NothingChanged(UserWarning):
86:    """Raised when reformatted code is the same as source."""
87:
88:
89:class CannotSplit(Exception):
90:    """A readable split that fits the allotted line length is impossible."""
91:
92:
93:class InvalidInput(ValueError):
94:    """Raised when input source code fails all parse attempts."""
95:
96:
97:class WriteBack(Enum):
98:    NO = 0
99:    YES = 1
100:    DIFF = 2
101:    CHECK = 3
102:
103:    @classmethod
104:    def from_configuration(cls, *, check: bool, diff: bool) -> "WriteBack":
105:        if check and not diff:
106:            return cls.CHECK
107:
108:        return cls.DIFF if diff else cls.YES
109:
110:
111:class Changed(Enum):
112:    NO = 0
113:    CACHED = 1
114:    YES = 2
115:
116:
117:class TargetVersion(Enum):
118:    PY27 = 2
119:    PY33 = 3
120:    PY34 = 4
121:    PY35 = 5
122:    PY36 = 6
123:    PY37 = 7
124:    PY38 = 8
125:
126:    def is_python2(self) -> bool:
127:        return self is TargetVersion.PY27
128:
129:
130:PY36_VERSIONS = {TargetVersion.PY36, TargetVersion.PY37, TargetVersion.PY38}

Calling tool: grepTool
get_grammars\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/black.py:751:def get_grammars(target_versions: Set[TargetVersion]) -> List[Grammar]:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/black.py:772:    for grammar in get_grammars(set(target_versions)):

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: lib2to3_parse() now iterates over get_parser_configs(), constructs driver.Driver with parser_config.grammar and passes tokenizer_config through. Driver forwards tokenizer_config into tokenize.generate_tokens. Error handling remains: it catches ParseError, records location, and only raises InvalidInput after all attempts fail with correct line/column info.


Todo item 5 of 15 is correct
description: 'Confirm lib2to3_parse() iterates over get_parser_configs() results, constructs driver.Driver with parser_config.grammar and tokenizer_config, and passes tokenizer_config through to parsing (no regression in error handling: InvalidInput raised only after all attempts fail and includes correct location info).'
Validating todo item 6 of 15
description: 'Verify blib2to3/pgen2/driver.Driver __init__ signature adds tokenizer_config with default tokenize.TokenizerConfig(), stores it on self, and that parse_stream_raw() and parse_string() pass config=self.tokenizer_config to tokenize.generate_tokens().'
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "class Driver\\(", "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2" ]
Calling tool: grepTool
class Driver\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py:30:class Driver(object):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Modifications:
5:# Copyright 2006 Google, Inc. All Rights Reserved.
6:# Licensed to PSF under a Contributor Agreement.
7:
8:"""Parser driver.
9:
10:This provides a high-level interface to parse a file into a syntax tree.
11:
12:"""
13:
14:__author__ = "Guido van Rossum <guido@python.org>"
15:
16:__all__ = ["Driver", "load_grammar"]
17:
18:# Python imports
19:import codecs
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(
33:        self,
34:        grammar,
35:        convert=None,
36:        logger=None,
37:        tokenizer_config=tokenize.TokenizerConfig(),
38:    ):
39:        self.grammar = grammar
40:        if logger is None:
41:            logger = logging.getLogger(__name__)
42:        self.logger = logger
43:        self.convert = convert
44:        self.tokenizer_config = tokenizer_config
45:
46:    def parse_tokens(self, tokens, debug=False):
47:        """Parse a series of tokens and return the syntax tree."""
48:        # XXX Move the prefix computation into a wrapper around tokenize.
49:        p = parse.Parser(self.grammar, self.convert)
50:        p.setup()
51:        lineno = 1
52:        column = 0
53:        indent_columns = []
54:        type = value = start = end = line_text = None
55:        prefix = ""
56:        for quintuple in tokens:
57:            type, value, start, end, line_text = quintuple
58:            if start != (lineno, column):
59:                assert (lineno, column) <= start, ((lineno, column), start)
60:                s_lineno, s_column = start
61:                if lineno < s_lineno:
62:                    prefix += "\n" * (s_lineno - lineno)
63:                    lineno = s_lineno
64:                    column = 0
65:                if column < s_column:
66:                    prefix += line_text[column:s_column]
67:                    column = s_column
68:            if type in (tokenize.COMMENT, tokenize.NL):
69:                prefix += value
70:                lineno, column = end
71:                if value.endswith("\n"):
72:                    lineno += 1
73:                    column = 0
74:                continue
75:            if type == token.OP:
76:                type = grammar.opmap[value]
77:            if debug:
78:                self.logger.debug("%s %r (prefix=%r)",
79:                                  token.tok_name[type], value, prefix)
80:            if type == token.INDENT:
81:                indent_columns.append(len(value))
82:                _prefix = prefix + value
83:                prefix = ""
84:                value = ""
85:            elif type == token.DEDENT:
86:                _indent_col = indent_columns.pop()
87:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
88:            if p.addtoken(type, value, (prefix, start)):
89:                if debug:
90:                    self.logger.debug("Stop.")
91:                break
92:            prefix = ""
93:            if type in {token.INDENT, token.DEDENT}:
94:                prefix = _prefix
95:            lineno, column = end
96:            if value.endswith("\n"):
97:                lineno += 1
98:                column = 0
99:        else:
100:            # We never broke out -- EOF is too soon (how can this happen???)
101:            raise parse.ParseError("incomplete input",
102:                                   type, value, (prefix, start))
103:        return p.rootnode
104:
105:    def parse_stream_raw(self, stream, debug=False):
106:        """Parse a stream and return the syntax tree."""
107:        tokens = tokenize.generate_tokens(stream.readline, config=self.tokenizer_config)
108:        return self.parse_tokens(tokens, debug)
109:
110:    def parse_stream(self, stream, debug=False):
111:        """Parse a stream and return the syntax tree."""
112:        return self.parse_stream_raw(stream, debug)
113:
114:    def parse_file(self, filename, encoding=None, debug=False):
115:        """Parse a file and return the syntax tree."""
116:        with io.open(filename, "r", encoding=encoding) as stream:
117:            return self.parse_stream(stream, debug)
118:
119:    def parse_string(self, text, debug=False):
120:        """Parse a string and return the syntax tree."""
121:        tokens = tokenize.generate_tokens(
122:            io.StringIO(text).readline,
123:            config=self.tokenizer_config,
124:        )
125:        return self.parse_tokens(tokens, debug)
126:
127:    def _partially_consume_prefix(self, prefix, column):
128:        lines = []
129:        current_line = ""
130:        current_column = 0
131:        wait_for_nl = False
132:        for char in prefix:
133:            current_line += char
134:            if wait_for_nl:
135:                if char == '\n':
136:                    if current_line.strip() and current_column < column:
137:                        res = ''.join(lines)
138:                        return res, prefix[len(res):]
139:
140:                    lines.append(current_line)
141:                    current_line = ""
142:                    current_column = 0
143:                    wait_for_nl = False
144:            elif char in ' \t':
145:                current_column += 1
146:            elif char == '\n':
147:                # unexpected empty line
148:                current_column = 0
149:            else:
150:                # indent is finished
151:                wait_for_nl = True
152:        return ''.join(lines), current_line
153:
154:
155:def _generate_pickle_name(gt, cache_dir=None):
156:    head, tail = os.path.splitext(gt)
157:    if tail == ".txt":
158:        tail = ""
159:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
160:    if cache_dir:
161:        return os.path.join(cache_dir, os.path.basename(name))
162:    else:
163:        return name
164:
165:
166:def load_grammar(gt="Grammar.txt", gp=None,
167:                 save=True, force=False, logger=None):
168:    """Load the grammar (maybe from a pickle)."""
169:    if logger is None:
170:        logger = logging.getLogger(__name__)

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: Verified in blib2to3/pgen2/driver.py: __init__ includes tokenizer_config=tokenize.TokenizerConfig() and assigns self.tokenizer_config. Both parse_stream_raw() and parse_string() call tokenize.generate_tokens(..., config=self.tokenizer_config).


Todo item 6 of 15 is correct
description: 'Verify blib2to3/pgen2/driver.Driver __init__ signature adds tokenizer_config with default tokenize.TokenizerConfig(), stores it on self, and that parse_stream_raw() and parse_string() pass config=self.tokenizer_config to tokenize.generate_tokens().'
Validating todo item 7 of 15
description: 'Ensure blib2to3/pgen2/tokenize.TokenizerConfig dataclass exists, is frozen, and defines bool field async_is_reserved_keyword defaulting to False.'
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "TokenizerConfig", "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black" ]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "from attr import dataclass",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py"
]
Calling tool: grepTool
TokenizerConfig
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:142:class TokenizerConfig:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py:37:        tokenizer_config=tokenize.TokenizerConfig(),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.pyi:11:from blib2to3.pgen2.tokenize import TokenizerConfig
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.pyi:23:        tokenizer_config: TokenizerConfig = ...
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi:14:class TokenizerConfig:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:51:from blib2to3.pgen2.tokenize import TokenizerConfig
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:766:    tokenizer_config: TokenizerConfig = TokenizerConfig()
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:776:                TokenizerConfig(async_is_reserved_keyword=True),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:781:                TokenizerConfig(async_is_reserved_keyword=False),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:805:                    TokenizerConfig(async_is_reserved_keyword=True),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:813:                    TokenizerConfig(async_is_reserved_keyword=False),

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
120:)
121:
122:endprogs = {"'": re.compile(Single), '"': re.compile(Double),
123:            "'''": single3prog, '"""': double3prog,
124:            **{f"{prefix}'''": single3prog for prefix in _strprefixes},
125:            **{f'{prefix}"""': double3prog for prefix in _strprefixes},
126:            **{prefix: None for prefix in _strprefixes}}
127:
128:triple_quoted = (
129:    {"'''", '"""'} |
130:    {f"{prefix}'''" for prefix in _strprefixes} |
131:    {f'{prefix}"""' for prefix in _strprefixes}
132:)
133:single_quoted = (
134:    {"'", '"'} |
135:    {f"{prefix}'" for prefix in _strprefixes} |
136:    {f'{prefix}"' for prefix in _strprefixes}
137:)
138:
139:tabsize = 8
140:
141:@dataclass(frozen=True)
142:class TokenizerConfig:
143:    async_is_reserved_keyword: bool = False
144:
145:class TokenError(Exception): pass
146:
147:class StopTokenizing(Exception): pass
148:
149:def printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line): # for testing
150:    (srow, scol) = xxx_todo_changeme
151:    (erow, ecol) = xxx_todo_changeme1
152:    print("%d,%d-%d,%d:\t%s\t%s" % \
153:        (srow, scol, erow, ecol, tok_name[type], repr(token)))
154:
155:def tokenize(readline, tokeneater=printtoken):
156:    """
157:    The tokenize() function accepts two parameters: one representing the
158:    input stream, and one providing an output mechanism for tokenize().
159:
160:    The first parameter, readline, must be a callable object which provides
161:    the same interface as the readline() method of built-in file objects.
162:    Each call to the function should return one line of input as a string.
163:
164:    The second parameter, tokeneater, must also be a callable object. It is
165:    called once for each token, with five arguments, corresponding to the
166:    tuples generated by generate_tokens().
167:    """
168:    try:
169:        tokenize_loop(readline, tokeneater)
170:    except StopTokenizing:
171:        pass
172:
173:# backwards compatible interface
174:def tokenize_loop(readline, tokeneater):
175:    for token_info in generate_tokens(readline):
176:        tokeneater(*token_info)
177:
178:class Untokenizer:
179:

Calling tool: grepTool
from attr import dataclass
Matched content:
34:from attr import dataclass

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
24:are the same, except instead of generating tokens, tokeneater is a callback
25:function to which the 5 fields described above are passed as 5 arguments,
26:each time a new token is found."""
27:
28:__author__ = 'Ka-Ping Yee <ping@lfw.org>'
29:__credits__ = \
30:    'GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro'
31:
32:import re
33:from codecs import BOM_UTF8, lookup
34:from attr import dataclass
35:from blib2to3.pgen2.token import *
36:
37:from . import token
38:__all__ = [x for x in dir(token) if x[0] != '_'] + ["tokenize",
39:           "generate_tokens", "untokenize"]
40:del token
41:
42:try:
43:    bytes

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: In blib2to3/pgen2/tokenize.py, a dataclass TokenizerConfig is defined with @dataclass(frozen=True) and contains the field async_is_reserved_keyword: bool = False. Verified via grep and reading lines 120-148 showing the exact implementation.


Todo item 7 of 15 is correct
description: 'Ensure blib2to3/pgen2/tokenize.TokenizerConfig dataclass exists, is frozen, and defines bool field async_is_reserved_keyword defaulting to False.'
Validating todo item 8 of 15
description: 'Validate blib2to3/pgen2/tokenize.generate_tokens() signature now accepts optional config: TokenizerConfig = TokenizerConfig(), and inside sets async_is_reserved_keyword = config.async_is_reserved_keyword.'
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "def generate_tokens\\(",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "class TokenizerConfig",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "async_is_reserved_keyword",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "def generate_tokens\\(",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.py"
]
Calling tool: globTool
**/blib2to3/pgen2/tokenize.py
Matched files:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.py

Calling tool: grepTool
def generate_tokens\(
Matched content:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
320:
321:def untokenize(iterable):
322:    """Transform tokens back into Python source code.
323:
324:    Each element returned by the iterable must be a token sequence
325:    with at least two elements, a token number and token value.  If
326:    only two tokens are passed, the resulting output is poor.
327:
328:    Round-trip invariant for full input:
329:        Untokenized source will match input source exactly
330:
331:    Round-trip invariant for limited intput:
332:        # Output text will tokenize the back to the input
333:        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
334:        newcode = untokenize(t1)
335:        readline = iter(newcode.splitlines(1)).next
336:        t2 = [tok[:2] for tokin generate_tokens(readline)]
337:        assert t1 == t2
338:    """
339:    ut = Untokenizer()
340:    return ut.untokenize(iterable)
341:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
343:    """
344:    The generate_tokens() generator requires one argument, readline, which
345:    must be a callable object which provides the same interface as the
346:    readline() method of built-in file objects. Each call to the function
347:    should return one line of input as a string.  Alternately, readline
348:    can be a callable function terminating with StopIteration:
349:        readline = open(myfile).next    # Example of alternate readline
350:
351:    The generator produces 5-tuples with these members: the token type; the
352:    token string; a 2-tuple (srow, scol) of ints specifying the row and
353:    column where the token begins in the source; a 2-tuple (erow, ecol) of
354:    ints specifying the row and column where the token ends in the source;
355:    and the line on which the token was found. The line passed is the
356:    logical line; continuation lines are included.
357:    """
358:    lnum = parenlev = continued = 0
359:    numchars = '0123456789'
360:    contstr, needcont = '', 0
361:    contline = None
362:    indents = [0]
363:
364:    # If we know we're parsing 3.7+, we can unconditionally parse `async` and
365:    # `await` as keywords.
366:    async_is_reserved_keyword = config.async_is_reserved_keyword
367:    # 'stashed' and 'async_*' are used for async/await parsing
368:    stashed = None
369:    async_def = False
370:    async_def_indent = 0
371:    async_def_nl = False
372:
373:    while 1:                                   # loop over lines in stream
374:        try:
375:            line = readline()
376:        except StopIteration:
377:            line = ''
378:        lnum = lnum + 1
379:        pos, max = 0, len(line)
380:
381:        if contstr:                            # continued string
382:            if not line:
383:                raise TokenError("EOF in multi-line string", strstart)
384:            endmatch = endprog.match(line)
385:            if endmatch:
386:                pos = end = endmatch.end(0)
387:                yield (STRING, contstr + line[:end],
388:                       strstart, (lnum, end), contline + line)
389:                contstr, needcont = '', 0
390:                contline = None
391:            elif needcont and line[-2:] != '\\\n' and line[-3:] != '\\\r\n':
392:                yield (ERRORTOKEN, contstr + line,
393:                           strstart, (lnum, len(line)), contline)
394:                contstr = ''
395:                contline = None
396:                continue
397:            else:
398:                contstr = contstr + line
399:                contline = contline + line
400:                continue
401:
402:        elif parenlev == 0 and not continued:  # new statement
403:            if not line: break
404:            column = 0
405:            while pos < max:                   # measure leading whitespace
406:                if line[pos] == ' ': column = column + 1
407:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize
408:                elif line[pos] == '\f': column = 0
409:                else: break
410:                pos = pos + 1
411:            if pos == max: break
412:
413:            if stashed:
414:                yield stashed
415:                stashed = None
416:
417:            if line[pos] in '\r\n':            # skip blank lines
418:                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)
419:                continue
420:
421:            if line[pos] == '#':               # skip comments
422:                comment_token = line[pos:].rstrip('\r\n')
423:                nl_pos = pos + len(comment_token)
424:                yield (COMMENT, comment_token,
425:                        (lnum, pos), (lnum, pos + len(comment_token)), line)
426:                yield (NL, line[nl_pos:],
427:                        (lnum, nl_pos), (lnum, len(line)), line)
428:                continue
429:
430:            if column > indents[-1]:           # count indents
431:                indents.append(column)
432:                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)
433:
434:            while column < indents[-1]:        # count dedents
435:                if column not in indents:
436:                    raise IndentationError(
437:                        "unindent does not match any outer indentation level",
438:                        ("<tokenize>", lnum, pos, line))
439:                indents = indents[:-1]

Calling tool: grepTool
class TokenizerConfig
Matched content:
142:class TokenizerConfig:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
130:    {f"{prefix}'''" for prefix in _strprefixes} |
131:    {f'{prefix}"""' for prefix in _strprefixes}
132:)
133:single_quoted = (
134:    {"'", '"'} |
135:    {f"{prefix}'" for prefix in _strprefixes} |
136:    {f'{prefix}"' for prefix in _strprefixes}
137:)
138:
139:tabsize = 8
140:
141:@dataclass(frozen=True)
142:class TokenizerConfig:
143:    async_is_reserved_keyword: bool = False
144:
145:class TokenError(Exception): pass
146:
147:class StopTokenizing(Exception): pass
148:
149:def printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line): # for testing
150:    (srow, scol) = xxx_todo_changeme
151:    (erow, ecol) = xxx_todo_changeme1
152:    print("%d,%d-%d,%d:\t%s\t%s" % \
153:        (srow, scol, erow, ecol, tok_name[type], repr(token)))
154:
155:def tokenize(readline, tokeneater=printtoken):
156:    """
157:    The tokenize() function accepts two parameters: one representing the
158:    input stream, and one providing an output mechanism for tokenize().
159:

Calling tool: grepTool
async_is_reserved_keyword
Matched content:
143:    async_is_reserved_keyword: bool = False
366:    async_is_reserved_keyword = config.async_is_reserved_keyword
517:                        if async_is_reserved_keyword or async_def:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
500:                elif initial in single_quoted or \
501:                    token[:2] in single_quoted or \
502:                    token[:3] in single_quoted:
503:                    if token[-1] == '\n':                  # continued string
504:                        strstart = (lnum, start)
505:                        endprog = (endprogs[initial] or endprogs[token[1]] or
506:                                   endprogs[token[2]])
507:                        contstr, needcont = line[start:], 1
508:                        contline = line
509:                        break
510:                    else:                                  # ordinary string
511:                        if stashed:
512:                            yield stashed
513:                            stashed = None
514:                        yield (STRING, token, spos, epos, line)
515:                elif initial.isidentifier():               # ordinary name
516:                    if token in ('async', 'await'):
517:                        if async_is_reserved_keyword or async_def:
518:                            yield (ASYNC if token == 'async' else AWAIT,
519:                                   token, spos, epos, line)
520:                            continue
521:
522:                    tok = (NAME, token, spos, epos, line)
523:                    if token == 'async' and not stashed:
524:                        stashed = tok
525:                        continue
526:
527:                    if token in ('def', 'for'):
528:                        if (stashed
529:                                and stashed[0] == NAME
530:                                and stashed[1] == 'async'):
531:
532:                            if token == 'def':
533:                                async_def = True
534:                                async_def_indent = indents[-1]
535:
536:                            yield (ASYNC, stashed[1],
537:                                   stashed[2], stashed[3],
538:                                   stashed[4])
539:                            stashed = None

Calling tool: grepTool
def generate_tokens\(
Matched content:
337:def generate_tokens(readline):

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: In blib2to3/pgen2/tokenize.py, generate_tokens now has signature def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()) (line ~342), and inside it assigns async_is_reserved_keyword = config.async_is_reserved_keyword (line ~366), satisfying the requirement.


Todo item 8 of 15 is correct
description: 'Validate blib2to3/pgen2/tokenize.generate_tokens() signature now accepts optional config: TokenizerConfig = TokenizerConfig(), and inside sets async_is_reserved_keyword = config.async_is_reserved_keyword.'
Validating todo item 9 of 15
description: 'Inspect generate_tokens() tokenization rule for identifiers: when token in ('async','await') and (async_is_reserved_keyword or async_def) is True, yield ASYNC/await tokens; otherwise treat them as NAME. This should enable always-keyword behavior for Python 3.7+ mode while preserving legacy behavior for earlier versions.'
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "def generate_tokens\\(",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/{{FOLDER_PATH}}"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "def generate_tokens\\(",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black"
]
Calling tool: grepTool
def generate_tokens\(
Grep command failed with exit code 2: rg: .rgignore: No such file or directory (os error 2)
rg: /Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/{{FOLDER_PATH}}: IO error for operation on /Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/{{FOLDER_PATH}}: No such file or directory (os error 2)


Calling tool: grepTool
def generate_tokens\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi:32:def generate_tokens(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.py:337:def generate_tokens(readline):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.pyi:28:def generate_tokens(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/flake8/processor.py:259:    def generate_tokens(self):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pycodestyle.py:2026:    def generate_tokens(self):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/2/tokenize.pyi:117:def generate_tokens(readline: Callable[[], str]) -> Generator[_TokenType, None, None]: ...
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/3/tokenize.pyi:46:def generate_tokens(readline: Callable[[], str]) -> Generator[TokenInfo, None, None]: ...  # undocumented
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/2and3/lib2to3/pgen2/tokenize.pyi:28:def generate_tokens(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/coverage/phystokens.py:142:    def generate_tokens(self, text):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
320:
321:def untokenize(iterable):
322:    """Transform tokens back into Python source code.
323:
324:    Each element returned by the iterable must be a token sequence
325:    with at least two elements, a token number and token value.  If
326:    only two tokens are passed, the resulting output is poor.
327:
328:    Round-trip invariant for full input:
329:        Untokenized source will match input source exactly
330:
331:    Round-trip invariant for limited intput:
332:        # Output text will tokenize the back to the input
333:        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
334:        newcode = untokenize(t1)
335:        readline = iter(newcode.splitlines(1)).next
336:        t2 = [tok[:2] for tokin generate_tokens(readline)]
337:        assert t1 == t2
338:    """
339:    ut = Untokenizer()
340:    return ut.untokenize(iterable)
341:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
343:    """
344:    The generate_tokens() generator requires one argument, readline, which
345:    must be a callable object which provides the same interface as the
346:    readline() method of built-in file objects. Each call to the function
347:    should return one line of input as a string.  Alternately, readline
348:    can be a callable function terminating with StopIteration:
349:        readline = open(myfile).next    # Example of alternate readline
350:
351:    The generator produces 5-tuples with these members: the token type; the
352:    token string; a 2-tuple (srow, scol) of ints specifying the row and
353:    column where the token begins in the source; a 2-tuple (erow, ecol) of
354:    ints specifying the row and column where the token ends in the source;
355:    and the line on which the token was found. The line passed is the
356:    logical line; continuation lines are included.
357:    """
358:    lnum = parenlev = continued = 0
359:    numchars = '0123456789'
360:    contstr, needcont = '', 0
361:    contline = None
362:    indents = [0]
363:
364:    # If we know we're parsing 3.7+, we can unconditionally parse `async` and
365:    # `await` as keywords.
366:    async_is_reserved_keyword = config.async_is_reserved_keyword
367:    # 'stashed' and 'async_*' are used for async/await parsing
368:    stashed = None
369:    async_def = False
370:    async_def_indent = 0
371:    async_def_nl = False
372:
373:    while 1:                                   # loop over lines in stream
374:        try:
375:            line = readline()
376:        except StopIteration:
377:            line = ''
378:        lnum = lnum + 1
379:        pos, max = 0, len(line)
380:
381:        if contstr:                            # continued string
382:            if not line:
383:                raise TokenError("EOF in multi-line string", strstart)
384:            endmatch = endprog.match(line)
385:            if endmatch:
386:                pos = end = endmatch.end(0)
387:                yield (STRING, contstr + line[:end],
388:                       strstart, (lnum, end), contline + line)
389:                contstr, needcont = '', 0
390:                contline = None
391:            elif needcont and line[-2:] != '\\\n' and line[-3:] != '\\\r\n':
392:                yield (ERRORTOKEN, contstr + line,
393:                           strstart, (lnum, len(line)), contline)
394:                contstr = ''
395:                contline = None
396:                continue
397:            else:
398:                contstr = contstr + line
399:                contline = contline + line
400:                continue
401:
402:        elif parenlev == 0 and not continued:  # new statement
403:            if not line: break
404:            column = 0
405:            while pos < max:                   # measure leading whitespace
406:                if line[pos] == ' ': column = column + 1
407:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize
408:                elif line[pos] == '\f': column = 0
409:                else: break
410:                pos = pos + 1
411:            if pos == max: break
412:
413:            if stashed:
414:                yield stashed
415:                stashed = None
416:
417:            if line[pos] in '\r\n':            # skip blank lines
418:                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)
419:                continue
420:
421:            if line[pos] == '#':               # skip comments
422:                comment_token = line[pos:].rstrip('\r\n')
423:                nl_pos = pos + len(comment_token)
424:                yield (COMMENT, comment_token,
425:                        (lnum, pos), (lnum, pos + len(comment_token)), line)
426:                yield (NL, line[nl_pos:],
427:                        (lnum, nl_pos), (lnum, len(line)), line)
428:                continue
429:
430:            if column > indents[-1]:           # count indents
431:                indents.append(column)
432:                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)
433:
434:            while column < indents[-1]:        # count dedents
435:                if column not in indents:
436:                    raise IndentationError(
437:                        "unindent does not match any outer indentation level",
438:                        ("<tokenize>", lnum, pos, line))
439:                indents = indents[:-1]

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
440:
441:                if async_def and async_def_indent >= indents[-1]:
442:                    async_def = False
443:                    async_def_nl = False
444:                    async_def_indent = 0
445:
446:                yield (DEDENT, '', (lnum, pos), (lnum, pos), line)
447:
448:            if async_def and async_def_nl and async_def_indent >= indents[-1]:
449:                async_def = False
450:                async_def_nl = False
451:                async_def_indent = 0
452:
453:        else:                                  # continued statement
454:            if not line:
455:                raise TokenError("EOF in multi-line statement", (lnum, 0))
456:            continued = 0
457:
458:        while pos < max:
459:            pseudomatch = pseudoprog.match(line, pos)
460:            if pseudomatch:                                # scan for tokens
461:                start, end = pseudomatch.span(1)
462:                spos, epos, pos = (lnum, start), (lnum, end), end
463:                token, initial = line[start:end], line[start]
464:
465:                if initial in numchars or \
466:                   (initial == '.' and token != '.'):      # ordinary number
467:                    yield (NUMBER, token, spos, epos, line)
468:                elif initial in '\r\n':
469:                    newline = NEWLINE
470:                    if parenlev > 0:
471:                        newline = NL
472:                    elif async_def:
473:                        async_def_nl = True
474:                    if stashed:
475:                        yield stashed
476:                        stashed = None
477:                    yield (newline, token, spos, epos, line)
478:
479:                elif initial == '#':
480:                    assert not token.endswith("\n")
481:                    if stashed:
482:                        yield stashed
483:                        stashed = None
484:                    yield (COMMENT, token, spos, epos, line)
485:                elif token in triple_quoted:
486:                    endprog = endprogs[token]
487:                    endmatch = endprog.match(line, pos)
488:                    if endmatch:                           # all on one line
489:                        pos = endmatch.end(0)
490:                        token = line[start:pos]
491:                        if stashed:
492:                            yield stashed
493:                            stashed = None
494:                        yield (STRING, token, spos, (lnum, pos), line)
495:                    else:
496:                        strstart = (lnum, start)           # multiple lines
497:                        contstr = line[start:]
498:                        contline = line
499:                        break
500:                elif initial in single_quoted or \
501:                    token[:2] in single_quoted or \
502:                    token[:3] in single_quoted:
503:                    if token[-1] == '\n':                  # continued string
504:                        strstart = (lnum, start)
505:                        endprog = (endprogs[initial] or endprogs[token[1]] or
506:                                   endprogs[token[2]])
507:                        contstr, needcont = line[start:], 1
508:                        contline = line
509:                        break
510:                    else:                                  # ordinary string
511:                        if stashed:
512:                            yield stashed
513:                            stashed = None
514:                        yield (STRING, token, spos, epos, line)
515:                elif initial.isidentifier():               # ordinary name
516:                    if token in ('async', 'await'):
517:                        if async_is_reserved_keyword or async_def:
518:                            yield (ASYNC if token == 'async' else AWAIT,
519:                                   token, spos, epos, line)
520:                            continue
521:
522:                    tok = (NAME, token, spos, epos, line)
523:                    if token == 'async' and not stashed:
524:                        stashed = tok
525:                        continue
526:
527:                    if token in ('def', 'for'):
528:                        if (stashed
529:                                and stashed[0] == NAME
530:                                and stashed[1] == 'async'):
531:
532:                            if token == 'def':
533:                                async_def = True
534:                                async_def_indent = indents[-1]
535:
536:                            yield (ASYNC, stashed[1],
537:                                   stashed[2], stashed[3],
538:                                   stashed[4])
539:                            stashed = None
540:
541:                    if stashed:
542:                        yield stashed
543:                        stashed = None
544:
545:                    yield tok
546:                elif initial == '\\':                      # continued stmt
547:                    # This yield is new; needed for better idempotency:
548:                    if stashed:
549:                        yield stashed
550:                        stashed = None
551:                    yield (NL, token, spos, (lnum, pos), line)
552:                    continued = 1
553:                else:
554:                    if initial in '([{': parenlev = parenlev + 1
555:                    elif initial in ')]}': parenlev = parenlev - 1
556:                    if stashed:
557:                        yield stashed
558:                        stashed = None
559:                    yield (OP, token, spos, epos, line)
560:            else:
561:                yield (ERRORTOKEN, line[pos],
562:                           (lnum, pos), (lnum, pos+1), line)
563:                pos = pos + 1
564:
565:    if stashed:
566:        yield stashed
567:        stashed = None
568:
569:    for indent in indents[1:]:                 # pop remaining indent levels
570:        yield (DEDENT, '', (lnum, 0), (lnum, 0), '')
571:    yield (ENDMARKER, '', (lnum, 0), (lnum, 0), '')
572:
573:if __name__ == '__main__':                     # testing
574:    import sys
575:    if len(sys.argv) > 1: tokenize(open(sys.argv[1]).readline)
576:    else: tokenize(sys.stdin.readline)

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: In blib2to3/pgen2/tokenize.py, generate_tokens reads config.async_is_reserved_keyword (lines ~364-367) and in the identifier branch (lines ~515-521) emits ASYNC/await tokens when token in ('async','await') and (async_is_reserved_keyword or async_def) is True, otherwise it falls through to NAME. This enables always-keyword behavior for 3.7+ mode while preserving legacy behavior.


Todo item 9 of 15 is correct
description: 'Inspect generate_tokens() tokenization rule for identifiers: when token in ('async','await') and (async_is_reserved_keyword or async_def) is True, yield ASYNC/await tokens; otherwise treat them as NAME. This should enable always-keyword behavior for Python 3.7+ mode while preserving legacy behavior for earlier versions.'
Validating todo item 10 of 15
description: 'Verify Untokenizer.compat() includes ASYNC and AWAIT tokens in the set that receive a trailing space (toknum in (NAME, NUMBER, ASYNC, AWAIT)).'
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "class Untokenizer",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black"
]
Calling tool: grepTool
class Untokenizer
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:178:class Untokenizer:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi:22:class Untokenizer:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.py:173:class Untokenizer:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.pyi:18:class Untokenizer:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/3/tokenize.pyi:33:class Untokenizer:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/2/tokenize.pyi:129:class Untokenizer:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/2and3/lib2to3/pgen2/tokenize.pyi:18:class Untokenizer:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
150:    (srow, scol) = xxx_todo_changeme
151:    (erow, ecol) = xxx_todo_changeme1
152:    print("%d,%d-%d,%d:\t%s\t%s" % \
153:        (srow, scol, erow, ecol, tok_name[type], repr(token)))
154:
155:def tokenize(readline, tokeneater=printtoken):
156:    """
157:    The tokenize() function accepts two parameters: one representing the
158:    input stream, and one providing an output mechanism for tokenize().
159:
160:    The first parameter, readline, must be a callable object which provides
161:    the same interface as the readline() method of built-in file objects.
162:    Each call to the function should return one line of input as a string.
163:
164:    The second parameter, tokeneater, must also be a callable object. It is
165:    called once for each token, with five arguments, corresponding to the
166:    tuples generated by generate_tokens().
167:    """
168:    try:
169:        tokenize_loop(readline, tokeneater)
170:    except StopTokenizing:
171:        pass
172:
173:# backwards compatible interface
174:def tokenize_loop(readline, tokeneater):
175:    for token_info in generate_tokens(readline):
176:        tokeneater(*token_info)
177:
178:class Untokenizer:
179:
180:    def __init__(self):
181:        self.tokens = []
182:        self.prev_row = 1
183:        self.prev_col = 0
184:
185:    def add_whitespace(self, start):
186:        row, col = start
187:        assert row <= self.prev_row
188:        col_offset = col - self.prev_col
189:        if col_offset:
190:            self.tokens.append(" " * col_offset)
191:
192:    def untokenize(self, iterable):
193:        for t in iterable:
194:            if len(t) == 2:
195:                self.compat(t, iterable)
196:                break
197:            tok_type, token, start, end, line = t
198:            self.add_whitespace(start)
199:            self.tokens.append(token)
200:            self.prev_row, self.prev_col = end
201:            if tok_type in (NEWLINE, NL):
202:                self.prev_row += 1
203:                self.prev_col = 0
204:        return "".join(self.tokens)
205:
206:    def compat(self, token, iterable):
207:        startline = False
208:        indents = []
209:        toks_append = self.tokens.append
210:        toknum, tokval = token
211:        if toknum in (NAME, NUMBER):
212:            tokval += ' '
213:        if toknum in (NEWLINE, NL):
214:            startline = True
215:        for tok in iterable:
216:            toknum, tokval = tok[:2]
217:
218:            if toknum in (NAME, NUMBER, ASYNC, AWAIT):
219:                tokval += ' '
220:
221:            if toknum == INDENT:
222:                indents.append(tokval)
223:                continue
224:            elif toknum == DEDENT:
225:                indents.pop()
226:                continue
227:            elif toknum in (NEWLINE, NL):
228:                startline = True
229:            elif startline and indents:
230:                toks_append(indents[-1])
231:                startline = False
232:            toks_append(tokval)
233:
234:cookie_re = re.compile(r'^[ \t\f]*#.*?coding[:=][ \t]*([-\w.]+)', re.ASCII)
235:blank_re = re.compile(br'^[ \t\f]*(?:[#\r\n]|$)', re.ASCII)
236:
237:def _get_normal_name(orig_enc):
238:    """Imitates get_normal_name in tokenizer.c."""
239:    # Only care about the first 12 characters.
240:    enc = orig_enc[:12].lower().replace("_", "-")
241:    if enc == "utf-8" or enc.startswith("utf-8-"):
242:        return "utf-8"
243:    if enc in ("latin-1", "iso-8859-1", "iso-latin-1") or \
244:       enc.startswith(("latin-1-", "iso-8859-1-", "iso-latin-1-")):
245:        return "iso-8859-1"
246:    return orig_enc
247:
248:def detect_encoding(readline):
249:    """
250:    The detect_encoding() function is used to detect the encoding that should
251:    be used to decode a Python source file. It requires one argument, readline,
252:    in the same way as the tokenize() generator.
253:
254:    It will call readline a maximum of twice, and return the encoding used
255:    (as a string) and a list of any lines (left as bytes) it has read
256:    in.
257:
258:    It detects the encoding from the presence of a utf-8 bom or an encoding
259:    cookie as specified in pep-0263. If both a bom and a cookie are present, but
260:    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid
261:    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,
262:    'utf-8-sig' is returned.
263:
264:    If no encoding is specified, then the default of 'utf-8' will be returned.
265:    """
266:    bom_found = False
267:    encoding = None
268:    default = 'utf-8'
269:    def read_or_stop():

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.py
160:    called once for each token, with five arguments, corresponding to the
161:    tuples generated by generate_tokens().
162:    """
163:    try:
164:        tokenize_loop(readline, tokeneater)
165:    except StopTokenizing:
166:        pass
167:
168:# backwards compatible interface
169:def tokenize_loop(readline, tokeneater):
170:    for token_info in generate_tokens(readline):
171:        tokeneater(*token_info)
172:
173:class Untokenizer:
174:
175:    def __init__(self):
176:        self.tokens = []
177:        self.prev_row = 1
178:        self.prev_col = 0
179:
180:    def add_whitespace(self, start):
181:        row, col = start
182:        assert row <= self.prev_row
183:        col_offset = col - self.prev_col
184:        if col_offset:
185:            self.tokens.append(" " * col_offset)
186:
187:    def untokenize(self, iterable):
188:        for t in iterable:
189:            if len(t) == 2:
190:                self.compat(t, iterable)
191:                break
192:            tok_type, token, start, end, line = t
193:            self.add_whitespace(start)
194:            self.tokens.append(token)
195:            self.prev_row, self.prev_col = end
196:            if tok_type in (NEWLINE, NL):
197:                self.prev_row += 1
198:                self.prev_col = 0
199:        return "".join(self.tokens)
200:
201:    def compat(self, token, iterable):
202:        startline = False
203:        indents = []
204:        toks_append = self.tokens.append
205:        toknum, tokval = token
206:        if toknum in (NAME, NUMBER):
207:            tokval += ' '
208:        if toknum in (NEWLINE, NL):
209:            startline = True
210:        for tok in iterable:
211:            toknum, tokval = tok[:2]
212:
213:            if toknum in (NAME, NUMBER, ASYNC, AWAIT):
214:                tokval += ' '
215:
216:            if toknum == INDENT:
217:                indents.append(tokval)
218:                continue
219:            elif toknum == DEDENT:
220:                indents.pop()
221:                continue
222:            elif toknum in (NEWLINE, NL):
223:                startline = True
224:            elif startline and indents:
225:                toks_append(indents[-1])
226:                startline = False
227:            toks_append(tokval)
228:
229:cookie_re = re.compile(r'^[ \t\f]*#.*?coding[:=][ \t]*([-\w.]+)', re.ASCII)
230:blank_re = re.compile(br'^[ \t\f]*(?:[#\r\n]|$)', re.ASCII)
231:
232:def _get_normal_name(orig_enc):
233:    """Imitates get_normal_name in tokenizer.c."""
234:    # Only care about the first 12 characters.
235:    enc = orig_enc[:12].lower().replace("_", "-")
236:    if enc == "utf-8" or enc.startswith("utf-8-"):
237:        return "utf-8"
238:    if enc in ("latin-1", "iso-8859-1", "iso-latin-1") or \
239:       enc.startswith(("latin-1-", "iso-8859-1-", "iso-latin-1-")):
240:        return "iso-8859-1"
241:    return orig_enc
242:
243:def detect_encoding(readline):
244:    """
245:    The detect_encoding() function is used to detect the encoding that should
246:    be used to decode a Python source file. It requires one argument, readline,
247:    in the same way as the tokenize() generator.
248:
249:    It will call readline a maximum of twice, and return the encoding used
250:    (as a string) and a list of any lines (left as bytes) it has read
251:    in.
252:
253:    It detects the encoding from the presence of a utf-8 bom or an encoding
254:    cookie as specified in pep-0263. If both a bom and a cookie are present, but
255:    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid
256:    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,
257:    'utf-8-sig' is returned.
258:
259:    If no encoding is specified, then the default of 'utf-8' will be returned.
260:    """
261:    bom_found = False
262:    encoding = None
263:    default = 'utf-8'
264:    def read_or_stop():
265:        try:
266:            return readline()
267:        except StopIteration:
268:            return bytes()
269:
270:    def find_cookie(line):
271:        try:
272:            line_string = line.decode('ascii')
273:        except UnicodeDecodeError:
274:            return None
275:        match = cookie_re.match(line_string)
276:        if not match:
277:            return None
278:        encoding = _get_normal_name(match.group(1))
279:        try:

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: In blib2to3/pgen2/tokenize.py, Untokenizer.compat() adds a trailing space for tokens when toknum in (NAME, NUMBER, ASYNC, AWAIT). Verified at lines ~213-214 where the condition explicitly includes ASYNC and AWAIT.


Todo item 10 of 15 is correct
description: 'Verify Untokenizer.compat() includes ASYNC and AWAIT tokens in the set that receive a trailing space (toknum in (NAME, NUMBER, ASYNC, AWAIT)).'
Validating todo item 11 of 15
description: 'Confirm all internal call sites to tokenize.generate_tokens either pass the new config (driver) or rely on the default (e.g., pgen2/pgen.py). Ensure no remaining internal call sites break due to signature change.'
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "generate_tokens\\(",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "from dataclasses import dataclass",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "driver\\.Driver\\(", "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black" ]
Calling tool: grepTool
generate_tokens\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:6:generate_tokens(readline) is a generator that breaks a stream of
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:166:    tuples generated by generate_tokens().
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:175:    for token_info in generate_tokens(readline):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:333:        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:336:        t2 = [tok[:2] for tokin generate_tokens(readline)]
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:344:    The generate_tokens() generator requires one argument, readline, which
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py:107:        tokens = tokenize.generate_tokens(stream.readline, config=self.tokenizer_config)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py:121:        tokens = tokenize.generate_tokens(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/pgen.py:19:        self.generator = tokenize.generate_tokens(stream.readline)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi:32:def generate_tokens(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.py:6:generate_tokens(readline) is a generator that breaks a stream of
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.py:161:    tuples generated by generate_tokens().
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.py:170:    for token_info in generate_tokens(readline):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.py:328:        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.py:331:        t2 = [tok[:2] for tokin generate_tokens(readline)]
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.py:337:def generate_tokens(readline):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.py:339:    The generate_tokens() generator requires one argument, readline, which
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/driver.py:100:        tokens = tokenize.generate_tokens(stream.readline)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/driver.py:114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.pyi:28:def generate_tokens(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/pgen.py:19:        self.generator = tokenize.generate_tokens(stream.readline)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/flake8/checker.py:578:        for token in file_processor.generate_tokens():
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/flake8/processor.py:120:                    tokenize.generate_tokens(lambda: next(line_iter))
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/flake8/processor.py:259:    def generate_tokens(self):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/flake8/processor.py:267:            for token in tokenize.generate_tokens(self.next_line):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pycodestyle.py:2026:    def generate_tokens(self):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pycodestyle.py:2030:        tokengen = tokenize.generate_tokens(self.readline)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pycodestyle.py:2088:        for token in self.generate_tokens():
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/sphinx/pycode/parser.py:134:        self.tokens = tokenize.generate_tokens(lambda: next(lines))
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/babel/messages/extract.py:417:    tokens = generate_tokens(next_line)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/2/tokenize.pyi:117:def generate_tokens(readline: Callable[[], str]) -> Generator[_TokenType, None, None]: ...
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/3/tokenize.pyi:46:def generate_tokens(readline: Callable[[], str]) -> Generator[TokenInfo, None, None]: ...  # undocumented
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/2and3/lib2to3/pgen2/tokenize.pyi:28:def generate_tokens(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/coverage/parser.py:138:        tokgen = generate_tokens(self.text)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/coverage/phystokens.py:21:    tokenize.generate_tokens() doesn't return a token for the backslash that
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/coverage/phystokens.py:25:    Returns the same values as generate_tokens()
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/coverage/phystokens.py:95:    tokgen = generate_tokens(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/coverage/phystokens.py:142:    def generate_tokens(self, text):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/coverage/phystokens.py:147:            self.last_tokens = list(tokenize.generate_tokens(readline))

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi
1:# Stubs for lib2to3.pgen2.tokenize (Python 3.6)
2:# NOTE: Only elements from __all__ are present.
3:
4:from typing import Callable, Iterable, Iterator, List, Text, Tuple
5:from attr import dataclass
6:from blib2to3.pgen2.token import *  # noqa
7:
8:
9:_Coord = Tuple[int, int]
10:_TokenEater = Callable[[int, Text, _Coord, _Coord, Text], None]
11:_TokenInfo = Tuple[int, Text, _Coord, _Coord, Text]
12:
13:@dataclass(frozen=True)
14:class TokenizerConfig:
15:    async_is_reserved_keyword: bool = False
16:
17:class TokenError(Exception): ...
18:class StopTokenizing(Exception): ...
19:
20:def tokenize(readline: Callable[[], Text], tokeneater: _TokenEater = ...) -> None: ...
21:
22:class Untokenizer:
23:    tokens: List[Text]
24:    prev_row: int
25:    prev_col: int
26:    def __init__(self) -> None: ...
27:    def add_whitespace(self, start: _Coord) -> None: ...
28:    def untokenize(self, iterable: Iterable[_TokenInfo]) -> Text: ...
29:    def compat(self, token: Tuple[int, Text], iterable: Iterable[_TokenInfo]) -> None: ...
30:
31:def untokenize(iterable: Iterable[_TokenInfo]) -> Text: ...
32:def generate_tokens(
33:    readline: Callable[[], Text]
34:) -> Iterator[_TokenInfo]: ...

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Modifications:
5:# Copyright 2006 Google, Inc. All Rights Reserved.
6:# Licensed to PSF under a Contributor Agreement.
7:
8:"""Parser driver.
9:
10:This provides a high-level interface to parse a file into a syntax tree.
11:
12:"""
13:
14:__author__ = "Guido van Rossum <guido@python.org>"
15:
16:__all__ = ["Driver", "load_grammar"]
17:
18:# Python imports
19:import codecs
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(
33:        self,
34:        grammar,
35:        convert=None,
36:        logger=None,
37:        tokenizer_config=tokenize.TokenizerConfig(),
38:    ):
39:        self.grammar = grammar
40:        if logger is None:
41:            logger = logging.getLogger(__name__)
42:        self.logger = logger
43:        self.convert = convert
44:        self.tokenizer_config = tokenizer_config
45:
46:    def parse_tokens(self, tokens, debug=False):
47:        """Parse a series of tokens and return the syntax tree."""
48:        # XXX Move the prefix computation into a wrapper around tokenize.
49:        p = parse.Parser(self.grammar, self.convert)
50:        p.setup()
51:        lineno = 1
52:        column = 0
53:        indent_columns = []
54:        type = value = start = end = line_text = None
55:        prefix = ""
56:        for quintuple in tokens:
57:            type, value, start, end, line_text = quintuple
58:            if start != (lineno, column):
59:                assert (lineno, column) <= start, ((lineno, column), start)
60:                s_lineno, s_column = start
61:                if lineno < s_lineno:
62:                    prefix += "\n" * (s_lineno - lineno)
63:                    lineno = s_lineno
64:                    column = 0
65:                if column < s_column:
66:                    prefix += line_text[column:s_column]
67:                    column = s_column
68:            if type in (tokenize.COMMENT, tokenize.NL):
69:                prefix += value
70:                lineno, column = end
71:                if value.endswith("\n"):
72:                    lineno += 1
73:                    column = 0
74:                continue
75:            if type == token.OP:
76:                type = grammar.opmap[value]
77:            if debug:
78:                self.logger.debug("%s %r (prefix=%r)",
79:                                  token.tok_name[type], value, prefix)
80:            if type == token.INDENT:
81:                indent_columns.append(len(value))
82:                _prefix = prefix + value
83:                prefix = ""
84:                value = ""
85:            elif type == token.DEDENT:
86:                _indent_col = indent_columns.pop()
87:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
88:            if p.addtoken(type, value, (prefix, start)):
89:                if debug:
90:                    self.logger.debug("Stop.")
91:                break
92:            prefix = ""
93:            if type in {token.INDENT, token.DEDENT}:
94:                prefix = _prefix
95:            lineno, column = end
96:            if value.endswith("\n"):
97:                lineno += 1
98:                column = 0
99:        else:
100:            # We never broke out -- EOF is too soon (how can this happen???)
101:            raise parse.ParseError("incomplete input",
102:                                   type, value, (prefix, start))
103:        return p.rootnode
104:
105:    def parse_stream_raw(self, stream, debug=False):
106:        """Parse a stream and return the syntax tree."""
107:        tokens = tokenize.generate_tokens(stream.readline, config=self.tokenizer_config)
108:        return self.parse_tokens(tokens, debug)
109:
110:    def parse_stream(self, stream, debug=False):
111:        """Parse a stream and return the syntax tree."""
112:        return self.parse_stream_raw(stream, debug)
113:
114:    def parse_file(self, filename, encoding=None, debug=False):
115:        """Parse a file and return the syntax tree."""
116:        with io.open(filename, "r", encoding=encoding) as stream:
117:            return self.parse_stream(stream, debug)
118:
119:    def parse_string(self, text, debug=False):
120:        """Parse a string and return the syntax tree."""
121:        tokens = tokenize.generate_tokens(
122:            io.StringIO(text).readline,
123:            config=self.tokenizer_config,
124:        )
125:        return self.parse_tokens(tokens, debug)
126:
127:    def _partially_consume_prefix(self, prefix, column):
128:        lines = []
129:        current_line = ""
130:        current_column = 0
131:        wait_for_nl = False
132:        for char in prefix:
133:            current_line += char
134:            if wait_for_nl:
135:                if char == '\n':
136:                    if current_line.strip() and current_column < column:
137:                        res = ''.join(lines)
138:                        return res, prefix[len(res):]
139:
140:                    lines.append(current_line)
141:                    current_line = ""
142:                    current_column = 0
143:                    wait_for_nl = False
144:            elif char in ' \t':
145:                current_column += 1
146:            elif char == '\n':
147:                # unexpected empty line
148:                current_column = 0
149:            else:
150:                # indent is finished
151:                wait_for_nl = True
152:        return ''.join(lines), current_line
153:
154:
155:def _generate_pickle_name(gt, cache_dir=None):
156:    head, tail = os.path.splitext(gt)
157:    if tail == ".txt":
158:        tail = ""
159:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
160:    if cache_dir:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/pgen.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Pgen imports
5:from . import grammar, token, tokenize
6:
7:class PgenGrammar(grammar.Grammar):
8:    pass
9:
10:class ParserGenerator(object):
11:
12:    def __init__(self, filename, stream=None):
13:        close_stream = None
14:        if stream is None:
15:            stream = open(filename)
16:            close_stream = stream.close
17:        self.filename = filename
18:        self.stream = stream
19:        self.generator = tokenize.generate_tokens(stream.readline)
20:        self.gettoken() # Initialize lookahead
21:        self.dfas, self.startsymbol = self.parse()
22:        if close_stream is not None:
23:            close_stream()
24:        self.first = {} # map from symbol name to set of tokens
25:        self.addfirstsets()
26:
27:    def make_grammar(self):
28:        c = PgenGrammar()
29:        names = list(self.dfas.keys())
30:        names.sort()
31:        names.remove(self.startsymbol)
32:        names.insert(0, self.startsymbol)
33:        for name in names:
34:            i = 256 + len(c.symbol2number)
35:            c.symbol2number[name] = i
36:            c.number2symbol[i] = name
37:        for name in names:
38:            dfa = self.dfas[name]
39:            states = []
40:            for state in dfa:
41:                arcs = []
42:                for label, next in sorted(state.arcs.items()):
43:                    arcs.append((self.make_label(c, label), dfa.index(next)))
44:                if state.isfinal:
45:                    arcs.append((0, dfa.index(state)))
46:                states.append(arcs)
47:            c.states.append(states)
48:            c.dfas[c.symbol2number[name]] = (states, self.make_first(c, name))
49:        c.start = c.symbol2number[self.startsymbol]
50:        return c
51:
52:    def make_first(self, c, name):
53:        rawfirst = self.first[name]
54:        first = {}
55:        for label in sorted(rawfirst):
56:            ilabel = self.make_label(c, label)
57:            ##assert ilabel not in first # XXX failed on <> ... !=
58:            first[ilabel] = 1
59:        return first
60:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
320:
321:def untokenize(iterable):
322:    """Transform tokens back into Python source code.
323:
324:    Each element returned by the iterable must be a token sequence
325:    with at least two elements, a token number and token value.  If
326:    only two tokens are passed, the resulting output is poor.
327:
328:    Round-trip invariant for full input:
329:        Untokenized source will match input source exactly
330:
331:    Round-trip invariant for limited intput:
332:        # Output text will tokenize the back to the input
333:        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
334:        newcode = untokenize(t1)
335:        readline = iter(newcode.splitlines(1)).next
336:        t2 = [tok[:2] for tokin generate_tokens(readline)]
337:        assert t1 == t2
338:    """
339:    ut = Untokenizer()
340:    return ut.untokenize(iterable)
341:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
343:    """
344:    The generate_tokens() generator requires one argument, readline, which
345:    must be a callable object which provides the same interface as the
346:    readline() method of built-in file objects. Each call to the function
347:    should return one line of input as a string.  Alternately, readline
348:    can be a callable function terminating with StopIteration:
349:        readline = open(myfile).next    # Example of alternate readline
350:
351:    The generator produces 5-tuples with these members: the token type; the
352:    token string; a 2-tuple (srow, scol) of ints specifying the row and
353:    column where the token begins in the source; a 2-tuple (erow, ecol) of
354:    ints specifying the row and column where the token ends in the source;
355:    and the line on which the token was found. The line passed is the
356:    logical line; continuation lines are included.
357:    """
358:    lnum = parenlev = continued = 0
359:    numchars = '0123456789'
360:    contstr, needcont = '', 0
361:    contline = None
362:    indents = [0]
363:
364:    # If we know we're parsing 3.7+, we can unconditionally parse `async` and
365:    # `await` as keywords.
366:    async_is_reserved_keyword = config.async_is_reserved_keyword
367:    # 'stashed' and 'async_*' are used for async/await parsing
368:    stashed = None
369:    async_def = False
370:    async_def_indent = 0
371:    async_def_nl = False
372:
373:    while 1:                                   # loop over lines in stream
374:        try:
375:            line = readline()
376:        except StopIteration:
377:            line = ''
378:        lnum = lnum + 1
379:        pos, max = 0, len(line)

Calling tool: grepTool
from dataclasses import dataclass
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/dataclasses-0.6.dist-info/METADATA:47:  from dataclasses import dataclass
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:14:from dataclasses import dataclass
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pip/_vendor/rich/_windows.py:2:from dataclasses import dataclass
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pip/_vendor/rich/progress.py:8:from dataclasses import dataclass, field
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pip/_vendor/rich/table.py:1:from dataclasses import dataclass, field, replace
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pip/_vendor/rich/pretty.py:9:from dataclasses import dataclass, fields, is_dataclass
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pip/_vendor/rich/_ratio.py:150:    from dataclasses import dataclass
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pip/_vendor/rich/traceback.py:6:from dataclasses import dataclass, field
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pip/_vendor/rich/console.py:9:from dataclasses import dataclass, field
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pip/_internal/self_outdated_check.py:9:from dataclasses import dataclass
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pip/_internal/models/link.py:8:from dataclasses import dataclass
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pip/_internal/utils/logging.py:8:from dataclasses import dataclass

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
1:import asyncio
2:from asyncio.base_events import BaseEventLoop
3:from concurrent.futures import Executor, ProcessPoolExecutor
4:from datetime import datetime
5:from enum import Enum
6:from functools import lru_cache, partial, wraps
7:import io
8:import itertools
9:import logging
10:from multiprocessing import Manager, freeze_support
11:import os
12:from pathlib import Path
13:import pickle
14:import re
15:import signal
16:import sys
17:import tempfile
18:import tokenize
19:from typing import (
20:    Any,
21:    Callable,
22:    Collection,
23:    Dict,
24:    Generator,
25:    Generic,
26:    Iterable,
27:    Iterator,
28:    List,
29:    Optional,
30:    Pattern,
31:    Sequence,
32:    Set,
33:    Tuple,
34:    TypeVar,
35:    Union,
36:    cast,
37:)
38:
39:from appdirs import user_cache_dir
40:from attr import dataclass, evolve, Factory
41:import click
42:import toml
43:from typed_ast import ast3, ast27
44:
45:# lib2to3 fork
46:from blib2to3.pytree import Node, Leaf, type_repr
47:from blib2to3 import pygram, pytree
48:from blib2to3.pgen2 import driver, token
49:from blib2to3.pgen2.grammar import Grammar
50:from blib2to3.pgen2.parse import ParseError
51:from blib2to3.pgen2.tokenize import TokenizerConfig
52:
53:
54:__version__ = "19.3b0"
55:DEFAULT_LINE_LENGTH = 88
56:DEFAULT_EXCLUDES = (
57:    r"/(\.eggs|\.git|\.hg|\.mypy_cache|\.nox|\.tox|\.venv|_build|buck-out|build|dist)/"
58:)
59:DEFAULT_INCLUDES = r"\.pyi?$"
60:CACHE_DIR = Path(user_cache_dir("black", version=__version__))
61:
62:
63:# types
64:FileContent = str
65:Encoding = str
66:NewLine = str
67:Depth = int
68:NodeType = int
69:LeafID = int
70:Priority = int
71:Index = int
72:LN = Union[Leaf, Node]
73:SplitFunc = Callable[["Line", Collection["Feature"]], Iterator["Line"]]
74:Timestamp = float
75:FileSize = int
76:CacheInfo = Tuple[Timestamp, FileSize]
77:Cache = Dict[Path, CacheInfo]
78:out = partial(click.secho, bold=True, err=True)
79:err = partial(click.secho, fg="red", err=True)
80:
81:pygram.initialize(CACHE_DIR)
82:syms = pygram.python_symbols
83:
84:
85:class NothingChanged(UserWarning):
86:    """Raised when reformatted code is the same as source."""
87:
88:
89:class CannotSplit(Exception):
90:    """A readable split that fits the allotted line length is impossible."""
91:
92:
93:class InvalidInput(ValueError):
94:    """Raised when input source code fails all parse attempts."""
95:
96:
97:class WriteBack(Enum):
98:    NO = 0
99:    YES = 1
100:    DIFF = 2
101:    CHECK = 3
102:
103:    @classmethod
104:    def from_configuration(cls, *, check: bool, diff: bool) -> "WriteBack":
105:        if check and not diff:
106:            return cls.CHECK
107:
108:        return cls.DIFF if diff else cls.YES
109:
110:
111:class Changed(Enum):
112:    NO = 0
113:    CACHED = 1
114:    YES = 2
115:
116:
117:class TargetVersion(Enum):
118:    PY27 = 2
119:    PY33 = 3
120:    PY34 = 4
121:    PY35 = 5
122:    PY36 = 6
123:    PY37 = 7
124:    PY38 = 8
125:
126:    def is_python2(self) -> bool:
127:        return self is TargetVersion.PY27
128:
129:
130:PY36_VERSIONS = {TargetVersion.PY36, TargetVersion.PY37, TargetVersion.PY38}
131:
132:
133:class Feature(Enum):
134:    # All string literals are unicode
135:    UNICODE_LITERALS = 1
136:    F_STRINGS = 2
137:    NUMERIC_UNDERSCORES = 3
138:    TRAILING_COMMA_IN_CALL = 4
139:    TRAILING_COMMA_IN_DEF = 5
140:    # The following two feature-flags are mutually exclusive, and exactly one should be
141:    # set for every version of python.
142:    ASYNC_IS_VALID_IDENTIFIER = 6
143:    ASYNC_IS_RESERVED_KEYWORD = 7
144:
145:
146:VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {
147:    TargetVersion.PY27: {Feature.ASYNC_IS_VALID_IDENTIFIER},
148:    TargetVersion.PY33: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
149:    TargetVersion.PY34: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
150:    TargetVersion.PY35: {
151:        Feature.UNICODE_LITERALS,
152:        Feature.TRAILING_COMMA_IN_CALL,
153:        Feature.ASYNC_IS_VALID_IDENTIFIER,
154:    },
155:    TargetVersion.PY36: {
156:        Feature.UNICODE_LITERALS,
157:        Feature.F_STRINGS,
158:        Feature.NUMERIC_UNDERSCORES,
159:        Feature.TRAILING_COMMA_IN_CALL,
160:        Feature.TRAILING_COMMA_IN_DEF,
161:        Feature.ASYNC_IS_VALID_IDENTIFIER,
162:    },
163:    TargetVersion.PY37: {
164:        Feature.UNICODE_LITERALS,
165:        Feature.F_STRINGS,
166:        Feature.NUMERIC_UNDERSCORES,
167:        Feature.TRAILING_COMMA_IN_CALL,
168:        Feature.TRAILING_COMMA_IN_DEF,
169:        Feature.ASYNC_IS_RESERVED_KEYWORD,
170:    },
171:    TargetVersion.PY38: {
172:        Feature.UNICODE_LITERALS,
173:        Feature.F_STRINGS,
174:        Feature.NUMERIC_UNDERSCORES,
175:        Feature.TRAILING_COMMA_IN_CALL,
176:        Feature.TRAILING_COMMA_IN_DEF,
177:        Feature.ASYNC_IS_RESERVED_KEYWORD,
178:    },
179:}
180:
181:
182:@dataclass
183:class FileMode:
184:    target_versions: Set[TargetVersion] = Factory(set)
185:    line_length: int = DEFAULT_LINE_LENGTH
186:    string_normalization: bool = True
187:    is_pyi: bool = False
188:
189:    def get_cache_key(self) -> str:
190:        if self.target_versions:
191:            version_str = ",".join(
192:                str(version.value)
193:                for version in sorted(self.target_versions, key=lambda v: v.value)
194:            )
195:        else:
196:            version_str = "-"
197:        parts = [
198:            version_str,
199:            str(self.line_length),
200:            str(int(self.string_normalization)),
201:            str(int(self.is_pyi)),
202:        ]
203:        return ".".join(parts)
204:
205:
206:def supports_feature(target_versions: Set[TargetVersion], feature: Feature) -> bool:
207:    return all(feature in VERSION_TO_FEATURES[version] for version in target_versions)
208:
209:
210:def read_pyproject_toml(
211:    ctx: click.Context, param: click.Parameter, value: Union[str, int, bool, None]
212:) -> Optional[str]:
213:    """Inject Black configuration from "pyproject.toml" into defaults in `ctx`.
214:
215:    Returns the path to a successfully found and read configuration file, None
216:    otherwise.
217:    """
218:    assert not isinstance(value, (int, bool)), "Invalid parameter type passed"
219:    if not value:
220:        root = find_project_root(ctx.params.get("src", ()))

Calling tool: grepTool
driver\.Driver\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:827:        drv = driver.Driver(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/black.py:773:        drv = driver.Driver(grammar, pytree.convert)

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: Searched for all internal calls to generate_tokens. Only blib2to3/pgen2/driver.py and blib2to3/pgen2/pgen.py call it. Driver was updated to pass tokenizer_config via keyword, while pgen.py still calls it with only readline, relying on the new default TokenizerConfig(). No other internal call sites exist, so the signature change doesnt break anything.


Todo item 11 of 15 is correct
description: 'Confirm all internal call sites to tokenize.generate_tokens either pass the new config (driver) or rely on the default (e.g., pgen2/pgen.py). Ensure no remaining internal call sites break due to signature change.'
Validating todo item 12 of 15
description: 'Check blib2to3/pgen2/tokenize.pyi is consistent with implementation: it defines TokenizerConfig dataclass (frozen) with async_is_reserved_keyword, but its generate_tokens stub currently lacks the config parameter. Flag this inconsistency and require updating the stub to include the optional config parameter to match runtime signature.'
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "def generate_tokens\\(",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py"
]
Calling tool: globTool
**/tokenize.pyi
Matched files:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.pyi
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/2/tokenize.pyi
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/3/tokenize.pyi
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/2and3/lib2to3/pgen2/tokenize.pyi

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi
1:# Stubs for lib2to3.pgen2.tokenize (Python 3.6)
2:# NOTE: Only elements from __all__ are present.
3:
4:from typing import Callable, Iterable, Iterator, List, Text, Tuple
5:from attr import dataclass
6:from blib2to3.pgen2.token import *  # noqa
7:
8:
9:_Coord = Tuple[int, int]
10:_TokenEater = Callable[[int, Text, _Coord, _Coord, Text], None]
11:_TokenInfo = Tuple[int, Text, _Coord, _Coord, Text]
12:
13:@dataclass(frozen=True)
14:class TokenizerConfig:
15:    async_is_reserved_keyword: bool = False
16:
17:class TokenError(Exception): ...
18:class StopTokenizing(Exception): ...
19:
20:def tokenize(readline: Callable[[], Text], tokeneater: _TokenEater = ...) -> None: ...
21:
22:class Untokenizer:
23:    tokens: List[Text]
24:    prev_row: int
25:    prev_col: int
26:    def __init__(self) -> None: ...
27:    def add_whitespace(self, start: _Coord) -> None: ...
28:    def untokenize(self, iterable: Iterable[_TokenInfo]) -> Text: ...
29:    def compat(self, token: Tuple[int, Text], iterable: Iterable[_TokenInfo]) -> None: ...
30:
31:def untokenize(iterable: Iterable[_TokenInfo]) -> Text: ...
32:def generate_tokens(
33:    readline: Callable[[], Text]
34:) -> Iterator[_TokenInfo]: ...

Calling tool: grepTool
def generate_tokens\(
Matched content:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
330:
331:    Round-trip invariant for limited intput:
332:        # Output text will tokenize the back to the input
333:        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
334:        newcode = untokenize(t1)
335:        readline = iter(newcode.splitlines(1)).next
336:        t2 = [tok[:2] for tokin generate_tokens(readline)]
337:        assert t1 == t2
338:    """
339:    ut = Untokenizer()
340:    return ut.untokenize(iterable)
341:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
343:    """
344:    The generate_tokens() generator requires one argument, readline, which
345:    must be a callable object which provides the same interface as the
346:    readline() method of built-in file objects. Each call to the function
347:    should return one line of input as a string.  Alternately, readline
348:    can be a callable function terminating with StopIteration:
349:        readline = open(myfile).next    # Example of alternate readline
350:
351:    The generator produces 5-tuples with these members: the token type; the
352:    token string; a 2-tuple (srow, scol) of ints specifying the row and
353:    column where the token begins in the source; a 2-tuple (erow, ecol) of
354:    ints specifying the row and column where the token ends in the source;
355:    and the line on which the token was found. The line passed is the
356:    logical line; continuation lines are included.
357:    """
358:    lnum = parenlev = continued = 0
359:    numchars = '0123456789'
360:    contstr, needcont = '', 0
361:    contline = None
362:    indents = [0]
363:
364:    # If we know we're parsing 3.7+, we can unconditionally parse `async` and
365:    # `await` as keywords.
366:    async_is_reserved_keyword = config.async_is_reserved_keyword
367:    # 'stashed' and 'async_*' are used for async/await parsing
368:    stashed = None
369:    async_def = False

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as INCORRECT
Reason: In implementation, generate_tokens(readline, config: TokenizerConfig = ...) was added, but the stub blib2to3/pgen2/tokenize.pyi still declares generate_tokens(readline) without the config parameter. While TokenizerConfig is defined in the stub, the function signature is inconsistent and needs updating.


Validating todo item 1 of 15
description: 'Verify Feature enum includes ASYNC_IS_VALID_IDENTIFIER and ASYNC_IS_RESERVED_KEYWORD and that VERSION_TO_FEATURES maps each TargetVersion (PY27, PY33, PY34, PY35, PY36, PY37, PY38) to exactly one of these two flags (mutually exclusive, never both, never neither).'

Calling tool: grepTool
class Feature\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:133:class Feature(Enum):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/black.py:132:class Feature(Enum):

Calling tool: grepTool
VERSION_TO_FEATURES
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:146:VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:207:    return all(feature in VERSION_TO_FEATURES[version] for version in target_versions)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:3187:        version for version in TargetVersion if features <= VERSION_TO_FEATURES[version]
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/black.py:141:VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/black.py:195:    return all(feature in VERSION_TO_FEATURES[version] for version in target_versions)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/black.py:3129:        version for version in TargetVersion if features <= VERSION_TO_FEATURES[version]

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
120:    PY34 = 4
121:    PY35 = 5
122:    PY36 = 6
123:    PY37 = 7
124:    PY38 = 8
125:
126:    def is_python2(self) -> bool:
127:        return self is TargetVersion.PY27
128:
129:
130:PY36_VERSIONS = {TargetVersion.PY36, TargetVersion.PY37, TargetVersion.PY38}
131:
132:
133:class Feature(Enum):
134:    # All string literals are unicode
135:    UNICODE_LITERALS = 1
136:    F_STRINGS = 2
137:    NUMERIC_UNDERSCORES = 3
138:    TRAILING_COMMA_IN_CALL = 4
139:    TRAILING_COMMA_IN_DEF = 5
140:    # The following two feature-flags are mutually exclusive, and exactly one should be
141:    # set for every version of python.
142:    ASYNC_IS_VALID_IDENTIFIER = 6
143:    ASYNC_IS_RESERVED_KEYWORD = 7
144:
145:
146:VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {
147:    TargetVersion.PY27: {Feature.ASYNC_IS_VALID_IDENTIFIER},
148:    TargetVersion.PY33: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
149:    TargetVersion.PY34: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
150:    TargetVersion.PY35: {
151:        Feature.UNICODE_LITERALS,
152:        Feature.TRAILING_COMMA_IN_CALL,
153:        Feature.ASYNC_IS_VALID_IDENTIFIER,
154:    },
155:    TargetVersion.PY36: {
156:        Feature.UNICODE_LITERALS,
157:        Feature.F_STRINGS,
158:        Feature.NUMERIC_UNDERSCORES,
159:        Feature.TRAILING_COMMA_IN_CALL,
160:        Feature.TRAILING_COMMA_IN_DEF,
161:        Feature.ASYNC_IS_VALID_IDENTIFIER,
162:    },
163:    TargetVersion.PY37: {
164:        Feature.UNICODE_LITERALS,
165:        Feature.F_STRINGS,
166:        Feature.NUMERIC_UNDERSCORES,
167:        Feature.TRAILING_COMMA_IN_CALL,
168:        Feature.TRAILING_COMMA_IN_DEF,
169:        Feature.ASYNC_IS_RESERVED_KEYWORD,
170:    },
171:    TargetVersion.PY38: {
172:        Feature.UNICODE_LITERALS,
173:        Feature.F_STRINGS,
174:        Feature.NUMERIC_UNDERSCORES,
175:        Feature.TRAILING_COMMA_IN_CALL,
176:        Feature.TRAILING_COMMA_IN_DEF,
177:        Feature.ASYNC_IS_RESERVED_KEYWORD,
178:    },
179:}
180:
181:
182:@dataclass
183:class FileMode:
184:    target_versions: Set[TargetVersion] = Factory(set)
185:    line_length: int = DEFAULT_LINE_LENGTH
186:    string_normalization: bool = True
187:    is_pyi: bool = False
188:
189:    def get_cache_key(self) -> str:
190:        if self.target_versions:
191:            version_str = ",".join(
192:                str(version.value)
193:                for version in sorted(self.target_versions, key=lambda v: v.value)
194:            )
195:        else:
196:            version_str = "-"
197:        parts = [
198:            version_str,
199:            str(self.line_length),
200:            str(int(self.string_normalization)),
201:            str(int(self.is_pyi)),
202:        ]
203:        return ".".join(parts)
204:
205:
206:def supports_feature(target_versions: Set[TargetVersion], feature: Feature) -> bool:
207:    return all(feature in VERSION_TO_FEATURES[version] for version in target_versions)
208:
209:
210:def read_pyproject_toml(
211:    ctx: click.Context, param: click.Parameter, value: Union[str, int, bool, None]
212:) -> Optional[str]:
213:    """Inject Black configuration from "pyproject.toml" into defaults in `ctx`.
214:
215:    Returns the path to a successfully found and read configuration file, None
216:    otherwise.
217:    """
218:    assert not isinstance(value, (int, bool)), "Invalid parameter type passed"
219:    if not value:
220:        root = find_project_root(ctx.params.get("src", ()))
221:        path = root / "pyproject.toml"
222:        if path.is_file():
223:            value = str(path)
224:        else:
225:            return None
226:
227:    try:
228:        pyproject_toml = toml.load(value)
229:        config = pyproject_toml.get("tool", {}).get("black", {})
230:    except (toml.TomlDecodeError, OSError) as e:
231:        raise click.FileError(
232:            filename=value, hint=f"Error reading configuration file: {e}"
233:        )
234:
235:    if not config:
236:        return None
237:
238:    if ctx.default_map is None:
239:        ctx.default_map = {}

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/black.py
120:    PY35 = 5
121:    PY36 = 6
122:    PY37 = 7
123:    PY38 = 8
124:
125:    def is_python2(self) -> bool:
126:        return self is TargetVersion.PY27
127:
128:
129:PY36_VERSIONS = {TargetVersion.PY36, TargetVersion.PY37, TargetVersion.PY38}
130:
131:
132:class Feature(Enum):
133:    # All string literals are unicode
134:    UNICODE_LITERALS = 1
135:    F_STRINGS = 2
136:    NUMERIC_UNDERSCORES = 3
137:    TRAILING_COMMA_IN_CALL = 4
138:    TRAILING_COMMA_IN_DEF = 5
139:
140:
141:VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {
142:    TargetVersion.PY27: set(),
143:    TargetVersion.PY33: {Feature.UNICODE_LITERALS},
144:    TargetVersion.PY34: {Feature.UNICODE_LITERALS},
145:    TargetVersion.PY35: {Feature.UNICODE_LITERALS, Feature.TRAILING_COMMA_IN_CALL},
146:    TargetVersion.PY36: {
147:        Feature.UNICODE_LITERALS,
148:        Feature.F_STRINGS,
149:        Feature.NUMERIC_UNDERSCORES,
150:        Feature.TRAILING_COMMA_IN_CALL,
151:        Feature.TRAILING_COMMA_IN_DEF,
152:    },
153:    TargetVersion.PY37: {
154:        Feature.UNICODE_LITERALS,
155:        Feature.F_STRINGS,
156:        Feature.NUMERIC_UNDERSCORES,
157:        Feature.TRAILING_COMMA_IN_CALL,
158:        Feature.TRAILING_COMMA_IN_DEF,
159:    },
160:    TargetVersion.PY38: {
161:        Feature.UNICODE_LITERALS,
162:        Feature.F_STRINGS,
163:        Feature.NUMERIC_UNDERSCORES,
164:        Feature.TRAILING_COMMA_IN_CALL,
165:        Feature.TRAILING_COMMA_IN_DEF,
166:    },
167:}
168:
169:
170:@dataclass
171:class FileMode:
172:    target_versions: Set[TargetVersion] = Factory(set)
173:    line_length: int = DEFAULT_LINE_LENGTH
174:    string_normalization: bool = True
175:    is_pyi: bool = False
176:
177:    def get_cache_key(self) -> str:
178:        if self.target_versions:
179:            version_str = ",".join(
180:                str(version.value)
181:                for version in sorted(self.target_versions, key=lambda v: v.value)
182:            )
183:        else:
184:            version_str = "-"
185:        parts = [
186:            version_str,
187:            str(self.line_length),
188:            str(int(self.string_normalization)),
189:            str(int(self.is_pyi)),
190:        ]
191:        return ".".join(parts)
192:
193:
194:def supports_feature(target_versions: Set[TargetVersion], feature: Feature) -> bool:
195:    return all(feature in VERSION_TO_FEATURES[version] for version in target_versions)
196:
197:
198:def read_pyproject_toml(
199:    ctx: click.Context, param: click.Parameter, value: Union[str, int, bool, None]
200:) -> Optional[str]:
201:    """Inject Black configuration from "pyproject.toml" into defaults in `ctx`.
202:
203:    Returns the path to a successfully found and read configuration file, None
204:    otherwise.
205:    """
206:    assert not isinstance(value, (int, bool)), "Invalid parameter type passed"
207:    if not value:
208:        root = find_project_root(ctx.params.get("src", ()))
209:        path = root / "pyproject.toml"
210:        if path.is_file():
211:            value = str(path)
212:        else:
213:            return None
214:
215:    try:
216:        pyproject_toml = toml.load(value)
217:        config = pyproject_toml.get("tool", {}).get("black", {})
218:    except (toml.TomlDecodeError, OSError) as e:
219:        raise click.FileError(
220:            filename=value, hint=f"Error reading configuration file: {e}"
221:        )
222:
223:    if not config:
224:        return None
225:
226:    if ctx.default_map is None:
227:        ctx.default_map = {}
228:    ctx.default_map.update(  # type: ignore  # bad types in .pyi
229:        {k.replace("--", "").replace("-", "_"): v for k, v in config.items()}
230:    )
231:    return value
232:
233:
234:@click.command(context_settings=dict(help_option_names=["-h", "--help"]))
235:@click.option("-c", "--code", type=str, help="Format the code passed in as a string.")
236:@click.option(
237:    "-l",
238:    "--line-length",
239:    type=int,

Calling tool: globTool
**/env/src/black/black.py
Matched files:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/black.py

Calling tool: grepTool
test_async_as_identifier
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/coverage_bugsinpy.txt:1:coverage run -m unittest -q tests.test_black.BlackTestCase.test_async_as_identifier
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:506:    def test_async_as_identifier(self) -> None:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/bugsinpy_run_test.sh:1:python -m unittest -q tests.test_black.BlackTestCase.test_async_as_identifier

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py
480:    @patch("black.dump_to_file", dump_to_stderr)
481:    def test_python2_print_function(self) -> None:
482:        source, expected = read_data("python2_print_function")
483:        mode = black.FileMode(target_versions={TargetVersion.PY27})
484:        actual = fs(source, mode=mode)
485:        self.assertFormatEqual(expected, actual)
486:        black.assert_equivalent(source, actual)
487:        black.assert_stable(source, actual, mode)
488:
489:    @patch("black.dump_to_file", dump_to_stderr)
490:    def test_python2_unicode_literals(self) -> None:
491:        source, expected = read_data("python2_unicode_literals")
492:        actual = fs(source)
493:        self.assertFormatEqual(expected, actual)
494:        black.assert_equivalent(source, actual)
495:        black.assert_stable(source, actual, black.FileMode())
496:
497:    @patch("black.dump_to_file", dump_to_stderr)
498:    def test_stub(self) -> None:
499:        mode = black.FileMode(is_pyi=True)
500:        source, expected = read_data("stub.pyi")
501:        actual = fs(source, mode=mode)
502:        self.assertFormatEqual(expected, actual)
503:        black.assert_stable(source, actual, mode)
504:
505:    @patch("black.dump_to_file", dump_to_stderr)
506:    def test_async_as_identifier(self) -> None:
507:        source_path = (THIS_DIR / "data" / "async_as_identifier.py").resolve()
508:        source, expected = read_data("async_as_identifier")
509:        actual = fs(source)
510:        self.assertFormatEqual(expected, actual)
511:        major, minor = sys.version_info[:2]
512:        if major < 3 or (major <= 3 and minor < 7):
513:            black.assert_equivalent(source, actual)
514:        black.assert_stable(source, actual, black.FileMode())
515:        # ensure black can parse this when the target is 3.6
516:        self.invokeBlack([str(source_path), "--target-version", "py36"])
517:        # but not on 3.7, because async/await is no longer an identifier
518:        self.invokeBlack([str(source_path), "--target-version", "py37"], exit_code=123)
519:
520:    @patch("black.dump_to_file", dump_to_stderr)
521:    def test_python37(self) -> None:
522:        source_path = (THIS_DIR / "data" / "python37.py").resolve()
523:        source, expected = read_data("python37")
524:        actual = fs(source)
525:        self.assertFormatEqual(expected, actual)
526:        major, minor = sys.version_info[:2]
527:        if major > 3 or (major == 3 and minor >= 7):
528:            black.assert_equivalent(source, actual)
529:        black.assert_stable(source, actual, black.FileMode())
530:        # ensure black can parse this when the target is 3.7
531:        self.invokeBlack([str(source_path), "--target-version", "py37"])
532:        # but not on 3.6, because we use async as a reserved keyword
533:        self.invokeBlack([str(source_path), "--target-version", "py36"], exit_code=123)
534:
535:    @patch("black.dump_to_file", dump_to_stderr)
536:    def test_fmtonoff(self) -> None:
537:        source, expected = read_data("fmtonoff")
538:        actual = fs(source)
539:        self.assertFormatEqual(expected, actual)
540:        black.assert_equivalent(source, actual)
541:        black.assert_stable(source, actual, black.FileMode())
542:
543:    @patch("black.dump_to_file", dump_to_stderr)
544:    def test_fmtonoff2(self) -> None:
545:        source, expected = read_data("fmtonoff2")
546:        actual = fs(source)
547:        self.assertFormatEqual(expected, actual)
548:        black.assert_equivalent(source, actual)
549:        black.assert_stable(source, actual, black.FileMode())
550:
551:    @patch("black.dump_to_file", dump_to_stderr)
552:    def test_remove_empty_parentheses_after_class(self) -> None:
553:        source, expected = read_data("class_blank_parentheses")
554:        actual = fs(source)
555:        self.assertFormatEqual(expected, actual)
556:        black.assert_equivalent(source, actual)
557:        black.assert_stable(source, actual, black.FileMode())
558:
559:    @patch("black.dump_to_file", dump_to_stderr)

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py
1:#!/usr/bin/env python3
2:import asyncio
3:import logging
4:from concurrent.futures import ThreadPoolExecutor
5:from contextlib import contextmanager
6:from functools import partial, wraps
7:from io import BytesIO, TextIOWrapper
8:import os
9:from pathlib import Path
10:import re
11:import sys
12:from tempfile import TemporaryDirectory
13:from typing import (
14:    Any,
15:    BinaryIO,
16:    Callable,
17:    Coroutine,
18:    Generator,
19:    List,
20:    Tuple,
21:    Iterator,
22:    TypeVar,
23:)
24:import unittest
25:from unittest.mock import patch, MagicMock
26:
27:from click import unstyle
28:from click.testing import CliRunner
29:
30:import black
31:from black import Feature, TargetVersion
32:
33:try:
34:    import blackd
35:    from aiohttp.test_utils import TestClient, TestServer
36:except ImportError:
37:    has_blackd_deps = False
38:else:
39:    has_blackd_deps = True
40:

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: In black.py, Feature enum includes ASYNC_IS_VALID_IDENTIFIER and ASYNC_IS_RESERVED_KEYWORD. VERSION_TO_FEATURES maps PY27PY36 to ASYNC_IS_VALID_IDENTIFIER and PY37PY38 to ASYNC_IS_RESERVED_KEYWORD, ensuring each version has exactly one of the two flags (mutually exclusive).

Todo item 1 of 15 is correct
description: 'Verify Feature enum includes ASYNC_IS_VALID_IDENTIFIER and ASYNC_IS_RESERVED_KEYWORD and that VERSION_TO_FEATURES maps each TargetVersion (PY27, PY33, PY34, PY35, PY36, PY37, PY38) to exactly one of these two flags (mutually exclusive, never both, never neither).'

Validating todo item 2 of 15
description: 'Confirm black.ParserConfig dataclass exists with fields: grammar: Grammar and tokenizer_config: TokenizerConfig defaulting to TokenizerConfig(). Ensure TokenizerConfig is imported from blib2to3.pgen2.tokenize.'

Calling tool: grepTool
class ParserConfig|TokenizerConfig
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:142:class TokenizerConfig:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py:37:        tokenizer_config=tokenize.TokenizerConfig(),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.pyi:11:from blib2to3.pgen2.tokenize import TokenizerConfig
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.pyi:23:        tokenizer_config: TokenizerConfig = ...
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi:14:class TokenizerConfig:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:51:from blib2to3.pgen2.tokenize import TokenizerConfig
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:764:class ParserConfig:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:766:    tokenizer_config: TokenizerConfig = TokenizerConfig()
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:776:                TokenizerConfig(async_is_reserved_keyword=True),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:781:                TokenizerConfig(async_is_reserved_keyword=False),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:805:                    TokenizerConfig(async_is_reserved_keyword=True),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:813:                    TokenizerConfig(async_is_reserved_keyword=False),

Calling tool: grepTool
from dataclasses import dataclass
No files found. Call grepTool again with a more generic pattern.

Calling tool: grepTool
dataclass
Matched content:
40:from attr import dataclass, evolve, Factory
182:@dataclass
763:@dataclass(frozen=True)
887:@dataclass
1034:@dataclass
1166:@dataclass
1460:@dataclass
1571:@dataclass
2180:@dataclass
3357:@dataclass

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
30:    Pattern,
31:    Sequence,
32:    Set,
33:    Tuple,
34:    TypeVar,
35:    Union,
36:    cast,
37:)
38:
39:from appdirs import user_cache_dir
40:from attr import dataclass, evolve, Factory
41:import click
42:import toml
43:from typed_ast import ast3, ast27
44:
45:# lib2to3 fork
46:from blib2to3.pytree import Node, Leaf, type_repr
47:from blib2to3 import pygram, pytree
48:from blib2to3.pgen2 import driver, token
49:from blib2to3.pgen2.grammar import Grammar
50:from blib2to3.pgen2.parse import ParseError
51:from blib2to3.pgen2.tokenize import TokenizerConfig
52:
53:
54:__version__ = "19.3b0"
55:DEFAULT_LINE_LENGTH = 88
56:DEFAULT_EXCLUDES = (
57:    r"/(\.eggs|\.git|\.hg|\.mypy_cache|\.nox|\.tox|\.venv|_build|buck-out|build|dist)/"
58:)
59:DEFAULT_INCLUDES = r"\.pyi?$"
60:CACHE_DIR = Path(user_cache_dir("black", version=__version__))
61:
62:
63:# types
64:FileContent = str
65:Encoding = str
66:NewLine = str
67:Depth = int
68:NodeType = int
69:LeafID = int

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
740:            current_line, line_length=mode.line_length, features=split_line_features
741:        ):
742:            dst_contents += str(line)
743:    return dst_contents
744:
745:
746:def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:
747:    """Return a tuple of (decoded_contents, encoding, newline).
748:
749:    `newline` is either CRLF or LF but `decoded_contents` is decoded with
750:    universal newlines (i.e. only contains LF).
751:    """
752:    srcbuf = io.BytesIO(src)
753:    encoding, lines = tokenize.detect_encoding(srcbuf.readline)
754:    if not lines:
755:        return "", encoding, "\n"
756:
757:    newline = "\r\n" if b"\r\n" == lines[0][-2:] else "\n"
758:    srcbuf.seek(0)
759:    with io.TextIOWrapper(srcbuf, encoding) as tiow:
760:        return tiow.read(), encoding, newline
761:
762:
763:@dataclass(frozen=True)
764:class ParserConfig:
765:    grammar: Grammar
766:    tokenizer_config: TokenizerConfig = TokenizerConfig()
767:
768:
769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:
770:    if not target_versions:
771:        # No target_version specified, so try all grammars.
772:        return [
773:            # Python 3.7+
774:            ParserConfig(
775:                pygram.python_grammar_no_print_statement_no_exec_statement,
776:                TokenizerConfig(async_is_reserved_keyword=True),
777:            ),
778:            # Python 3.0-3.6
779:            ParserConfig(
780:                pygram.python_grammar_no_print_statement_no_exec_statement,
781:                TokenizerConfig(async_is_reserved_keyword=False),
782:            ),
783:            # Python 2.7 with future print_function import
784:            ParserConfig(pygram.python_grammar_no_print_statement),
785:            # Python 2.7
786:            ParserConfig(pygram.python_grammar),
787:        ]
788:    elif all(version.is_python2() for version in target_versions):
789:        # Python 2-only code, so try Python 2 grammars.

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
130:    {f"{prefix}'''" for prefix in _strprefixes} |
131:    {f'{prefix}"""' for prefix in _strprefixes}
132:)
133:single_quoted = (
134:    {"'", '"'} |
135:    {f"{prefix}'" for prefix in _strprefixes} |
136:    {f'{prefix}"' for prefix in _strprefixes}
137:)
138:
139:tabsize = 8
140:
141:@dataclass(frozen=True)
142:class TokenizerConfig:
143:    async_is_reserved_keyword: bool = False
144:
145:class TokenError(Exception): pass
146:
147:class StopTokenizing(Exception): pass
148:
149:def printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line): # for testing
150:    (srow, scol) = xxx_todo_changeme
151:    (erow, ecol) = xxx_todo_changeme1
152:    print("%d,%d-%d,%d:\t%s\t%s" % \
153:        (srow, scol, erow, ecol, tok_name[type], repr(token)))

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: Verified in black.py: ParserConfig dataclass exists (lines 763-767) with fields grammar: Grammar and tokenizer_config: TokenizerConfig = TokenizerConfig(). Also confirmed import: from blib2to3.pgen2.tokenize import TokenizerConfig (line 51).

Todo item 2 of 15 is correct
description: 'Confirm black.ParserConfig dataclass exists with fields: grammar: Grammar and tokenizer_config: TokenizerConfig defaulting to TokenizerConfig(). Ensure TokenizerConfig is imported from blib2to3.pgen2.tokenize.'

Validating todo item 3 of 15
description: 'Validate get_parser_configs() logic and order for parsing attempts: (a) when target_versions is empty, returns four ParserConfig entries in this order: Python 3 grammar with TokenizerConfig(async_is_reserved_keyword=True), Python 3 grammar with TokenizerConfig(async_is_reserved_keyword=False), Python 2 no_print grammar, Python 2 grammar; (b) when all target_versions are Python 2, returns only the two Python 2 grammars (no TokenizerConfig overrides); (c) when any Python 3 target_versions, returns Python 3 ParserConfigs depending on supports_feature: include reserved-keyword config if ASYNC_IS_VALID_IDENTIFIER is not supported by all versions; include valid-identifier config if ASYNC_IS_RESERVED_KEYWORD is not supported by all versions, with reserved-first order when both are added.'

Calling tool: grepTool
def supports_feature\(
Grep command failed with exit code 2: rg: .rgignore: No such file or directory (os error 2)
rg: /Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/{{FOLDER_PATH}}: IO error for operation on /Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/{{FOLDER_PATH}}: No such file or directory (os error 2)


Calling tool: grepTool
get_parser_configs\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:826:    for parser_config in get_parser_configs(set(target_versions)):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
730:        for feature in {Feature.TRAILING_COMMA_IN_CALL, Feature.TRAILING_COMMA_IN_DEF}
731:        if supports_feature(versions, feature)
732:    }
733:    for current_line in lines.visit(src_node):
734:        for _ in range(after):
735:            dst_contents += str(empty_line)
736:        before, after = elt.maybe_empty_lines(current_line)
737:        for _ in range(before):
738:            dst_contents += str(empty_line)
739:        for line in split_line(
740:            current_line, line_length=mode.line_length, features=split_line_features
741:        ):
742:            dst_contents += str(line)
743:    return dst_contents
744:
745:
746:def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:
747:    """Return a tuple of (decoded_contents, encoding, newline).
748:
749:    `newline` is either CRLF or LF but `decoded_contents` is decoded with
750:    universal newlines (i.e. only contains LF).
751:    """
752:    srcbuf = io.BytesIO(src)
753:    encoding, lines = tokenize.detect_encoding(srcbuf.readline)
754:    if not lines:
755:        return "", encoding, "\n"
756:
757:    newline = "\r\n" if b"\r\n" == lines[0][-2:] else "\n"
758:    srcbuf.seek(0)
759:    with io.TextIOWrapper(srcbuf, encoding) as tiow:
760:        return tiow.read(), encoding, newline
761:
762:
763:@dataclass(frozen=True)
764:class ParserConfig:
765:    grammar: Grammar
766:    tokenizer_config: TokenizerConfig = TokenizerConfig()
767:
768:
769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:
770:    if not target_versions:
771:        # No target_version specified, so try all grammars.
772:        return [
773:            # Python 3.7+
774:            ParserConfig(
775:                pygram.python_grammar_no_print_statement_no_exec_statement,
776:                TokenizerConfig(async_is_reserved_keyword=True),
777:            ),
778:            # Python 3.0-3.6
779:            ParserConfig(
780:                pygram.python_grammar_no_print_statement_no_exec_statement,
781:                TokenizerConfig(async_is_reserved_keyword=False),
782:            ),
783:            # Python 2.7 with future print_function import
784:            ParserConfig(pygram.python_grammar_no_print_statement),
785:            # Python 2.7
786:            ParserConfig(pygram.python_grammar),
787:        ]
788:    elif all(version.is_python2() for version in target_versions):
789:        # Python 2-only code, so try Python 2 grammars.
790:        return [
791:            # Python 2.7 with future print_function import
792:            ParserConfig(pygram.python_grammar_no_print_statement),
793:            # Python 2.7
794:            ParserConfig(pygram.python_grammar),
795:        ]
796:    else:
797:        # Python 3-compatible code, so only try Python 3 grammar.
798:        configs = []
799:        # If we have to parse both, try to parse async as a keyword first
800:        if not supports_feature(target_versions, Feature.ASYNC_IS_VALID_IDENTIFIER):
801:            # Python 3.7+
802:            configs.append(
803:                ParserConfig(
804:                    pygram.python_grammar_no_print_statement_no_exec_statement,
805:                    TokenizerConfig(async_is_reserved_keyword=True),
806:                )
807:            )
808:        if not supports_feature(target_versions, Feature.ASYNC_IS_RESERVED_KEYWORD):
809:            # Python 3.0-3.6
810:            configs.append(
811:                ParserConfig(
812:                    pygram.python_grammar_no_print_statement_no_exec_statement,
813:                    TokenizerConfig(async_is_reserved_keyword=False),
814:                )
815:            )
816:        # At least one of the above branches must have been taken, because every Python
817:        # version has exactly one of the two 'ASYNC_IS_*' flags
818:        return configs
819:
820:
821:def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:
822:    """Given a string with source, return the lib2to3 Node."""
823:    if src_txt[-1:] != "\n":
824:        src_txt += "\n"
825:
826:    for parser_config in get_parser_configs(set(target_versions)):
827:        drv = driver.Driver(
828:            parser_config.grammar,
829:            pytree.convert,
830:            tokenizer_config=parser_config.tokenizer_config,
831:        )
832:        try:
833:            result = drv.parse_string(src_txt, True)
834:            break
835:
836:        except ParseError as pe:
837:            lineno, column = pe.context[1]
838:            lines = src_txt.splitlines()
839:            try:
840:                faulty_line = lines[lineno - 1]
841:            except IndexError:
842:                faulty_line = "<line number missing in source>"
843:            exc = InvalidInput(f"Cannot parse: {lineno}:{column}: {faulty_line}")
844:    else:
845:        raise exc from None
846:
847:    if isinstance(result, Leaf):
848:        result = Node(syms.file_input, [result])
849:    return result
850:
851:
852:def lib2to3_unparse(node: Node) -> str:
853:    """Given a lib2to3 node, return its string representation."""
854:    code = str(node)
855:    return code
856:
857:
858:T = TypeVar("T")
859:
860:
861:class Visitor(Generic[T]):
862:    """Basic lib2to3 visitor that yields things of type `T` on `visit()`."""
863:
864:    def visit(self, node: LN) -> Iterator[T]:
865:        """Main method to visit `node` and its children.
866:
867:        It tries to find a `visit_*()` method for the given `node.type`, like
868:        `visit_simple_stmt` for Node objects or `visit_INDENT` for Leaf objects.
869:        If no dedicated `visit_*()` method is found, chooses `visit_default()`

Calling tool: grepTool
def supports_feature\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:206:def supports_feature(target_versions: Set[TargetVersion], feature: Feature) -> bool:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/black.py:194:def supports_feature(target_versions: Set[TargetVersion], feature: Feature) -> bool:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
180:
181:
182:@dataclass
183:class FileMode:
184:    target_versions: Set[TargetVersion] = Factory(set)
185:    line_length: int = DEFAULT_LINE_LENGTH
186:    string_normalization: bool = True
187:    is_pyi: bool = False
188:
189:    def get_cache_key(self) -> str:
190:        if self.target_versions:
191:            version_str = ",".join(
192:                str(version.value)
193:                for version in sorted(self.target_versions, key=lambda v: v.value)
194:            )
195:        else:
196:            version_str = "-"
197:        parts = [
198:            version_str,
199:            str(self.line_length),
200:            str(int(self.string_normalization)),
201:            str(int(self.is_pyi)),
202:        ]
203:        return ".".join(parts)
204:
205:
206:def supports_feature(target_versions: Set[TargetVersion], feature: Feature) -> bool:
207:    return all(feature in VERSION_TO_FEATURES[version] for version in target_versions)
208:
209:
210:def read_pyproject_toml(
211:    ctx: click.Context, param: click.Parameter, value: Union[str, int, bool, None]
212:) -> Optional[str]:
213:    """Inject Black configuration from "pyproject.toml" into defaults in `ctx`.
214:
215:    Returns the path to a successfully found and read configuration file, None
216:    otherwise.
217:    """
218:    assert not isinstance(value, (int, bool)), "Invalid parameter type passed"
219:    if not value:
220:        root = find_project_root(ctx.params.get("src", ()))
221:        path = root / "pyproject.toml"
222:        if path.is_file():
223:            value = str(path)
224:        else:
225:            return None
226:
227:    try:
228:        pyproject_toml = toml.load(value)
229:        config = pyproject_toml.get("tool", {}).get("black", {})

Calling tool: grepTool
from attr import dataclass
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:34:from attr import dataclass
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi:5:from attr import dataclass
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:40:from attr import dataclass, evolve, Factory
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/black.py:40:from attr import dataclass, evolve, Factory

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
1:import asyncio
2:from asyncio.base_events import BaseEventLoop
3:from concurrent.futures import Executor, ProcessPoolExecutor
4:from datetime import datetime
5:from enum import Enum
6:from functools import lru_cache, partial, wraps
7:import io
8:import itertools
9:import logging
10:from multiprocessing import Manager, freeze_support
11:import os
12:from pathlib import Path
13:import pickle
14:import re
15:import signal
16:import sys
17:import tempfile
18:import tokenize
19:from typing import (
20:    Any,
21:    Callable,
22:    Collection,
23:    Dict,
24:    Generator,
25:    Generic,
26:    Iterable,
27:    Iterator,
28:    List,
29:    Optional,
30:    Pattern,
31:    Sequence,
32:    Set,
33:    Tuple,
34:    TypeVar,
35:    Union,
36:    cast,
37:)
38:
39:from appdirs import user_cache_dir
40:from attr import dataclass, evolve, Factory
41:import click
42:import toml
43:from typed_ast import ast3, ast27
44:
45:# lib2to3 fork
46:from blib2to3.pytree import Node, Leaf, type_repr
47:from blib2to3 import pygram, pytree
48:from blib2to3.pgen2 import driver, token
49:from blib2to3.pgen2.grammar import Grammar
50:from blib2to3.pgen2.parse import ParseError
51:from blib2to3.pgen2.tokenize import TokenizerConfig
52:
53:
54:__version__ = "19.3b0"
55:DEFAULT_LINE_LENGTH = 88
56:DEFAULT_EXCLUDES = (
57:    r"/(\.eggs|\.git|\.hg|\.mypy_cache|\.nox|\.tox|\.venv|_build|buck-out|build|dist)/"
58:)
59:DEFAULT_INCLUDES = r"\.pyi?$"
60:CACHE_DIR = Path(user_cache_dir("black", version=__version__))
61:
62:
63:# types
64:FileContent = str
65:Encoding = str
66:NewLine = str
67:Depth = int
68:NodeType = int
69:LeafID = int
70:Priority = int
71:Index = int
72:LN = Union[Leaf, Node]
73:SplitFunc = Callable[["Line", Collection["Feature"]], Iterator["Line"]]
74:Timestamp = float
75:FileSize = int
76:CacheInfo = Tuple[Timestamp, FileSize]
77:Cache = Dict[Path, CacheInfo]
78:out = partial(click.secho, bold=True, err=True)
79:err = partial(click.secho, fg="red", err=True)
80:

Calling tool: grepTool
class Driver\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py:30:class Driver(object):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/driver.py:30:class Driver(object):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Modifications:
5:# Copyright 2006 Google, Inc. All Rights Reserved.
6:# Licensed to PSF under a Contributor Agreement.
7:
8:"""Parser driver.
9:
10:This provides a high-level interface to parse a file into a syntax tree.
11:
12:"""
13:
14:__author__ = "Guido van Rossum <guido@python.org>"
15:
16:__all__ = ["Driver", "load_grammar"]
17:
18:# Python imports
19:import codecs
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(
33:        self,
34:        grammar,
35:        convert=None,
36:        logger=None,
37:        tokenizer_config=tokenize.TokenizerConfig(),
38:    ):
39:        self.grammar = grammar
40:        if logger is None:
41:            logger = logging.getLogger(__name__)
42:        self.logger = logger
43:        self.convert = convert
44:        self.tokenizer_config = tokenizer_config
45:
46:    def parse_tokens(self, tokens, debug=False):
47:        """Parse a series of tokens and return the syntax tree."""
48:        # XXX Move the prefix computation into a wrapper around tokenize.
49:        p = parse.Parser(self.grammar, self.convert)
50:        p.setup()
51:        lineno = 1
52:        column = 0
53:        indent_columns = []
54:        type = value = start = end = line_text = None
55:        prefix = ""
56:        for quintuple in tokens:
57:            type, value, start, end, line_text = quintuple
58:            if start != (lineno, column):
59:                assert (lineno, column) <= start, ((lineno, column), start)
60:                s_lineno, s_column = start
61:                if lineno < s_lineno:
62:                    prefix += "\n" * (s_lineno - lineno)
63:                    lineno = s_lineno
64:                    column = 0
65:                if column < s_column:
66:                    prefix += line_text[column:s_column]
67:                    column = s_column
68:            if type in (tokenize.COMMENT, tokenize.NL):
69:                prefix += value
70:                lineno, column = end
71:                if value.endswith("\n"):
72:                    lineno += 1
73:                    column = 0
74:                continue
75:            if type == token.OP:
76:                type = grammar.opmap[value]
77:            if debug:
78:                self.logger.debug("%s %r (prefix=%r)",
79:                                  token.tok_name[type], value, prefix)
80:            if type == token.INDENT:
81:                indent_columns.append(len(value))
82:                _prefix = prefix + value
83:                prefix = ""
84:                value = ""
85:            elif type == token.DEDENT:
86:                _indent_col = indent_columns.pop()
87:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
88:            if p.addtoken(type, value, (prefix, start)):
89:                if debug:
90:                    self.logger.debug("Stop.")
91:                break
92:            prefix = ""
93:            if type in {token.INDENT, token.DEDENT}:
94:                prefix = _prefix
95:            lineno, column = end
96:            if value.endswith("\n"):
97:                lineno += 1
98:                column = 0
99:        else:
100:            # We never broke out -- EOF is too soon (how can this happen???)
101:            raise parse.ParseError("incomplete input",
102:                                   type, value, (prefix, start))
103:        return p.rootnode
104:
105:    def parse_stream_raw(self, stream, debug=False):
106:        """Parse a stream and return the syntax tree."""
107:        tokens = tokenize.generate_tokens(stream.readline, config=self.tokenizer_config)
108:        return self.parse_tokens(tokens, debug)
109:
110:    def parse_stream(self, stream, debug=False):
111:        """Parse a stream and return the syntax tree."""
112:        return self.parse_stream_raw(stream, debug)
113:
114:    def parse_file(self, filename, encoding=None, debug=False):
115:        """Parse a file and return the syntax tree."""
116:        with io.open(filename, "r", encoding=encoding) as stream:
117:            return self.parse_stream(stream, debug)
118:
119:    def parse_string(self, text, debug=False):
120:        """Parse a string and return the syntax tree."""
121:        tokens = tokenize.generate_tokens(
122:            io.StringIO(text).readline,
123:            config=self.tokenizer_config,
124:        )
125:        return self.parse_tokens(tokens, debug)
126:
127:    def _partially_consume_prefix(self, prefix, column):
128:        lines = []
129:        current_line = ""
130:        current_column = 0
131:        wait_for_nl = False
132:        for char in prefix:
133:            current_line += char
134:            if wait_for_nl:
135:                if char == '\n':
136:                    if current_line.strip() and current_column < column:
137:                        res = ''.join(lines)
138:                        return res, prefix[len(res):]
139:
140:                    lines.append(current_line)

Calling tool: grepTool
class TokenizerConfig
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:142:class TokenizerConfig:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi:14:class TokenizerConfig:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
120:)
121:
122:endprogs = {"'": re.compile(Single), '"': re.compile(Double),
123:            "'''": single3prog, '"""': double3prog,
124:            **{f"{prefix}'''": single3prog for prefix in _strprefixes},
125:            **{f'{prefix}"""': double3prog for prefix in _strprefixes},
126:            **{prefix: None for prefix in _strprefixes}}
127:
128:triple_quoted = (
129:    {"'''", '"""'} |
130:    {f"{prefix}'''" for prefix in _strprefixes} |
131:    {f'{prefix}"""' for prefix in _strprefixes}
132:)
133:single_quoted = (
134:    {"'", '"'} |
135:    {f"{prefix}'" for prefix in _strprefixes} |
136:    {f'{prefix}"' for prefix in _strprefixes}
137:)
138:
139:tabsize = 8
140:
141:@dataclass(frozen=True)
142:class TokenizerConfig:
143:    async_is_reserved_keyword: bool = False
144:
145:class TokenError(Exception): pass
146:
147:class StopTokenizing(Exception): pass
148:
149:def printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line): # for testing
150:    (srow, scol) = xxx_todo_changeme
151:    (erow, ecol) = xxx_todo_changeme1
152:    print("%d,%d-%d,%d:\t%s\t%s" % \
153:        (srow, scol, erow, ecol, tok_name[type], repr(token)))
154:
155:def tokenize(readline, tokeneater=printtoken):
156:    """
157:    The tokenize() function accepts two parameters: one representing the
158:    input stream, and one providing an output mechanism for tokenize().
159:
160:    The first parameter, readline, must be a callable object which provides
161:    the same interface as the readline() method of built-in file objects.
162:    Each call to the function should return one line of input as a string.
163:
164:    The second parameter, tokeneater, must also be a callable object. It is
165:    called once for each token, with five arguments, corresponding to the
166:    tuples generated by generate_tokens().
167:    """
168:    try:
169:        tokenize_loop(readline, tokeneater)
170:    except StopTokenizing:
171:        pass
172:
173:# backwards compatible interface
174:def tokenize_loop(readline, tokeneater):
175:    for token_info in generate_tokens(readline):
176:        tokeneater(*token_info)
177:
178:class Untokenizer:
179:
180:    def __init__(self):
181:        self.tokens = []
182:        self.prev_row = 1
183:        self.prev_col = 0
184:
185:    def add_whitespace(self, start):
186:        row, col = start
187:        assert row <= self.prev_row
188:        col_offset = col - self.prev_col
189:        if col_offset:
190:            self.tokens.append(" " * col_offset)
191:
192:    def untokenize(self, iterable):
193:        for t in iterable:
194:            if len(t) == 2:
195:                self.compat(t, iterable)
196:                break
197:            tok_type, token, start, end, line = t
198:            self.add_whitespace(start)
199:            self.tokens.append(token)
200:            self.prev_row, self.prev_col = end
201:            if tok_type in (NEWLINE, NL):
202:                self.prev_row += 1
203:                self.prev_col = 0
204:        return "".join(self.tokens)
205:
206:    def compat(self, token, iterable):
207:        startline = False
208:        indents = []
209:        toks_append = self.tokens.append
210:        toknum, tokval = token
211:        if toknum in (NAME, NUMBER):
212:            tokval += ' '
213:        if toknum in (NEWLINE, NL):
214:            startline = True
215:        for tok in iterable:
216:            toknum, tokval = tok[:2]
217:
218:            if toknum in (NAME, NUMBER, ASYNC, AWAIT):
219:                tokval += ' '

Calling tool: grepTool
def generate_tokens\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi:32:def generate_tokens(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.py:337:def generate_tokens(readline):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.pyi:28:def generate_tokens(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/flake8/processor.py:259:    def generate_tokens(self):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/coverage/phystokens.py:142:    def generate_tokens(self, text):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pycodestyle.py:2026:    def generate_tokens(self):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/2/tokenize.pyi:117:def generate_tokens(readline: Callable[[], str]) -> Generator[_TokenType, None, None]: ...
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/3/tokenize.pyi:46:def generate_tokens(readline: Callable[[], str]) -> Generator[TokenInfo, None, None]: ...  # undocumented
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/2and3/lib2to3/pgen2/tokenize.pyi:28:def generate_tokens(

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
330:
331:    Round-trip invariant for limited intput:
332:        # Output text will tokenize the back to the input
333:        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
334:        newcode = untokenize(t1)
335:        readline = iter(newcode.splitlines(1)).next
336:        t2 = [tok[:2] for tokin generate_tokens(readline)]
337:        assert t1 == t2
338:    """
339:    ut = Untokenizer()
340:    return ut.untokenize(iterable)
341:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
343:    """
344:    The generate_tokens() generator requires one argument, readline, which
345:    must be a callable object which provides the same interface as the
346:    readline() method of built-in file objects. Each call to the function
347:    should return one line of input as a string.  Alternately, readline
348:    can be a callable function terminating with StopIteration:
349:        readline = open(myfile).next    # Example of alternate readline
350:
351:    The generator produces 5-tuples with these members: the token type; the
352:    token string; a 2-tuple (srow, scol) of ints specifying the row and
353:    column where the token begins in the source; a 2-tuple (erow, ecol) of
354:    ints specifying the row and column where the token ends in the source;
355:    and the line on which the token was found. The line passed is the
356:    logical line; continuation lines are included.
357:    """
358:    lnum = parenlev = continued = 0
359:    numchars = '0123456789'
360:    contstr, needcont = '', 0
361:    contline = None
362:    indents = [0]
363:
364:    # If we know we're parsing 3.7+, we can unconditionally parse `async` and
365:    # `await` as keywords.
366:    async_is_reserved_keyword = config.async_is_reserved_keyword
367:    # 'stashed' and 'async_*' are used for async/await parsing
368:    stashed = None
369:    async_def = False
370:    async_def_indent = 0
371:    async_def_nl = False
372:
373:    while 1:                                   # loop over lines in stream
374:        try:
375:            line = readline()
376:        except StopIteration:
377:            line = ''
378:        lnum = lnum + 1
379:        pos, max = 0, len(line)
380:
381:        if contstr:                            # continued string
382:            if not line:
383:                raise TokenError("EOF in multi-line string", strstart)
384:            endmatch = endprog.match(line)
385:            if endmatch:
386:                pos = end = endmatch.end(0)
387:                yield (STRING, contstr + line[:end],
388:                       strstart, (lnum, end), contline + line)
389:                contstr, needcont = '', 0
390:                contline = None
391:            elif needcont and line[-2:] != '\\\n' and line[-3:] != '\\\r\n':
392:                yield (ERRORTOKEN, contstr + line,
393:                           strstart, (lnum, len(line)), contline)
394:                contstr = ''
395:                contline = None
396:                continue
397:            else:
398:                contstr = contstr + line
399:                contline = contline + line
400:                continue
401:
402:        elif parenlev == 0 and not continued:  # new statement
403:            if not line: break
404:            column = 0
405:            while pos < max:                   # measure leading whitespace
406:                if line[pos] == ' ': column = column + 1
407:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize
408:                elif line[pos] == '\f': column = 0
409:                else: break
410:                pos = pos + 1
411:            if pos == max: break
412:
413:            if stashed:
414:                yield stashed
415:                stashed = None
416:
417:            if line[pos] in '\r\n':            # skip blank lines
418:                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)
419:                continue
420:
421:            if line[pos] == '#':               # skip comments
422:                comment_token = line[pos:].rstrip('\r\n')
423:                nl_pos = pos + len(comment_token)
424:                yield (COMMENT, comment_token,
425:                        (lnum, pos), (lnum, pos + len(comment_token)), line)
426:                yield (NL, line[nl_pos:],
427:                        (lnum, nl_pos), (lnum, len(line)), line)
428:                continue
429:
430:            if column > indents[-1]:           # count indents
431:                indents.append(column)
432:                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)
433:
434:            while column < indents[-1]:        # count dedents
435:                if column not in indents:
436:                    raise IndentationError(
437:                        "unindent does not match any outer indentation level",
438:                        ("<tokenize>", lnum, pos, line))
439:                indents = indents[:-1]
440:
441:                if async_def and async_def_indent >= indents[-1]:
442:                    async_def = False
443:                    async_def_nl = False
444:                    async_def_indent = 0
445:
446:                yield (DEDENT, '', (lnum, pos), (lnum, pos), line)
447:
448:            if async_def and async_def_nl and async_def_indent >= indents[-1]:
449:                async_def = False
450:                async_def_nl = False
451:                async_def_indent = 0
452:
453:        else:                                  # continued statement
454:            if not line:
455:                raise TokenError("EOF in multi-line statement", (lnum, 0))
456:            continued = 0
457:
458:        while pos < max:
459:            pseudomatch = pseudoprog.match(line, pos)
460:            if pseudomatch:                                # scan for tokens
461:                start, end = pseudomatch.span(1)
462:                spos, epos, pos = (lnum, start), (lnum, end), end
463:                token, initial = line[start:end], line[start]
464:
465:                if initial in numchars or \
466:                   (initial == '.' and token != '.'):      # ordinary number
467:                    yield (NUMBER, token, spos, epos, line)
468:                elif initial in '\r\n':
469:                    newline = NEWLINE
470:                    if parenlev > 0:
471:                        newline = NL
472:                    elif async_def:
473:                        async_def_nl = True
474:                    if stashed:
475:                        yield stashed
476:                        stashed = None
477:                    yield (newline, token, spos, epos, line)
478:
479:                elif initial == '#':
480:                    assert not token.endswith("\n")
481:                    if stashed:
482:                        yield stashed
483:                        stashed = None
484:                    yield (COMMENT, token, spos, epos, line)
485:                elif token in triple_quoted:
486:                    endprog = endprogs[token]
487:                    endmatch = endprog.match(line, pos)
488:                    if endmatch:                           # all on one line
489:                        pos = endmatch.end(0)
490:                        token = line[start:pos]
491:                        if stashed:
492:                            yield stashed
493:                            stashed = None
494:                        yield (STRING, token, spos, (lnum, pos), line)
495:                    else:
496:                        strstart = (lnum, start)           # multiple lines
497:                        contstr = line[start:]
498:                        contline = line
499:                        break
500:                elif initial in single_quoted or \
501:                    token[:2] in single_quoted or \
502:                    token[:3] in single_quoted:
503:                    if token[-1] == '\n':                  # continued string
504:                        strstart = (lnum, start)
505:                        endprog = (endprogs[initial] or endprogs[token[1]] or
506:                                   endprogs[token[2]])
507:                        contstr, needcont = line[start:], 1
508:                        contline = line
509:                        break
510:                    else:                                  # ordinary string
511:                        if stashed:
512:                            yield stashed
513:                            stashed = None
514:                        yield (STRING, token, spos, epos, line)
515:                elif initial.isidentifier():               # ordinary name
516:                    if token in ('async', 'await'):
517:                        if async_is_reserved_keyword or async_def:
518:                            yield (ASYNC if token == 'async' else AWAIT,
519:                                   token, spos, epos, line)
520:                            continue
521:
522:                    tok = (NAME, token, spos, epos, line)
523:                    if token == 'async' and not stashed:
524:                        stashed = tok
525:                        continue
526:
527:                    if token in ('def', 'for'):
528:                        if (stashed
529:                                and stashed[0] == NAME
530:                                and stashed[1] == 'async'):
531:
532:                            if token == 'def':
533:                                async_def = True
534:                                async_def_indent = indents[-1]
535:
536:                            yield (ASYNC, stashed[1],
537:                                   stashed[2], stashed[3],
538:                                   stashed[4])
539:                            stashed = None
540:
541:                    if stashed:
542:                        yield stashed
543:                        stashed = None
544:
545:                    yield tok
546:                elif initial == '\\':                      # continued stmt
547:                    # This yield is new; needed for better idempotency:
548:                    if stashed:
549:                        yield stashed
550:                        stashed = None
551:                    yield (NL, token, spos, (lnum, pos), line)
552:                    continued = 1
553:                else:
554:                    if initial in '([{': parenlev = parenlev + 1
555:                    elif initial in ')]}': parenlev = parenlev - 1
556:                    if stashed:
557:                        yield stashed
558:                        stashed = None
559:                    yield (OP, token, spos, epos, line)
560:            else:
561:                yield (ERRORTOKEN, line[pos],
562:                           (lnum, pos), (lnum, pos+1), line)
563:                pos = pos + 1
564:
565:    if stashed:
566:        yield stashed
567:        stashed = None
568:
569:    for indent in indents[1:]:                 # pop remaining indent levels
570:        yield (DEDENT, '', (lnum, 0), (lnum, 0), '')
571:    yield (ENDMARKER, '', (lnum, 0), (lnum, 0), '')
572:
573:if __name__ == '__main__':                     # testing
574:    import sys
575:    if len(sys.argv) > 1: tokenize(open(sys.argv[1]).readline)
576:    else: tokenize(sys.stdin.readline)

Calling tool: grepTool
class Feature\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:133:class Feature(Enum):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/black.py:132:class Feature(Enum):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
120:    PY34 = 4
121:    PY35 = 5
122:    PY36 = 6
123:    PY37 = 7
124:    PY38 = 8
125:
126:    def is_python2(self) -> bool:
127:        return self is TargetVersion.PY27
128:
129:
130:PY36_VERSIONS = {TargetVersion.PY36, TargetVersion.PY37, TargetVersion.PY38}
131:
132:
133:class Feature(Enum):
134:    # All string literals are unicode
135:    UNICODE_LITERALS = 1
136:    F_STRINGS = 2
137:    NUMERIC_UNDERSCORES = 3
138:    TRAILING_COMMA_IN_CALL = 4
139:    TRAILING_COMMA_IN_DEF = 5
140:    # The following two feature-flags are mutually exclusive, and exactly one should be
141:    # set for every version of python.
142:    ASYNC_IS_VALID_IDENTIFIER = 6
143:    ASYNC_IS_RESERVED_KEYWORD = 7
144:
145:
146:VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {
147:    TargetVersion.PY27: {Feature.ASYNC_IS_VALID_IDENTIFIER},
148:    TargetVersion.PY33: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
149:    TargetVersion.PY34: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
150:    TargetVersion.PY35: {
151:        Feature.UNICODE_LITERALS,
152:        Feature.TRAILING_COMMA_IN_CALL,
153:        Feature.ASYNC_IS_VALID_IDENTIFIER,
154:    },
155:    TargetVersion.PY36: {
156:        Feature.UNICODE_LITERALS,
157:        Feature.F_STRINGS,
158:        Feature.NUMERIC_UNDERSCORES,
159:        Feature.TRAILING_COMMA_IN_CALL,
160:        Feature.TRAILING_COMMA_IN_DEF,
161:        Feature.ASYNC_IS_VALID_IDENTIFIER,
162:    },
163:    TargetVersion.PY37: {
164:        Feature.UNICODE_LITERALS,
165:        Feature.F_STRINGS,
166:        Feature.NUMERIC_UNDERSCORES,
167:        Feature.TRAILING_COMMA_IN_CALL,
168:        Feature.TRAILING_COMMA_IN_DEF,
169:        Feature.ASYNC_IS_RESERVED_KEYWORD,
170:    },
171:    TargetVersion.PY38: {
172:        Feature.UNICODE_LITERALS,
173:        Feature.F_STRINGS,
174:        Feature.NUMERIC_UNDERSCORES,
175:        Feature.TRAILING_COMMA_IN_CALL,
176:        Feature.TRAILING_COMMA_IN_DEF,
177:        Feature.ASYNC_IS_RESERVED_KEYWORD,
178:    },
179:}
180:
181:
182:@dataclass
183:class FileMode:
184:    target_versions: Set[TargetVersion] = Factory(set)
185:    line_length: int = DEFAULT_LINE_LENGTH
186:    string_normalization: bool = True
187:    is_pyi: bool = False
188:
189:    def get_cache_key(self) -> str:
190:        if self.target_versions:
191:            version_str = ",".join(
192:                str(version.value)
193:                for version in sorted(self.target_versions, key=lambda v: v.value)
194:            )
195:        else:
196:            version_str = "-"
197:        parts = [
198:            version_str,
199:            str(self.line_length),

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: Verified black.py:get_parser_configs() implementation. For empty targets (lines 770-787) returns four ParserConfigs in required order with tokenizer configs True then False, then Python 2 grammars. For Python 2-only (lines 788-795) returns only two Python 2 grammars without tokenizer overrides. For Python 3 targets (lines 796-818) conditionally adds reserved-keyword then valid-identifier configs based on supports_feature (line 206), preserving reserved-first order. Feature flags mapping ensures mutual exclusivity (lines 146-179).

Todo item 3 of 15 is correct
description: 'Validate get_parser_configs() logic and order for parsing attempts: (a) when target_versions is empty, returns four ParserConfig entries in this order: Python 3 grammar with TokenizerConfig(async_is_reserved_keyword=True), Python 3 grammar with TokenizerConfig(async_is_reserved_keyword=False), Python 2 no_print grammar, Python 2 grammar; (b) when all target_versions are Python 2, returns only the two Python 2 grammars (no TokenizerConfig overrides); (c) when any Python 3 target_versions, returns Python 3 ParserConfigs depending on supports_feature: include reserved-keyword config if ASYNC_IS_VALID_IDENTIFIER is not supported by all versions; include valid-identifier config if ASYNC_IS_RESERVED_KEYWORD is not supported by all versions, with reserved-first order when both are added.'

Validating todo item 4 of 15
description: 'Check supports_feature(target_versions, feature) returns True only if all versions in the set include the feature (i.e., uses all(...)). This ensures mixed targets (e.g., PY36+PY37) will add both parser configs.'

Calling tool: grepTool
def supports_feature\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:206:def supports_feature(target_versions: Set[TargetVersion], feature: Feature) -> bool:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/black.py:194:def supports_feature(target_versions: Set[TargetVersion], feature: Feature) -> bool:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
170:    },
171:    TargetVersion.PY38: {
172:        Feature.UNICODE_LITERALS,
173:        Feature.F_STRINGS,
174:        Feature.NUMERIC_UNDERSCORES,
175:        Feature.TRAILING_COMMA_IN_CALL,
176:        Feature.TRAILING_COMMA_IN_DEF,
177:        Feature.ASYNC_IS_RESERVED_KEYWORD,
178:    },
179:}
180:
181:
182:@dataclass
183:class FileMode:
184:    target_versions: Set[TargetVersion] = Factory(set)
185:    line_length: int = DEFAULT_LINE_LENGTH
186:    string_normalization: bool = True
187:    is_pyi: bool = False
188:
189:    def get_cache_key(self) -> str:
190:        if self.target_versions:
191:            version_str = ",".join(
192:                str(version.value)
193:                for version in sorted(self.target_versions, key=lambda v: v.value)
194:            )
195:        else:
196:            version_str = "-"
197:        parts = [
198:            version_str,
199:            str(self.line_length),
200:            str(int(self.string_normalization)),
201:            str(int(self.is_pyi)),
202:        ]
203:        return ".".join(parts)
204:
205:
206:def supports_feature(target_versions: Set[TargetVersion], feature: Feature) -> bool:
207:    return all(feature in VERSION_TO_FEATURES[version] for version in target_versions)
208:
209:
210:def read_pyproject_toml(
211:    ctx: click.Context, param: click.Parameter, value: Union[str, int, bool, None]
212:) -> Optional[str]:
213:    """Inject Black configuration from "pyproject.toml" into defaults in `ctx`.
214:
215:    Returns the path to a successfully found and read configuration file, None
216:    otherwise.
217:    """
218:    assert not isinstance(value, (int, bool)), "Invalid parameter type passed"
219:    if not value:
220:        root = find_project_root(ctx.params.get("src", ()))
221:        path = root / "pyproject.toml"
222:        if path.is_file():
223:            value = str(path)
224:        else:
225:            return None
226:
227:    try:
228:        pyproject_toml = toml.load(value)
229:        config = pyproject_toml.get("tool", {}).get("black", {})
230:    except (toml.TomlDecodeError, OSError) as e:
231:        raise click.FileError(
232:            filename=value, hint=f"Error reading configuration file: {e}"
233:        )
234:
235:    if not config:
236:        return None
237:
238:    if ctx.default_map is None:
239:        ctx.default_map = {}
240:    ctx.default_map.update(  # type: ignore  # bad types in .pyi
241:        {k.replace("--", "").replace("-", "_"): v for k, v in config.items()}
242:    )
243:    return value
244:
245:
246:@click.command(context_settings=dict(help_option_names=["-h", "--help"]))
247:@click.option("-c", "--code", type=str, help="Format the code passed in as a string.")
248:@click.option(
249:    "-l",

Calling tool: grepTool
def get_parser_configs\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
748:
749:    `newline` is either CRLF or LF but `decoded_contents` is decoded with
750:    universal newlines (i.e. only contains LF).
751:    """
752:    srcbuf = io.BytesIO(src)
753:    encoding, lines = tokenize.detect_encoding(srcbuf.readline)
754:    if not lines:
755:        return "", encoding, "\n"
756:
757:    newline = "\r\n" if b"\r\n" == lines[0][-2:] else "\n"
758:    srcbuf.seek(0)
759:    with io.TextIOWrapper(srcbuf, encoding) as tiow:
760:        return tiow.read(), encoding, newline
761:
762:
763:@dataclass(frozen=True)
764:class ParserConfig:
765:    grammar: Grammar
766:    tokenizer_config: TokenizerConfig = TokenizerConfig()
767:
768:
769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:
770:    if not target_versions:
771:        # No target_version specified, so try all grammars.
772:        return [
773:            # Python 3.7+
774:            ParserConfig(
775:                pygram.python_grammar_no_print_statement_no_exec_statement,
776:                TokenizerConfig(async_is_reserved_keyword=True),
777:            ),
778:            # Python 3.0-3.6
779:            ParserConfig(
780:                pygram.python_grammar_no_print_statement_no_exec_statement,
781:                TokenizerConfig(async_is_reserved_keyword=False),
782:            ),
783:            # Python 2.7 with future print_function import
784:            ParserConfig(pygram.python_grammar_no_print_statement),
785:            # Python 2.7
786:            ParserConfig(pygram.python_grammar),
787:        ]
788:    elif all(version.is_python2() for version in target_versions):
789:        # Python 2-only code, so try Python 2 grammars.
790:        return [
791:            # Python 2.7 with future print_function import
792:            ParserConfig(pygram.python_grammar_no_print_statement),
793:            # Python 2.7
794:            ParserConfig(pygram.python_grammar),
795:        ]
796:    else:
797:        # Python 3-compatible code, so only try Python 3 grammar.
798:        configs = []
799:        # If we have to parse both, try to parse async as a keyword first
800:        if not supports_feature(target_versions, Feature.ASYNC_IS_VALID_IDENTIFIER):
801:            # Python 3.7+
802:            configs.append(
803:                ParserConfig(
804:                    pygram.python_grammar_no_print_statement_no_exec_statement,
805:                    TokenizerConfig(async_is_reserved_keyword=True),
806:                )
807:            )
808:        if not supports_feature(target_versions, Feature.ASYNC_IS_RESERVED_KEYWORD):
809:            # Python 3.0-3.6
810:            configs.append(
811:                ParserConfig(
812:                    pygram.python_grammar_no_print_statement_no_exec_statement,
813:                    TokenizerConfig(async_is_reserved_keyword=False),
814:                )
815:            )
816:        # At least one of the above branches must have been taken, because every Python
817:        # version has exactly one of the two 'ASYNC_IS_*' flags
818:        return configs
819:
820:
821:def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:
822:    """Given a string with source, return the lib2to3 Node."""
823:    if src_txt[-1:] != "\n":
824:        src_txt += "\n"
825:
826:    for parser_config in get_parser_configs(set(target_versions)):
827:        drv = driver.Driver(
828:            parser_config.grammar,
829:            pytree.convert,
830:            tokenizer_config=parser_config.tokenizer_config,
831:        )
832:        try:
833:            result = drv.parse_string(src_txt, True)
834:            break
835:
836:        except ParseError as pe:
837:            lineno, column = pe.context[1]
838:            lines = src_txt.splitlines()
839:            try:
840:                faulty_line = lines[lineno - 1]
841:            except IndexError:
842:                faulty_line = "<line number missing in source>"
843:            exc = InvalidInput(f"Cannot parse: {lineno}:{column}: {faulty_line}")
844:    else:
845:        raise exc from None
846:
847:    if isinstance(result, Leaf):
848:        result = Node(syms.file_input, [result])
849:    return result
850:
851:
852:def lib2to3_unparse(node: Node) -> str:
853:    """Given a lib2to3 node, return its string representation."""
854:    code = str(node)
855:    return code
856:
857:
858:T = TypeVar("T")
859:
860:
861:class Visitor(Generic[T]):
862:    """Basic lib2to3 visitor that yields things of type `T` on `visit()`."""
863:
864:    def visit(self, node: LN) -> Iterator[T]:
865:        """Main method to visit `node` and its children.
866:
867:        It tries to find a `visit_*()` method for the given `node.type`, like

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/black.py
160:    TargetVersion.PY38: {
161:        Feature.UNICODE_LITERALS,
162:        Feature.F_STRINGS,
163:        Feature.NUMERIC_UNDERSCORES,
164:        Feature.TRAILING_COMMA_IN_CALL,
165:        Feature.TRAILING_COMMA_IN_DEF,
166:    },
167:}
168:
169:
170:@dataclass
171:class FileMode:
172:    target_versions: Set[TargetVersion] = Factory(set)
173:    line_length: int = DEFAULT_LINE_LENGTH
174:    string_normalization: bool = True
175:    is_pyi: bool = False
176:
177:    def get_cache_key(self) -> str:
178:        if self.target_versions:
179:            version_str = ",".join(
180:                str(version.value)
181:                for version in sorted(self.target_versions, key=lambda v: v.value)
182:            )
183:        else:
184:            version_str = "-"
185:        parts = [
186:            version_str,
187:            str(self.line_length),
188:            str(int(self.string_normalization)),
189:            str(int(self.is_pyi)),
190:        ]
191:        return ".".join(parts)
192:
193:
194:def supports_feature(target_versions: Set[TargetVersion], feature: Feature) -> bool:
195:    return all(feature in VERSION_TO_FEATURES[version] for version in target_versions)
196:
197:
198:def read_pyproject_toml(
199:    ctx: click.Context, param: click.Parameter, value: Union[str, int, bool, None]
200:) -> Optional[str]:
201:    """Inject Black configuration from "pyproject.toml" into defaults in `ctx`.
202:
203:    Returns the path to a successfully found and read configuration file, None
204:    otherwise.
205:    """
206:    assert not isinstance(value, (int, bool)), "Invalid parameter type passed"
207:    if not value:
208:        root = find_project_root(ctx.params.get("src", ()))
209:        path = root / "pyproject.toml"
210:        if path.is_file():
211:            value = str(path)
212:        else:
213:            return None
214:
215:    try:
216:        pyproject_toml = toml.load(value)
217:        config = pyproject_toml.get("tool", {}).get("black", {})
218:    except (toml.TomlDecodeError, OSError) as e:
219:        raise click.FileError(
220:            filename=value, hint=f"Error reading configuration file: {e}"
221:        )
222:
223:    if not config:
224:        return None
225:
226:    if ctx.default_map is None:
227:        ctx.default_map = {}
228:    ctx.default_map.update(  # type: ignore  # bad types in .pyi
229:        {k.replace("--", "").replace("-", "_"): v for k, v in config.items()}
230:    )
231:    return value
232:
233:
234:@click.command(context_settings=dict(help_option_names=["-h", "--help"]))
235:@click.option("-c", "--code", type=str, help="Format the code passed in as a string.")
236:@click.option(
237:    "-l",
238:    "--line-length",
239:    type=int,

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
120:    PY34 = 4
121:    PY35 = 5
122:    PY36 = 6
123:    PY37 = 7
124:    PY38 = 8
125:
126:    def is_python2(self) -> bool:
127:        return self is TargetVersion.PY27
128:
129:
130:PY36_VERSIONS = {TargetVersion.PY36, TargetVersion.PY37, TargetVersion.PY38}
131:
132:
133:class Feature(Enum):
134:    # All string literals are unicode
135:    UNICODE_LITERALS = 1
136:    F_STRINGS = 2
137:    NUMERIC_UNDERSCORES = 3
138:    TRAILING_COMMA_IN_CALL = 4
139:    TRAILING_COMMA_IN_DEF = 5
140:    # The following two feature-flags are mutually exclusive, and exactly one should be
141:    # set for every version of python.
142:    ASYNC_IS_VALID_IDENTIFIER = 6
143:    ASYNC_IS_RESERVED_KEYWORD = 7
144:
145:
146:VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {
147:    TargetVersion.PY27: {Feature.ASYNC_IS_VALID_IDENTIFIER},
148:    TargetVersion.PY33: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
149:    TargetVersion.PY34: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
150:    TargetVersion.PY35: {
151:        Feature.UNICODE_LITERALS,
152:        Feature.TRAILING_COMMA_IN_CALL,
153:        Feature.ASYNC_IS_VALID_IDENTIFIER,
154:    },
155:    TargetVersion.PY36: {
156:        Feature.UNICODE_LITERALS,
157:        Feature.F_STRINGS,
158:        Feature.NUMERIC_UNDERSCORES,
159:        Feature.TRAILING_COMMA_IN_CALL,
160:        Feature.TRAILING_COMMA_IN_DEF,
161:        Feature.ASYNC_IS_VALID_IDENTIFIER,
162:    },
163:    TargetVersion.PY37: {
164:        Feature.UNICODE_LITERALS,
165:        Feature.F_STRINGS,
166:        Feature.NUMERIC_UNDERSCORES,
167:        Feature.TRAILING_COMMA_IN_CALL,
168:        Feature.TRAILING_COMMA_IN_DEF,
169:        Feature.ASYNC_IS_RESERVED_KEYWORD,
170:    },
171:    TargetVersion.PY38: {
172:        Feature.UNICODE_LITERALS,
173:        Feature.F_STRINGS,
174:        Feature.NUMERIC_UNDERSCORES,
175:        Feature.TRAILING_COMMA_IN_CALL,
176:        Feature.TRAILING_COMMA_IN_DEF,
177:        Feature.ASYNC_IS_RESERVED_KEYWORD,
178:    },
179:}
180:
181:
182:@dataclass
183:class FileMode:
184:    target_versions: Set[TargetVersion] = Factory(set)
185:    line_length: int = DEFAULT_LINE_LENGTH
186:    string_normalization: bool = True
187:    is_pyi: bool = False
188:
189:    def get_cache_key(self) -> str:

Calling tool: grepTool
def get_parser_configs\(
No files found. Call grepTool again with a more generic pattern.

Calling tool: grepTool
test_async_as_identifier
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/coverage_bugsinpy.txt:1:coverage run -m unittest -q tests.test_black.BlackTestCase.test_async_as_identifier
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:506:    def test_async_as_identifier(self) -> None:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/bugsinpy_run_test.sh:1:python -m unittest -q tests.test_black.BlackTestCase.test_async_as_identifier

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py
470:        black.assert_stable(source, actual, black.FileMode())
471:
472:    @patch("black.dump_to_file", dump_to_stderr)
473:    def test_python2(self) -> None:
474:        source, expected = read_data("python2")
475:        actual = fs(source)
476:        self.assertFormatEqual(expected, actual)
477:        black.assert_equivalent(source, actual)
478:        black.assert_stable(source, actual, black.FileMode())
479:
480:    @patch("black.dump_to_file", dump_to_stderr)
481:    def test_python2_print_function(self) -> None:
482:        source, expected = read_data("python2_print_function")
483:        mode = black.FileMode(target_versions={TargetVersion.PY27})
484:        actual = fs(source, mode=mode)
485:        self.assertFormatEqual(expected, actual)
486:        black.assert_equivalent(source, actual)
487:        black.assert_stable(source, actual, mode)
488:
489:    @patch("black.dump_to_file", dump_to_stderr)
490:    def test_python2_unicode_literals(self) -> None:
491:        source, expected = read_data("python2_unicode_literals")
492:        actual = fs(source)
493:        self.assertFormatEqual(expected, actual)
494:        black.assert_equivalent(source, actual)
495:        black.assert_stable(source, actual, black.FileMode())
496:
497:    @patch("black.dump_to_file", dump_to_stderr)
498:    def test_stub(self) -> None:
499:        mode = black.FileMode(is_pyi=True)
500:        source, expected = read_data("stub.pyi")
501:        actual = fs(source, mode=mode)
502:        self.assertFormatEqual(expected, actual)
503:        black.assert_stable(source, actual, mode)
504:
505:    @patch("black.dump_to_file", dump_to_stderr)
506:    def test_async_as_identifier(self) -> None:
507:        source_path = (THIS_DIR / "data" / "async_as_identifier.py").resolve()
508:        source, expected = read_data("async_as_identifier")
509:        actual = fs(source)
510:        self.assertFormatEqual(expected, actual)
511:        major, minor = sys.version_info[:2]
512:        if major < 3 or (major <= 3 and minor < 7):
513:            black.assert_equivalent(source, actual)
514:        black.assert_stable(source, actual, black.FileMode())
515:        # ensure black can parse this when the target is 3.6
516:        self.invokeBlack([str(source_path), "--target-version", "py36"])
517:        # but not on 3.7, because async/await is no longer an identifier
518:        self.invokeBlack([str(source_path), "--target-version", "py37"], exit_code=123)
519:
520:    @patch("black.dump_to_file", dump_to_stderr)
521:    def test_python37(self) -> None:
522:        source_path = (THIS_DIR / "data" / "python37.py").resolve()
523:        source, expected = read_data("python37")
524:        actual = fs(source)
525:        self.assertFormatEqual(expected, actual)
526:        major, minor = sys.version_info[:2]
527:        if major > 3 or (major == 3 and minor >= 7):
528:            black.assert_equivalent(source, actual)
529:        black.assert_stable(source, actual, black.FileMode())
530:        # ensure black can parse this when the target is 3.7
531:        self.invokeBlack([str(source_path), "--target-version", "py37"])
532:        # but not on 3.6, because we use async as a reserved keyword
533:        self.invokeBlack([str(source_path), "--target-version", "py36"], exit_code=123)
534:
535:    @patch("black.dump_to_file", dump_to_stderr)
536:    def test_fmtonoff(self) -> None:
537:        source, expected = read_data("fmtonoff")
538:        actual = fs(source)
539:        self.assertFormatEqual(expected, actual)
540:        black.assert_equivalent(source, actual)
541:        black.assert_stable(source, actual, black.FileMode())
542:
543:    @patch("black.dump_to_file", dump_to_stderr)
544:    def test_fmtonoff2(self) -> None:
545:        source, expected = read_data("fmtonoff2")
546:        actual = fs(source)
547:        self.assertFormatEqual(expected, actual)
548:        black.assert_equivalent(source, actual)
549:        black.assert_stable(source, actual, black.FileMode())

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: supports_feature is implemented as `all(feature in VERSION_TO_FEATURES[version] for version in target_versions)`, ensuring True only if all versions include the feature. Consequently, in mixed targets (e.g., PY36+PY37), both async tokenizer parser configs are added in get_parser_configs.

Todo item 4 of 15 is correct
description: 'Check supports_feature(target_versions, feature) returns True only if all versions in the set include the feature (i.e., uses all(...)). This ensures mixed targets (e.g., PY36+PY37) will add both parser configs.'

Validating todo item 5 of 15
description: 'Confirm lib2to3_parse() iterates over get_parser_configs() results, constructs driver.Driver with parser_config.grammar and tokenizer_config, and passes tokenizer_config through to parsing (no regression in error handling: InvalidInput raised only after all attempts fail and includes correct location info).'

Calling tool: grepTool
def lib2to3_parse\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:821:def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/black.py:767:def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
730:        for feature in {Feature.TRAILING_COMMA_IN_CALL, Feature.TRAILING_COMMA_IN_DEF}
731:        if supports_feature(versions, feature)
732:    }
733:    for current_line in lines.visit(src_node):
734:        for _ in range(after):
735:            dst_contents += str(empty_line)
736:        before, after = elt.maybe_empty_lines(current_line)
737:        for _ in range(before):
738:            dst_contents += str(empty_line)
739:        for line in split_line(
740:            current_line, line_length=mode.line_length, features=split_line_features
741:        ):
742:            dst_contents += str(line)
743:    return dst_contents
744:
745:
746:def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:
747:    """Return a tuple of (decoded_contents, encoding, newline).
748:
749:    `newline` is either CRLF or LF but `decoded_contents` is decoded with
750:    universal newlines (i.e. only contains LF).
751:    """
752:    srcbuf = io.BytesIO(src)
753:    encoding, lines = tokenize.detect_encoding(srcbuf.readline)
754:    if not lines:
755:        return "", encoding, "\n"
756:
757:    newline = "\r\n" if b"\r\n" == lines[0][-2:] else "\n"
758:    srcbuf.seek(0)
759:    with io.TextIOWrapper(srcbuf, encoding) as tiow:
760:        return tiow.read(), encoding, newline
761:
762:
763:@dataclass(frozen=True)
764:class ParserConfig:
765:    grammar: Grammar
766:    tokenizer_config: TokenizerConfig = TokenizerConfig()
767:
768:
769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:
770:    if not target_versions:
771:        # No target_version specified, so try all grammars.
772:        return [
773:            # Python 3.7+
774:            ParserConfig(
775:                pygram.python_grammar_no_print_statement_no_exec_statement,
776:                TokenizerConfig(async_is_reserved_keyword=True),
777:            ),
778:            # Python 3.0-3.6
779:            ParserConfig(
780:                pygram.python_grammar_no_print_statement_no_exec_statement,
781:                TokenizerConfig(async_is_reserved_keyword=False),
782:            ),
783:            # Python 2.7 with future print_function import
784:            ParserConfig(pygram.python_grammar_no_print_statement),
785:            # Python 2.7
786:            ParserConfig(pygram.python_grammar),
787:        ]
788:    elif all(version.is_python2() for version in target_versions):
789:        # Python 2-only code, so try Python 2 grammars.
790:        return [
791:            # Python 2.7 with future print_function import
792:            ParserConfig(pygram.python_grammar_no_print_statement),
793:            # Python 2.7
794:            ParserConfig(pygram.python_grammar),
795:        ]
796:    else:
797:        # Python 3-compatible code, so only try Python 3 grammar.
798:        configs = []
799:        # If we have to parse both, try to parse async as a keyword first
800:        if not supports_feature(target_versions, Feature.ASYNC_IS_VALID_IDENTIFIER):
801:            # Python 3.7+
802:            configs.append(
803:                ParserConfig(
804:                    pygram.python_grammar_no_print_statement_no_exec_statement,
805:                    TokenizerConfig(async_is_reserved_keyword=True),
806:                )
807:            )
808:        if not supports_feature(target_versions, Feature.ASYNC_IS_RESERVED_KEYWORD):
809:            # Python 3.0-3.6
810:            configs.append(
811:                ParserConfig(
812:                    pygram.python_grammar_no_print_statement_no_exec_statement,
813:                    TokenizerConfig(async_is_reserved_keyword=False),
814:                )
815:            )
816:        # At least one of the above branches must have been taken, because every Python
817:        # version has exactly one of the two 'ASYNC_IS_*' flags
818:        return configs
819:
820:
821:def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:
822:    """Given a string with source, return the lib2to3 Node."""
823:    if src_txt[-1:] != "\n":
824:        src_txt += "\n"
825:
826:    for parser_config in get_parser_configs(set(target_versions)):
827:        drv = driver.Driver(
828:            parser_config.grammar,
829:            pytree.convert,
830:            tokenizer_config=parser_config.tokenizer_config,
831:        )
832:        try:
833:            result = drv.parse_string(src_txt, True)
834:            break
835:
836:        except ParseError as pe:
837:            lineno, column = pe.context[1]
838:            lines = src_txt.splitlines()
839:            try:
840:                faulty_line = lines[lineno - 1]
841:            except IndexError:
842:                faulty_line = "<line number missing in source>"
843:            exc = InvalidInput(f"Cannot parse: {lineno}:{column}: {faulty_line}")
844:    else:
845:        raise exc from None
846:
847:    if isinstance(result, Leaf):
848:        result = Node(syms.file_input, [result])
849:    return result
850:
851:
852:def lib2to3_unparse(node: Node) -> str:
853:    """Given a lib2to3 node, return its string representation."""
854:    code = str(node)
855:    return code
856:
857:
858:T = TypeVar("T")
859:
860:
861:class Visitor(Generic[T]):
862:    """Basic lib2to3 visitor that yields things of type `T` on `visit()`."""
863:
864:    def visit(self, node: LN) -> Iterator[T]:
865:        """Main method to visit `node` and its children.
866:
867:        It tries to find a `visit_*()` method for the given `node.type`, like
868:        `visit_simple_stmt` for Node objects or `visit_INDENT` for Leaf objects.
869:        If no dedicated `visit_*()` method is found, chooses `visit_default()`

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Modifications:
5:# Copyright 2006 Google, Inc. All Rights Reserved.
6:# Licensed to PSF under a Contributor Agreement.
7:
8:"""Parser driver.
9:
10:This provides a high-level interface to parse a file into a syntax tree.
11:
12:"""
13:
14:__author__ = "Guido van Rossum <guido@python.org>"
15:
16:__all__ = ["Driver", "load_grammar"]
17:
18:# Python imports
19:import codecs
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(
33:        self,
34:        grammar,
35:        convert=None,
36:        logger=None,
37:        tokenizer_config=tokenize.TokenizerConfig(),
38:    ):
39:        self.grammar = grammar
40:        if logger is None:
41:            logger = logging.getLogger(__name__)
42:        self.logger = logger
43:        self.convert = convert
44:        self.tokenizer_config = tokenizer_config
45:
46:    def parse_tokens(self, tokens, debug=False):
47:        """Parse a series of tokens and return the syntax tree."""
48:        # XXX Move the prefix computation into a wrapper around tokenize.
49:        p = parse.Parser(self.grammar, self.convert)
50:        p.setup()
51:        lineno = 1
52:        column = 0
53:        indent_columns = []
54:        type = value = start = end = line_text = None
55:        prefix = ""
56:        for quintuple in tokens:
57:            type, value, start, end, line_text = quintuple
58:            if start != (lineno, column):
59:                assert (lineno, column) <= start, ((lineno, column), start)
60:                s_lineno, s_column = start
61:                if lineno < s_lineno:
62:                    prefix += "\n" * (s_lineno - lineno)
63:                    lineno = s_lineno
64:                    column = 0
65:                if column < s_column:
66:                    prefix += line_text[column:s_column]
67:                    column = s_column
68:            if type in (tokenize.COMMENT, tokenize.NL):
69:                prefix += value
70:                lineno, column = end
71:                if value.endswith("\n"):
72:                    lineno += 1
73:                    column = 0
74:                continue
75:            if type == token.OP:
76:                type = grammar.opmap[value]
77:            if debug:
78:                self.logger.debug("%s %r (prefix=%r)",
79:                                  token.tok_name[type], value, prefix)
80:            if type == token.INDENT:
81:                indent_columns.append(len(value))
82:                _prefix = prefix + value
83:                prefix = ""
84:                value = ""
85:            elif type == token.DEDENT:
86:                _indent_col = indent_columns.pop()
87:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
88:            if p.addtoken(type, value, (prefix, start)):
89:                if debug:
90:                    self.logger.debug("Stop.")
91:                break
92:            prefix = ""
93:            if type in {token.INDENT, token.DEDENT}:
94:                prefix = _prefix
95:            lineno, column = end
96:            if value.endswith("\n"):
97:                lineno += 1
98:                column = 0
99:        else:
100:            # We never broke out -- EOF is too soon (how can this happen???)
101:            raise parse.ParseError("incomplete input",
102:                                   type, value, (prefix, start))
103:        return p.rootnode
104:
105:    def parse_stream_raw(self, stream, debug=False):
106:        """Parse a stream and return the syntax tree."""
107:        tokens = tokenize.generate_tokens(stream.readline, config=self.tokenizer_config)
108:        return self.parse_tokens(tokens, debug)
109:
110:    def parse_stream(self, stream, debug=False):
111:        """Parse a stream and return the syntax tree."""
112:        return self.parse_stream_raw(stream, debug)
113:
114:    def parse_file(self, filename, encoding=None, debug=False):
115:        """Parse a file and return the syntax tree."""
116:        with io.open(filename, "r", encoding=encoding) as stream:
117:            return self.parse_stream(stream, debug)
118:
119:    def parse_string(self, text, debug=False):
120:        """Parse a string and return the syntax tree."""
121:        tokens = tokenize.generate_tokens(
122:            io.StringIO(text).readline,
123:            config=self.tokenizer_config,
124:        )
125:        return self.parse_tokens(tokens, debug)
126:
127:    def _partially_consume_prefix(self, prefix, column):
128:        lines = []
129:        current_line = ""
130:        current_column = 0
131:        wait_for_nl = False
132:        for char in prefix:
133:            current_line += char
134:            if wait_for_nl:
135:                if char == '\n':
136:                    if current_line.strip() and current_column < column:
137:                        res = ''.join(lines)
138:                        return res, prefix[len(res):]
139:
140:                    lines.append(current_line)
141:                    current_line = ""
142:                    current_column = 0
143:                    wait_for_nl = False
144:            elif char in ' \t':
145:                current_column += 1
146:            elif char == '\n':
147:                # unexpected empty line
148:                current_column = 0
149:            else:
150:                # indent is finished
151:                wait_for_nl = True
152:        return ''.join(lines), current_line
153:
154:
155:def _generate_pickle_name(gt, cache_dir=None):
156:    head, tail = os.path.splitext(gt)
157:    if tail == ".txt":
158:        tail = ""
159:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
160:    if cache_dir:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
1:# Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006 Python Software Foundation.
2:# All rights reserved.
3:
4:"""Tokenization help for Python programs.
5:
6:generate_tokens(readline) is a generator that breaks a stream of
7:text into Python tokens.  It accepts a readline-like method which is called
8:repeatedly to get the next line of input (or "" for EOF).  It generates
9:5-tuples with these members:
10:
11:    the token type (see token.py)
12:    the token (a string)
13:    the starting (row, column) indices of the token (a 2-tuple of ints)
14:    the ending (row, column) indices of the token (a 2-tuple of ints)
15:    the original line (string)
16:
17:It is designed to match the working of the Python tokenizer exactly, except
18:that it produces COMMENT tokens for comments and gives type OP for all
19:operators
20:
21:Older entry points
22:    tokenize_loop(readline, tokeneater)
23:    tokenize(readline, tokeneater=printtoken)
24:are the same, except instead of generating tokens, tokeneater is a callback
25:function to which the 5 fields described above are passed as 5 arguments,
26:each time a new token is found."""
27:
28:__author__ = 'Ka-Ping Yee <ping@lfw.org>'
29:__credits__ = \
30:    'GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro'
31:
32:import re
33:from codecs import BOM_UTF8, lookup
34:from attr import dataclass
35:from blib2to3.pgen2.token import *
36:
37:from . import token
38:__all__ = [x for x in dir(token) if x[0] != '_'] + ["tokenize",
39:           "generate_tokens", "untokenize"]
40:del token
41:
42:try:
43:    bytes
44:except NameError:
45:    # Support bytes type in Python <= 2.5, so 2to3 turns itself into
46:    # valid Python 3 code.
47:    bytes = str
48:
49:def group(*choices): return '(' + '|'.join(choices) + ')'
50:def any(*choices): return group(*choices) + '*'
51:def maybe(*choices): return group(*choices) + '?'
52:def _combinations(*l):
53:    return set(
54:        x + y for x in l for y in l + ("",) if x.casefold() != y.casefold()
55:    )
56:
57:Whitespace = r'[ \f\t]*'
58:Comment = r'#[^\r\n]*'
59:Ignore = Whitespace + any(r'\\\r?\n' + Whitespace) + maybe(Comment)
60:Name = r'\w+'  # this is invalid but it's fine because Name comes after Number in all groups
61:
62:Binnumber = r'0[bB]_?[01]+(?:_[01]+)*'
63:Hexnumber = r'0[xX]_?[\da-fA-F]+(?:_[\da-fA-F]+)*[lL]?'
64:Octnumber = r'0[oO]?_?[0-7]+(?:_[0-7]+)*[lL]?'
65:Decnumber = group(r'[1-9]\d*(?:_\d+)*[lL]?', '0[lL]?')
66:Intnumber = group(Binnumber, Hexnumber, Octnumber, Decnumber)
67:Exponent = r'[eE][-+]?\d+(?:_\d+)*'
68:Pointfloat = group(r'\d+(?:_\d+)*\.(?:\d+(?:_\d+)*)?', r'\.\d+(?:_\d+)*') + maybe(Exponent)
69:Expfloat = r'\d+(?:_\d+)*' + Exponent
70:Floatnumber = group(Pointfloat, Expfloat)
71:Imagnumber = group(r'\d+(?:_\d+)*[jJ]', Floatnumber + r'[jJ]')
72:Number = group(Imagnumber, Floatnumber, Intnumber)
73:
74:# Tail end of ' string.
75:Single = r"[^'\\]*(?:\\.[^'\\]*)*'"
76:# Tail end of " string.
77:Double = r'[^"\\]*(?:\\.[^"\\]*)*"'
78:# Tail end of ''' string.
79:Single3 = r"[^'\\]*(?:(?:\\.|'(?!''))[^'\\]*)*'''"
80:# Tail end of """ string.
81:Double3 = r'[^"\\]*(?:(?:\\.|"(?!""))[^"\\]*)*"""'
82:_litprefix = r"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?"
83:Triple = group(_litprefix + "'''", _litprefix + '"""')
84:# Single-line ' or " string.
85:String = group(_litprefix + r"'[^\n'\\]*(?:\\.[^\n'\\]*)*'",
86:               _litprefix + r'"[^\n"\\]*(?:\\.[^\n"\\]*)*"')
87:
88:# Because of leftmost-then-longest match semantics, be sure to put the
89:# longest operators first (e.g., if = came before ==, == would get
90:# recognized as two instances of =).
91:Operator = group(r"\*\*=?", r">>=?", r"<<=?", r"<>", r"!=",
92:                 r"//=?", r"->",
93:                 r"[+\-*/%&@|^=<>]=?",
94:                 r"~")
95:
96:Bracket = '[][(){}]'
97:Special = group(r'\r?\n', r'[:;.,`@]')
98:Funny = group(Operator, Bracket, Special)
99:
100:PlainToken = group(Number, Funny, String, Name)
101:Token = Ignore + PlainToken
102:
103:# First (or only) line of ' or " string.
104:ContStr = group(_litprefix + r"'[^\n'\\]*(?:\\.[^\n'\\]*)*" +
105:                group("'", r'\\\r?\n'),
106:                _litprefix + r'"[^\n"\\]*(?:\\.[^\n"\\]*)*' +
107:                group('"', r'\\\r?\n'))
108:PseudoExtras = group(r'\\\r?\n', Comment, Triple)
109:PseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)
110:
111:tokenprog = re.compile(Token, re.UNICODE)
112:pseudoprog = re.compile(PseudoToken, re.UNICODE)
113:single3prog = re.compile(Single3)
114:double3prog = re.compile(Double3)
115:
116:_strprefixes = (
117:    _combinations('r', 'R', 'f', 'F') |
118:    _combinations('r', 'R', 'b', 'B') |
119:    {'u', 'U', 'ur', 'uR', 'Ur', 'UR'}
120:)
121:
122:endprogs = {"'": re.compile(Single), '"': re.compile(Double),
123:            "'''": single3prog, '"""': double3prog,
124:            **{f"{prefix}'''": single3prog for prefix in _strprefixes},
125:            **{f'{prefix}"""': double3prog for prefix in _strprefixes},
126:            **{prefix: None for prefix in _strprefixes}}
127:
128:triple_quoted = (
129:    {"'''", '"""'} |
130:    {f"{prefix}'''" for prefix in _strprefixes} |
131:    {f'{prefix}"""' for prefix in _strprefixes}
132:)
133:single_quoted = (
134:    {"'", '"'} |
135:    {f"{prefix}'" for prefix in _strprefixes} |
136:    {f'{prefix}"' for prefix in _strprefixes}
137:)
138:
139:tabsize = 8
140:
141:@dataclass(frozen=True)
142:class TokenizerConfig:
143:    async_is_reserved_keyword: bool = False
144:
145:class TokenError(Exception): pass
146:
147:class StopTokenizing(Exception): pass
148:
149:def printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line): # for testing
150:    (srow, scol) = xxx_todo_changeme
151:    (erow, ecol) = xxx_todo_changeme1
152:    print("%d,%d-%d,%d:\t%s\t%s" % \
153:        (srow, scol, erow, ecol, tok_name[type], repr(token)))
154:
155:def tokenize(readline, tokeneater=printtoken):
156:    """
157:    The tokenize() function accepts two parameters: one representing the
158:    input stream, and one providing an output mechanism for tokenize().
159:
160:    The first parameter, readline, must be a callable object which provides
161:    the same interface as the readline() method of built-in file objects.
162:    Each call to the function should return one line of input as a string.
163:
164:    The second parameter, tokeneater, must also be a callable object. It is
165:    called once for each token, with five arguments, corresponding to the
166:    tuples generated by generate_tokens().
167:    """
168:    try:
169:        tokenize_loop(readline, tokeneater)
170:    except StopTokenizing:
171:        pass
172:
173:# backwards compatible interface
174:def tokenize_loop(readline, tokeneater):
175:    for token_info in generate_tokens(readline):
176:        tokeneater(*token_info)
177:
178:class Untokenizer:
179:
180:    def __init__(self):
181:        self.tokens = []
182:        self.prev_row = 1
183:        self.prev_col = 0
184:
185:    def add_whitespace(self, start):
186:        row, col = start
187:        assert row <= self.prev_row
188:        col_offset = col - self.prev_col
189:        if col_offset:
190:            self.tokens.append(" " * col_offset)
191:
192:    def untokenize(self, iterable):
193:        for t in iterable:
194:            if len(t) == 2:
195:                self.compat(t, iterable)
196:                break
197:            tok_type, token, start, end, line = t
198:            self.add_whitespace(start)
199:            self.tokens.append(token)
200:            self.prev_row, self.prev_col = end

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
200:            self.prev_row, self.prev_col = end
201:            if tok_type in (NEWLINE, NL):
202:                self.prev_row += 1
203:                self.prev_col = 0
204:        return "".join(self.tokens)
205:
206:    def compat(self, token, iterable):
207:        startline = False
208:        indents = []
209:        toks_append = self.tokens.append
210:        toknum, tokval = token
211:        if toknum in (NAME, NUMBER):
212:            tokval += ' '
213:        if toknum in (NEWLINE, NL):
214:            startline = True
215:        for tok in iterable:
216:            toknum, tokval = tok[:2]
217:
218:            if toknum in (NAME, NUMBER, ASYNC, AWAIT):
219:                tokval += ' '
220:
221:            if toknum == INDENT:
222:                indents.append(tokval)
223:                continue
224:            elif toknum == DEDENT:
225:                indents.pop()
226:                continue
227:            elif toknum in (NEWLINE, NL):
228:                startline = True
229:            elif startline and indents:
230:                toks_append(indents[-1])
231:                startline = False
232:            toks_append(tokval)
233:
234:cookie_re = re.compile(r'^[ \t\f]*#.*?coding[:=][ \t]*([-\w.]+)', re.ASCII)
235:blank_re = re.compile(br'^[ \t\f]*(?:[#\r\n]|$)', re.ASCII)
236:
237:def _get_normal_name(orig_enc):
238:    """Imitates get_normal_name in tokenizer.c."""
239:    # Only care about the first 12 characters.
240:    enc = orig_enc[:12].lower().replace("_", "-")
241:    if enc == "utf-8" or enc.startswith("utf-8-"):
242:        return "utf-8"
243:    if enc in ("latin-1", "iso-8859-1", "iso-latin-1") or \
244:       enc.startswith(("latin-1-", "iso-8859-1-", "iso-latin-1-")):
245:        return "iso-8859-1"
246:    return orig_enc
247:
248:def detect_encoding(readline):
249:    """
250:    The detect_encoding() function is used to detect the encoding that should
251:    be used to decode a Python source file. It requires one argument, readline,
252:    in the same way as the tokenize() generator.
253:
254:    It will call readline a maximum of twice, and return the encoding used
255:    (as a string) and a list of any lines (left as bytes) it has read
256:    in.
257:
258:    It detects the encoding from the presence of a utf-8 bom or an encoding
259:    cookie as specified in pep-0263. If both a bom and a cookie are present, but
260:    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid
261:    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,
262:    'utf-8-sig' is returned.
263:
264:    If no encoding is specified, then the default of 'utf-8' will be returned.
265:    """
266:    bom_found = False
267:    encoding = None
268:    default = 'utf-8'
269:    def read_or_stop():
270:        try:
271:            return readline()
272:        except StopIteration:
273:            return bytes()
274:
275:    def find_cookie(line):
276:        try:
277:            line_string = line.decode('ascii')
278:        except UnicodeDecodeError:
279:            return None
280:        match = cookie_re.match(line_string)
281:        if not match:
282:            return None
283:        encoding = _get_normal_name(match.group(1))
284:        try:
285:            codec = lookup(encoding)
286:        except LookupError:
287:            # This behaviour mimics the Python interpreter
288:            raise SyntaxError("unknown encoding: " + encoding)
289:
290:        if bom_found:
291:            if codec.name != 'utf-8':
292:                # This behaviour mimics the Python interpreter
293:                raise SyntaxError('encoding problem: utf-8')
294:            encoding += '-sig'
295:        return encoding
296:
297:    first = read_or_stop()
298:    if first.startswith(BOM_UTF8):
299:        bom_found = True
300:        first = first[3:]
301:        default = 'utf-8-sig'
302:    if not first:
303:        return default, []
304:
305:    encoding = find_cookie(first)
306:    if encoding:
307:        return encoding, [first]
308:    if not blank_re.match(first):
309:        return default, [first]
310:
311:    second = read_or_stop()
312:    if not second:
313:        return default, [first]
314:
315:    encoding = find_cookie(second)
316:    if encoding:
317:        return encoding, [first, second]
318:
319:    return default, [first, second]
320:
321:def untokenize(iterable):
322:    """Transform tokens back into Python source code.
323:
324:    Each element returned by the iterable must be a token sequence
325:    with at least two elements, a token number and token value.  If
326:    only two tokens are passed, the resulting output is poor.
327:
328:    Round-trip invariant for full input:
329:        Untokenized source will match input source exactly
330:
331:    Round-trip invariant for limited intput:
332:        # Output text will tokenize the back to the input
333:        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
334:        newcode = untokenize(t1)
335:        readline = iter(newcode.splitlines(1)).next
336:        t2 = [tok[:2] for tokin generate_tokens(readline)]
337:        assert t1 == t2
338:    """
339:    ut = Untokenizer()
340:    return ut.untokenize(iterable)
341:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
343:    """
344:    The generate_tokens() generator requires one argument, readline, which
345:    must be a callable object which provides the same interface as the
346:    readline() method of built-in file objects. Each call to the function
347:    should return one line of input as a string.  Alternately, readline
348:    can be a callable function terminating with StopIteration:
349:        readline = open(myfile).next    # Example of alternate readline
350:
351:    The generator produces 5-tuples with these members: the token type; the
352:    token string; a 2-tuple (srow, scol) of ints specifying the row and
353:    column where the token begins in the source; a 2-tuple (erow, ecol) of
354:    ints specifying the row and column where the token ends in the source;
355:    and the line on which the token was found. The line passed is the
356:    logical line; continuation lines are included.
357:    """
358:    lnum = parenlev = continued = 0
359:    numchars = '0123456789'
360:    contstr, needcont = '', 0
361:    contline = None
362:    indents = [0]
363:
364:    # If we know we're parsing 3.7+, we can unconditionally parse `async` and
365:    # `await` as keywords.
366:    async_is_reserved_keyword = config.async_is_reserved_keyword
367:    # 'stashed' and 'async_*' are used for async/await parsing
368:    stashed = None
369:    async_def = False
370:    async_def_indent = 0
371:    async_def_nl = False
372:
373:    while 1:                                   # loop over lines in stream
374:        try:
375:            line = readline()
376:        except StopIteration:
377:            line = ''
378:        lnum = lnum + 1
379:        pos, max = 0, len(line)
380:
381:        if contstr:                            # continued string
382:            if not line:
383:                raise TokenError("EOF in multi-line string", strstart)
384:            endmatch = endprog.match(line)
385:            if endmatch:
386:                pos = end = endmatch.end(0)
387:                yield (STRING, contstr + line[:end],
388:                       strstart, (lnum, end), contline + line)
389:                contstr, needcont = '', 0
390:                contline = None
391:            elif needcont and line[-2:] != '\\\n' and line[-3:] != '\\\r\n':
392:                yield (ERRORTOKEN, contstr + line,
393:                           strstart, (lnum, len(line)), contline)
394:                contstr = ''
395:                contline = None
396:                continue
397:            else:
398:                contstr = contstr + line
399:                contline = contline + line
400:                continue
401:
402:        elif parenlev == 0 and not continued:  # new statement
403:            if not line: break
404:            column = 0
405:            while pos < max:                   # measure leading whitespace
406:                if line[pos] == ' ': column = column + 1
407:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize
408:                elif line[pos] == '\f': column = 0
409:                else: break
410:                pos = pos + 1
411:            if pos == max: break
412:
413:            if stashed:
414:                yield stashed
415:                stashed = None
416:
417:            if line[pos] in '\r\n':            # skip blank lines
418:                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)
419:                continue
420:
421:            if line[pos] == '#':               # skip comments
422:                comment_token = line[pos:].rstrip('\r\n')
423:                nl_pos = pos + len(comment_token)
424:                yield (COMMENT, comment_token,
425:                        (lnum, pos), (lnum, pos + len(comment_token)), line)
426:                yield (NL, line[nl_pos:],
427:                        (lnum, nl_pos), (lnum, len(line)), line)
428:                continue
429:
430:            if column > indents[-1]:           # count indents
431:                indents.append(column)
432:                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)
433:
434:            while column < indents[-1]:        # count dedents
435:                if column not in indents:
436:                    raise IndentationError(
437:                        "unindent does not match any outer indentation level",
438:                        ("<tokenize>", lnum, pos, line))
439:                indents = indents[:-1]
440:
441:                if async_def and async_def_indent >= indents[-1]:
442:                    async_def = False
443:                    async_def_nl = False
444:                    async_def_indent = 0
445:
446:                yield (DEDENT, '', (lnum, pos), (lnum, pos), line)
447:
448:            if async_def and async_def_nl and async_def_indent >= indents[-1]:
449:                async_def = False
450:                async_def_nl = False
451:                async_def_indent = 0
452:
453:        else:                                  # continued statement
454:            if not line:
455:                raise TokenError("EOF in multi-line statement", (lnum, 0))
456:            continued = 0
457:
458:        while pos < max:
459:            pseudomatch = pseudoprog.match(line, pos)
460:            if pseudomatch:                                # scan for tokens
461:                start, end = pseudomatch.span(1)
462:                spos, epos, pos = (lnum, start), (lnum, end), end
463:                token, initial = line[start:end], line[start]
464:
465:                if initial in numchars or \
466:                   (initial == '.' and token != '.'):      # ordinary number
467:                    yield (NUMBER, token, spos, epos, line)
468:                elif initial in '\r\n':
469:                    newline = NEWLINE
470:                    if parenlev > 0:
471:                        newline = NL
472:                    elif async_def:
473:                        async_def_nl = True
474:                    if stashed:
475:                        yield stashed
476:                        stashed = None
477:                    yield (newline, token, spos, epos, line)
478:
479:                elif initial == '#':
480:                    assert not token.endswith("\n")
481:                    if stashed:
482:                        yield stashed
483:                        stashed = None
484:                    yield (COMMENT, token, spos, epos, line)
485:                elif token in triple_quoted:
486:                    endprog = endprogs[token]
487:                    endmatch = endprog.match(line, pos)
488:                    if endmatch:                           # all on one line
489:                        pos = endmatch.end(0)
490:                        token = line[start:pos]
491:                        if stashed:
492:                            yield stashed
493:                            stashed = None
494:                        yield (STRING, token, spos, (lnum, pos), line)
495:                    else:
496:                        strstart = (lnum, start)           # multiple lines
497:                        contstr = line[start:]
498:                        contline = line
499:                        break
500:                elif initial in single_quoted or \
501:                    token[:2] in single_quoted or \
502:                    token[:3] in single_quoted:
503:                    if token[-1] == '\n':                  # continued string
504:                        strstart = (lnum, start)
505:                        endprog = (endprogs[initial] or endprogs[token[1]] or
506:                                   endprogs[token[2]])
507:                        contstr, needcont = line[start:], 1
508:                        contline = line
509:                        break
510:                    else:                                  # ordinary string
511:                        if stashed:
512:                            yield stashed
513:                            stashed = None
514:                        yield (STRING, token, spos, epos, line)
515:                elif initial.isidentifier():               # ordinary name
516:                    if token in ('async', 'await'):
517:                        if async_is_reserved_keyword or async_def:
518:                            yield (ASYNC if token == 'async' else AWAIT,
519:                                   token, spos, epos, line)
520:                            continue
521:
522:                    tok = (NAME, token, spos, epos, line)
523:                    if token == 'async' and not stashed:
524:                        stashed = tok
525:                        continue
526:
527:                    if token in ('def', 'for'):
528:                        if (stashed
529:                                and stashed[0] == NAME
530:                                and stashed[1] == 'async'):
531:
532:                            if token == 'def':
533:                                async_def = True
534:                                async_def_indent = indents[-1]
535:
536:                            yield (ASYNC, stashed[1],
537:                                   stashed[2], stashed[3],
538:                                   stashed[4])
539:                            stashed = None
540:
541:                    if stashed:
542:                        yield stashed
543:                        stashed = None
544:
545:                    yield tok
546:                elif initial == '\\':                      # continued stmt
547:                    # This yield is new; needed for better idempotency:
548:                    if stashed:
549:                        yield stashed
550:                        stashed = None
551:                    yield (NL, token, spos, (lnum, pos), line)
552:                    continued = 1
553:                else:
554:                    if initial in '([{': parenlev = parenlev + 1
555:                    elif initial in ')]}': parenlev = parenlev - 1
556:                    if stashed:
557:                        yield stashed
558:                        stashed = None
559:                    yield (OP, token, spos, epos, line)
560:            else:
561:                yield (ERRORTOKEN, line[pos],
562:                           (lnum, pos), (lnum, pos+1), line)
563:                pos = pos + 1
564:
565:    if stashed:
566:        yield stashed
567:        stashed = None
568:
569:    for indent in indents[1:]:                 # pop remaining indent levels
570:        yield (DEDENT, '', (lnum, 0), (lnum, 0), '')
571:    yield (ENDMARKER, '', (lnum, 0), (lnum, 0), '')
572:
573:if __name__ == '__main__':                     # testing
574:    import sys
575:    if len(sys.argv) > 1: tokenize(open(sys.argv[1]).readline)
576:    else: tokenize(sys.stdin.readline)

Calling tool: grepTool
class InvalidInput
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:93:class InvalidInput(ValueError):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/black.py:92:class InvalidInput(ValueError):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
1:import asyncio
2:from asyncio.base_events import BaseEventLoop
3:from concurrent.futures import Executor, ProcessPoolExecutor
4:from datetime import datetime
5:from enum import Enum
6:from functools import lru_cache, partial, wraps
7:import io
8:import itertools
9:import logging
10:from multiprocessing import Manager, freeze_support
11:import os
12:from pathlib import Path
13:import pickle
14:import re
15:import signal
16:import sys
17:import tempfile
18:import tokenize
19:from typing import (
20:    Any,
21:    Callable,
22:    Collection,
23:    Dict,
24:    Generator,
25:    Generic,
26:    Iterable,
27:    Iterator,
28:    List,
29:    Optional,
30:    Pattern,
31:    Sequence,
32:    Set,
33:    Tuple,
34:    TypeVar,
35:    Union,
36:    cast,
37:)
38:
39:from appdirs import user_cache_dir
40:from attr import dataclass, evolve, Factory
41:import click
42:import toml
43:from typed_ast import ast3, ast27
44:
45:# lib2to3 fork
46:from blib2to3.pytree import Node, Leaf, type_repr
47:from blib2to3 import pygram, pytree
48:from blib2to3.pgen2 import driver, token
49:from blib2to3.pgen2.grammar import Grammar
50:from blib2to3.pgen2.parse import ParseError
51:from blib2to3.pgen2.tokenize import TokenizerConfig
52:
53:
54:__version__ = "19.3b0"
55:DEFAULT_LINE_LENGTH = 88
56:DEFAULT_EXCLUDES = (
57:    r"/(\.eggs|\.git|\.hg|\.mypy_cache|\.nox|\.tox|\.venv|_build|buck-out|build|dist)/"
58:)
59:DEFAULT_INCLUDES = r"\.pyi?$"
60:CACHE_DIR = Path(user_cache_dir("black", version=__version__))
61:
62:
63:# types
64:FileContent = str
65:Encoding = str
66:NewLine = str
67:Depth = int
68:NodeType = int
69:LeafID = int
70:Priority = int
71:Index = int
72:LN = Union[Leaf, Node]
73:SplitFunc = Callable[["Line", Collection["Feature"]], Iterator["Line"]]
74:Timestamp = float
75:FileSize = int
76:CacheInfo = Tuple[Timestamp, FileSize]
77:Cache = Dict[Path, CacheInfo]
78:out = partial(click.secho, bold=True, err=True)
79:err = partial(click.secho, fg="red", err=True)
80:
81:pygram.initialize(CACHE_DIR)
82:syms = pygram.python_symbols
83:
84:
85:class NothingChanged(UserWarning):
86:    """Raised when reformatted code is the same as source."""
87:
88:
89:class CannotSplit(Exception):
90:    """A readable split that fits the allotted line length is impossible."""
91:
92:
93:class InvalidInput(ValueError):
94:    """Raised when input source code fails all parse attempts."""
95:
96:
97:class WriteBack(Enum):
98:    NO = 0
99:    YES = 1
100:    DIFF = 2
101:    CHECK = 3
102:
103:    @classmethod
104:    def from_configuration(cls, *, check: bool, diff: bool) -> "WriteBack":
105:        if check and not diff:
106:            return cls.CHECK
107:
108:        return cls.DIFF if diff else cls.YES
109:
110:
111:class Changed(Enum):
112:    NO = 0
113:    CACHED = 1
114:    YES = 2
115:
116:
117:class TargetVersion(Enum):
118:    PY27 = 2
119:    PY33 = 3
120:    PY34 = 4
121:    PY35 = 5
122:    PY36 = 6
123:    PY37 = 7
124:    PY38 = 8
125:
126:    def is_python2(self) -> bool:
127:        return self is TargetVersion.PY27
128:
129:
130:PY36_VERSIONS = {TargetVersion.PY36, TargetVersion.PY37, TargetVersion.PY38}

Calling tool: grepTool
get_grammars\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/black.py:751:def get_grammars(target_versions: Set[TargetVersion]) -> List[Grammar]:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/black.py:772:    for grammar in get_grammars(set(target_versions)):

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: lib2to3_parse() now iterates over get_parser_configs(), constructs driver.Driver with parser_config.grammar and passes tokenizer_config through. Driver forwards tokenizer_config into tokenize.generate_tokens. Error handling remains: it catches ParseError, records location, and only raises InvalidInput after all attempts fail with correct line/column info.

Todo item 5 of 15 is correct
description: 'Confirm lib2to3_parse() iterates over get_parser_configs() results, constructs driver.Driver with parser_config.grammar and tokenizer_config, and passes tokenizer_config through to parsing (no regression in error handling: InvalidInput raised only after all attempts fail and includes correct location info).'

Validating todo item 6 of 15
description: 'Verify blib2to3/pgen2/driver.Driver __init__ signature adds tokenizer_config with default tokenize.TokenizerConfig(), stores it on self, and that parse_stream_raw() and parse_string() pass config=self.tokenizer_config to tokenize.generate_tokens().'

Calling tool: grepTool
class Driver\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py:30:class Driver(object):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Modifications:
5:# Copyright 2006 Google, Inc. All Rights Reserved.
6:# Licensed to PSF under a Contributor Agreement.
7:
8:"""Parser driver.
9:
10:This provides a high-level interface to parse a file into a syntax tree.
11:
12:"""
13:
14:__author__ = "Guido van Rossum <guido@python.org>"
15:
16:__all__ = ["Driver", "load_grammar"]
17:
18:# Python imports
19:import codecs
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(
33:        self,
34:        grammar,
35:        convert=None,
36:        logger=None,
37:        tokenizer_config=tokenize.TokenizerConfig(),
38:    ):
39:        self.grammar = grammar
40:        if logger is None:
41:            logger = logging.getLogger(__name__)
42:        self.logger = logger
43:        self.convert = convert
44:        self.tokenizer_config = tokenizer_config
45:
46:    def parse_tokens(self, tokens, debug=False):
47:        """Parse a series of tokens and return the syntax tree."""
48:        # XXX Move the prefix computation into a wrapper around tokenize.
49:        p = parse.Parser(self.grammar, self.convert)
50:        p.setup()
51:        lineno = 1
52:        column = 0
53:        indent_columns = []
54:        type = value = start = end = line_text = None
55:        prefix = ""
56:        for quintuple in tokens:
57:            type, value, start, end, line_text = quintuple
58:            if start != (lineno, column):
59:                assert (lineno, column) <= start, ((lineno, column), start)
60:                s_lineno, s_column = start
61:                if lineno < s_lineno:
62:                    prefix += "\n" * (s_lineno - lineno)
63:                    lineno = s_lineno
64:                    column = 0
65:                if column < s_column:
66:                    prefix += line_text[column:s_column]
67:                    column = s_column
68:            if type in (tokenize.COMMENT, tokenize.NL):
69:                prefix += value
70:                lineno, column = end
71:                if value.endswith("\n"):
72:                    lineno += 1
73:                    column = 0
74:                continue
75:            if type == token.OP:
76:                type = grammar.opmap[value]
77:            if debug:
78:                self.logger.debug("%s %r (prefix=%r)",
79:                                  token.tok_name[type], value, prefix)
80:            if type == token.INDENT:
81:                indent_columns.append(len(value))
82:                _prefix = prefix + value
83:                prefix = ""
84:                value = ""
85:            elif type == token.DEDENT:
86:                _indent_col = indent_columns.pop()
87:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
88:            if p.addtoken(type, value, (prefix, start)):
89:                if debug:
90:                    self.logger.debug("Stop.")
91:                break
92:            prefix = ""
93:            if type in {token.INDENT, token.DEDENT}:
94:                prefix = _prefix
95:            lineno, column = end
96:            if value.endswith("\n"):
97:                lineno += 1
98:                column = 0
99:        else:
100:            # We never broke out -- EOF is too soon (how can this happen???)
101:            raise parse.ParseError("incomplete input",
102:                                   type, value, (prefix, start))
103:        return p.rootnode
104:
105:    def parse_stream_raw(self, stream, debug=False):
106:        """Parse a stream and return the syntax tree."""
107:        tokens = tokenize.generate_tokens(stream.readline, config=self.tokenizer_config)
108:        return self.parse_tokens(tokens, debug)
109:
110:    def parse_stream(self, stream, debug=False):
111:        """Parse a stream and return the syntax tree."""
112:        return self.parse_stream_raw(stream, debug)
113:
114:    def parse_file(self, filename, encoding=None, debug=False):
115:        """Parse a file and return the syntax tree."""
116:        with io.open(filename, "r", encoding=encoding) as stream:
117:            return self.parse_stream(stream, debug)
118:
119:    def parse_string(self, text, debug=False):
120:        """Parse a string and return the syntax tree."""
121:        tokens = tokenize.generate_tokens(
122:            io.StringIO(text).readline,
123:            config=self.tokenizer_config,
124:        )
125:        return self.parse_tokens(tokens, debug)
126:
127:    def _partially_consume_prefix(self, prefix, column):
128:        lines = []
129:        current_line = ""
130:        current_column = 0
131:        wait_for_nl = False
132:        for char in prefix:
133:            current_line += char
134:            if wait_for_nl:
135:                if char == '\n':
136:                    if current_line.strip() and current_column < column:
137:                        res = ''.join(lines)
138:                        return res, prefix[len(res):]
139:
140:                    lines.append(current_line)
141:                    current_line = ""
142:                    current_column = 0
143:                    wait_for_nl = False
144:            elif char in ' \t':
145:                current_column += 1
146:            elif char == '\n':
147:                # unexpected empty line
148:                current_column = 0
149:            else:
150:                # indent is finished
151:                wait_for_nl = True
152:        return ''.join(lines), current_line
153:
154:
155:def _generate_pickle_name(gt, cache_dir=None):
156:    head, tail = os.path.splitext(gt)
157:    if tail == ".txt":
158:        tail = ""
159:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
160:    if cache_dir:
161:        return os.path.join(cache_dir, os.path.basename(name))
162:    else:
163:        return name
164:
165:
166:def load_grammar(gt="Grammar.txt", gp=None,
167:                 save=True, force=False, logger=None):
168:    """Load the grammar (maybe from a pickle)."""
169:    if logger is None:
170:        logger = logging.getLogger(__name__)

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: Verified in blib2to3/pgen2/driver.py: __init__ includes tokenizer_config=tokenize.TokenizerConfig() and assigns self.tokenizer_config. Both parse_stream_raw() and parse_string() call tokenize.generate_tokens(..., config=self.tokenizer_config).

Todo item 6 of 15 is correct
description: 'Verify blib2to3/pgen2/driver.Driver __init__ signature adds tokenizer_config with default tokenize.TokenizerConfig(), stores it on self, and that parse_stream_raw() and parse_string() pass config=self.tokenizer_config to tokenize.generate_tokens().'

Validating todo item 7 of 15
description: 'Ensure blib2to3/pgen2/tokenize.TokenizerConfig dataclass exists, is frozen, and defines bool field async_is_reserved_keyword defaulting to False.'

Calling tool: grepTool
TokenizerConfig
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:142:class TokenizerConfig:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py:37:        tokenizer_config=tokenize.TokenizerConfig(),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.pyi:11:from blib2to3.pgen2.tokenize import TokenizerConfig
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.pyi:23:        tokenizer_config: TokenizerConfig = ...
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi:14:class TokenizerConfig:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:51:from blib2to3.pgen2.tokenize import TokenizerConfig
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:766:    tokenizer_config: TokenizerConfig = TokenizerConfig()
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:776:                TokenizerConfig(async_is_reserved_keyword=True),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:781:                TokenizerConfig(async_is_reserved_keyword=False),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:805:                    TokenizerConfig(async_is_reserved_keyword=True),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:813:                    TokenizerConfig(async_is_reserved_keyword=False),

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
120:)
121:
122:endprogs = {"'": re.compile(Single), '"': re.compile(Double),
123:            "'''": single3prog, '"""': double3prog,
124:            **{f"{prefix}'''": single3prog for prefix in _strprefixes},
125:            **{f'{prefix}"""': double3prog for prefix in _strprefixes},
126:            **{prefix: None for prefix in _strprefixes}}
127:
128:triple_quoted = (
129:    {"'''", '"""'} |
130:    {f"{prefix}'''" for prefix in _strprefixes} |
131:    {f'{prefix}"""' for prefix in _strprefixes}
132:)
133:single_quoted = (
134:    {"'", '"'} |
135:    {f"{prefix}'" for prefix in _strprefixes} |
136:    {f'{prefix}"' for prefix in _strprefixes}
137:)
138:
139:tabsize = 8
140:
141:@dataclass(frozen=True)
142:class TokenizerConfig:
143:    async_is_reserved_keyword: bool = False
144:
145:class TokenError(Exception): pass
146:
147:class StopTokenizing(Exception): pass
148:
149:def printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line): # for testing
150:    (srow, scol) = xxx_todo_changeme
151:    (erow, ecol) = xxx_todo_changeme1
152:    print("%d,%d-%d,%d:\t%s\t%s" % \
153:        (srow, scol, erow, ecol, tok_name[type], repr(token)))
154:
155:def tokenize(readline, tokeneater=printtoken):
156:    """
157:    The tokenize() function accepts two parameters: one representing the
158:    input stream, and one providing an output mechanism for tokenize().
159:
160:    The first parameter, readline, must be a callable object which provides
161:    the same interface as the readline() method of built-in file objects.
162:    Each call to the function should return one line of input as a string.
163:
164:    The second parameter, tokeneater, must also be a callable object. It is
165:    called once for each token, with five arguments, corresponding to the
166:    tuples generated by generate_tokens().
167:    """
168:    try:
169:        tokenize_loop(readline, tokeneater)
170:    except StopTokenizing:
171:        pass
172:
173:# backwards compatible interface
174:def tokenize_loop(readline, tokeneater):
175:    for token_info in generate_tokens(readline):
176:        tokeneater(*token_info)
177:
178:class Untokenizer:
179:

Calling tool: grepTool
from attr import dataclass
Matched content:
34:from attr import dataclass

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
24:are the same, except instead of generating tokens, tokeneater is a callback
25:function to which the 5 fields described above are passed as 5 arguments,
26:each time a new token is found."""
27:
28:__author__ = 'Ka-Ping Yee <ping@lfw.org>'
29:__credits__ = \
30:    'GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro'
31:
32:import re
33:from codecs import BOM_UTF8, lookup
34:from attr import dataclass
35:from blib2to3.pgen2.token import *
36:
37:from . import token
38:__all__ = [x for x in dir(token) if x[0] != '_'] + ["tokenize",
39:           "generate_tokens", "untokenize"]
40:del token
41:
42:try:
43:    bytes

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: In blib2to3/pgen2/tokenize.py, a dataclass TokenizerConfig is defined with @dataclass(frozen=True) and contains the field async_is_reserved_keyword: bool = False. Verified via grep and reading lines 120-148 showing the exact implementation.

Todo item 7 of 15 is correct
description: 'Ensure blib2to3/pgen2/tokenize.TokenizerConfig dataclass exists, is frozen, and defines bool field async_is_reserved_keyword defaulting to False.'

Validating todo item 8 of 15
description: 'Validate blib2to3/pgen2/tokenize.generate_tokens() signature now accepts optional config: TokenizerConfig = TokenizerConfig(), and inside sets async_is_reserved_keyword = config.async_is_reserved_keyword.'

Calling tool: globTool
**/blib2to3/pgen2/tokenize.py
Matched files:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.py

Calling tool: grepTool
def generate_tokens\(
Matched content:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
320:
321:def untokenize(iterable):
322:    """Transform tokens back into Python source code.
323:
324:    Each element returned by the iterable must be a token sequence
325:    with at least two elements, a token number and token value.  If
326:    only two tokens are passed, the resulting output is poor.
327:
328:    Round-trip invariant for full input:
329:        Untokenized source will match input source exactly
330:
331:    Round-trip invariant for limited intput:
332:        # Output text will tokenize the back to the input
333:        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
334:        newcode = untokenize(t1)
335:        readline = iter(newcode.splitlines(1)).next
336:        t2 = [tok[:2] for tokin generate_tokens(readline)]
337:        assert t1 == t2
338:    """
339:    ut = Untokenizer()
340:    return ut.untokenize(iterable)
341:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
343:    """
344:    The generate_tokens() generator requires one argument, readline, which
345:    must be a callable object which provides the same interface as the
346:    readline() method of built-in file objects. Each call to the function
347:    should return one line of input as a string.  Alternately, readline
348:    can be a callable function terminating with StopIteration:
349:        readline = open(myfile).next    # Example of alternate readline
350:
351:    The generator produces 5-tuples with these members: the token type; the
352:    token string; a 2-tuple (srow, scol) of ints specifying the row and
353:    column where the token begins in the source; a 2-tuple (erow, ecol) of
354:    ints specifying the row and column where the token ends in the source;
355:    and the line on which the token was found. The line passed is the
356:    logical line; continuation lines are included.
357:    """
358:    lnum = parenlev = continued = 0
359:    numchars = '0123456789'
360:    contstr, needcont = '', 0
361:    contline = None
362:    indents = [0]
363:
364:    # If we know we're parsing 3.7+, we can unconditionally parse `async` and
365:    # `await` as keywords.
366:    async_is_reserved_keyword = config.async_is_reserved_keyword
367:    # 'stashed' and 'async_*' are used for async/await parsing
368:    stashed = None
369:    async_def = False
370:    async_def_indent = 0
371:    async_def_nl = False
372:
373:    while 1:                                   # loop over lines in stream
374:        try:
375:            line = readline()
376:        except StopIteration:
377:            line = ''
378:        lnum = lnum + 1
379:        pos, max = 0, len(line)
380:
381:        if contstr:                            # continued string
382:            if not line:
383:                raise TokenError("EOF in multi-line string", strstart)
384:            endmatch = endprog.match(line)
385:            if endmatch:
386:                pos = end = endmatch.end(0)
387:                yield (STRING, contstr + line[:end],
388:                       strstart, (lnum, end), contline + line)
389:                contstr, needcont = '', 0
390:                contline = None
391:            elif needcont and line[-2:] != '\\\n' and line[-3:] != '\\\r\n':
392:                yield (ERRORTOKEN, contstr + line,
393:                           strstart, (lnum, len(line)), contline)
394:                contstr = ''
395:                contline = None
396:                continue
397:            else:
398:                contstr = contstr + line
399:                contline = contline + line
400:                continue
401:
402:        elif parenlev == 0 and not continued:  # new statement
403:            if not line: break
404:            column = 0
405:            while pos < max:                   # measure leading whitespace
406:                if line[pos] == ' ': column = column + 1
407:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize
408:                elif line[pos] == '\f': column = 0
409:                else: break
410:                pos = pos + 1
411:            if pos == max: break
412:
413:            if stashed:
414:                yield stashed
415:                stashed = None
416:
417:            if line[pos] in '\r\n':            # skip blank lines
418:                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)
419:                continue
420:
421:            if line[pos] == '#':               # skip comments
422:                comment_token = line[pos:].rstrip('\r\n')
423:                nl_pos = pos + len(comment_token)
424:                yield (COMMENT, comment_token,
425:                        (lnum, pos), (lnum, pos + len(comment_token)), line)
426:                yield (NL, line[nl_pos:],
427:                        (lnum, nl_pos), (lnum, len(line)), line)
428:                continue
429:
430:            if column > indents[-1]:           # count indents
431:                indents.append(column)
432:                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)
433:
434:            while column < indents[-1]:        # count dedents
435:                if column not in indents:
436:                    raise IndentationError(
437:                        "unindent does not match any outer indentation level",
438:                        ("<tokenize>", lnum, pos, line))
439:                indents = indents[:-1]

Calling tool: grepTool
class TokenizerConfig
Matched content:
142:class TokenizerConfig:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
130:    {f"{prefix}'''" for prefix in _strprefixes} |
131:    {f'{prefix}"""' for prefix in _strprefixes}
132:)
133:single_quoted = (
134:    {"'", '"'} |
135:    {f"{prefix}'" for prefix in _strprefixes} |
136:    {f'{prefix}"' for prefix in _strprefixes}
137:)
138:
139:tabsize = 8
140:
141:@dataclass(frozen=True)
142:class TokenizerConfig:
143:    async_is_reserved_keyword: bool = False
144:
145:class TokenError(Exception): pass
146:
147:class StopTokenizing(Exception): pass
148:
149:def printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line): # for testing
150:    (srow, scol) = xxx_todo_changeme
151:    (erow, ecol) = xxx_todo_changeme1
152:    print("%d,%d-%d,%d:\t%s\t%s" % \
153:        (srow, scol, erow, ecol, tok_name[type], repr(token)))
154:
155:def tokenize(readline, tokeneater=printtoken):
156:    """
157:    The tokenize() function accepts two parameters: one representing the
158:    input stream, and one providing an output mechanism for tokenize().
159:

Calling tool: grepTool
async_is_reserved_keyword
Matched content:
143:    async_is_reserved_keyword: bool = False
366:    async_is_reserved_keyword = config.async_is_reserved_keyword
517:                        if async_is_reserved_keyword or async_def:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
500:                elif initial in single_quoted or \
501:                    token[:2] in single_quoted or \
502:                    token[:3] in single_quoted:
503:                    if token[-1] == '\n':                  # continued string
504:                        strstart = (lnum, start)
505:                        endprog = (endprogs[initial] or endprogs[token[1]] or
506:                                   endprogs[token[2]])
507:                        contstr, needcont = line[start:], 1
508:                        contline = line
509:                        break
510:                    else:                                  # ordinary string
511:                        if stashed:
512:                            yield stashed
513:                            stashed = None
514:                        yield (STRING, token, spos, epos, line)
515:                elif initial.isidentifier():               # ordinary name
516:                    if token in ('async', 'await'):
517:                        if async_is_reserved_keyword or async_def:
518:                            yield (ASYNC if token == 'async' else AWAIT,
519:                                   token, spos, epos, line)
520:                            continue
521:
522:                    tok = (NAME, token, spos, epos, line)
523:                    if token == 'async' and not stashed:
524:                        stashed = tok
525:                        continue
526:
527:                    if token in ('def', 'for'):
528:                        if (stashed
529:                                and stashed[0] == NAME
530:                                and stashed[1] == 'async'):
531:
532:                            if token == 'def':
533:                                async_def = True
534:                                async_def_indent = indents[-1]
535:
536:                            yield (ASYNC, stashed[1],
537:                                   stashed[2], stashed[3],
538:                                   stashed[4])
539:                            stashed = None

Calling tool: grepTool
def generate_tokens\(
Matched content:
337:def generate_tokens(readline):

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: In blib2to3/pgen2/tokenize.py, generate_tokens now has signature def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()) (line ~342), and inside it assigns async_is_reserved_keyword = config.async_is_reserved_keyword (line ~366), satisfying the requirement.

Todo item 8 of 15 is correct
description: 'Validate blib2to3/pgen2/tokenize.generate_tokens() signature now accepts optional config: TokenizerConfig = TokenizerConfig(), and inside sets async_is_reserved_keyword = config.async_is_reserved_keyword.'

Validating todo item 9 of 15
description: 'Inspect generate_tokens() tokenization rule for identifiers: when token in ('async','await') and (async_is_reserved_keyword or async_def) is True, yield ASYNC/await tokens; otherwise treat them as NAME. This should enable always-keyword behavior for Python 3.7+ mode while preserving legacy behavior for earlier versions.'

Calling tool: grepTool
def generate_tokens\(
Grep command failed with exit code 2: rg: .rgignore: No such file or directory (os error 2)
rg: /Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/{{FOLDER_PATH}}: IO error for operation on /Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/{{FOLDER_PATH}}: No such file or directory (os error 2)


Calling tool: grepTool
def generate_tokens\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi:32:def generate_tokens(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.py:337:def generate_tokens(readline):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.pyi:28:def generate_tokens(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/flake8/processor.py:259:    def generate_tokens(self):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pycodestyle.py:2026:    def generate_tokens(self):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/2/tokenize.pyi:117:def generate_tokens(readline: Callable[[], str]) -> Generator[_TokenType, None, None]: ...
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/3/tokenize.pyi:46:def generate_tokens(readline: Callable[[], str]) -> Generator[TokenInfo, None, None]: ...  # undocumented
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/2and3/lib2to3/pgen2/tokenize.pyi:28:def generate_tokens(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/coverage/phystokens.py:142:    def generate_tokens(self, text):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
320:
321:def untokenize(iterable):
322:    """Transform tokens back into Python source code.
323:
324:    Each element returned by the iterable must be a token sequence
325:    with at least two elements, a token number and token value.  If
326:    only two tokens are passed, the resulting output is poor.
327:
328:    Round-trip invariant for full input:
329:        Untokenized source will match input source exactly
330:
331:    Round-trip invariant for limited intput:
332:        # Output text will tokenize the back to the input
333:        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
334:        newcode = untokenize(t1)
335:        readline = iter(newcode.splitlines(1)).next
336:        t2 = [tok[:2] for tokin generate_tokens(readline)]
337:        assert t1 == t2
338:    """
339:    ut = Untokenizer()
340:    return ut.untokenize(iterable)
341:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
343:    """
344:    The generate_tokens() generator requires one argument, readline, which
345:    must be a callable object which provides the same interface as the
346:    readline() method of built-in file objects. Each call to the function
347:    should return one line of input as a string.  Alternately, readline
348:    can be a callable function terminating with StopIteration:
349:        readline = open(myfile).next    # Example of alternate readline
350:
351:    The generator produces 5-tuples with these members: the token type; the
352:    token string; a 2-tuple (srow, scol) of ints specifying the row and
353:    column where the token begins in the source; a 2-tuple (erow, ecol) of
354:    ints specifying the row and column where the token ends in the source;
355:    and the line on which the token was found. The line passed is the
356:    logical line; continuation lines are included.
357:    """
358:    lnum = parenlev = continued = 0
359:    numchars = '0123456789'
360:    contstr, needcont = '', 0
361:    contline = None
362:    indents = [0]
363:
364:    # If we know we're parsing 3.7+, we can unconditionally parse `async` and
365:    # `await` as keywords.
366:    async_is_reserved_keyword = config.async_is_reserved_keyword
367:    # 'stashed' and 'async_*' are used for async/await parsing
368:    stashed = None
369:    async_def = False
370:    async_def_indent = 0
371:    async_def_nl = False
372:
373:    while 1:                                   # loop over lines in stream
374:        try:
375:            line = readline()
376:        except StopIteration:
377:            line = ''
378:        lnum = lnum + 1
379:        pos, max = 0, len(line)
380:
381:        if contstr:                            # continued string
382:            if not line:
383:                raise TokenError("EOF in multi-line string", strstart)
384:            endmatch = endprog.match(line)
385:            if endmatch:
386:                pos = end = endmatch.end(0)
387:                yield (STRING, contstr + line[:end],
388:                       strstart, (lnum, end), contline + line)
389:                contstr, needcont = '', 0
390:                contline = None
391:            elif needcont and line[-2:] != '\\\n' and line[-3:] != '\\\r\n':
392:                yield (ERRORTOKEN, contstr + line,
393:                           strstart, (lnum, len(line)), contline)
394:                contstr = ''
395:                contline = None
396:                continue
397:            else:
398:                contstr = contstr + line
399:                contline = contline + line
400:                continue
401:
402:        elif parenlev == 0 and not continued:  # new statement
403:            if not line: break
404:            column = 0
405:            while pos < max:                   # measure leading whitespace
406:                if line[pos] == ' ': column = column + 1
407:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize
408:                elif line[pos] == '\f': column = 0
409:                else: break
410:                pos = pos + 1
411:            if pos == max: break
412:
413:            if stashed:
414:                yield stashed
415:                stashed = None
416:
417:            if line[pos] in '\r\n':            # skip blank lines
418:                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)
419:                continue
420:
421:            if line[pos] == '#':               # skip comments
422:                comment_token = line[pos:].rstrip('\r\n')
423:                nl_pos = pos + len(comment_token)
424:                yield (COMMENT, comment_token,
425:                        (lnum, pos), (lnum, pos + len(comment_token)), line)
426:                yield (NL, line[nl_pos:],
427:                        (lnum, nl_pos), (lnum, len(line)), line)
428:                continue
429:
430:            if column > indents[-1]:           # count indents
431:                indents.append(column)
432:                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)
433:
434:            while column < indents[-1]:        # count dedents
435:                if column not in indents:
436:                    raise IndentationError(
437:                        "unindent does not match any outer indentation level",
438:                        ("<tokenize>", lnum, pos, line))
439:                indents = indents[:-1]

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
440:
441:                if async_def and async_def_indent >= indents[-1]:
442:                    async_def = False
443:                    async_def_nl = False
444:                    async_def_indent = 0
445:
446:                yield (DEDENT, '', (lnum, pos), (lnum, pos), line)
447:
448:            if async_def and async_def_nl and async_def_indent >= indents[-1]:
449:                async_def = False
450:                async_def_nl = False
451:                async_def_indent = 0
452:
453:        else:                                  # continued statement
454:            if not line:
455:                raise TokenError("EOF in multi-line statement", (lnum, 0))
456:            continued = 0
457:
458:        while pos < max:
459:            pseudomatch = pseudoprog.match(line, pos)
460:            if pseudomatch:                                # scan for tokens
461:                start, end = pseudomatch.span(1)
462:                spos, epos, pos = (lnum, start), (lnum, end), end
463:                token, initial = line[start:end], line[start]
464:
465:                if initial in numchars or \
466:                   (initial == '.' and token != '.'):      # ordinary number
467:                    yield (NUMBER, token, spos, epos, line)
468:                elif initial in '\r\n':
469:                    newline = NEWLINE
470:                    if parenlev > 0:
471:                        newline = NL
472:                    elif async_def:
473:                        async_def_nl = True
474:                    if stashed:
475:                        yield stashed
476:                        stashed = None
477:                    yield (newline, token, spos, epos, line)
478:
479:                elif initial == '#':
480:                    assert not token.endswith("\n")
481:                    if stashed:
482:                        yield stashed
483:                        stashed = None
484:                    yield (COMMENT, token, spos, epos, line)
485:                elif token in triple_quoted:
486:                    endprog = endprogs[token]
487:                    endmatch = endprog.match(line, pos)
488:                    if endmatch:                           # all on one line
489:                        pos = endmatch.end(0)
490:                        token = line[start:pos]
491:                        if stashed:
492:                            yield stashed
493:                            stashed = None
494:                        yield (STRING, token, spos, (lnum, pos), line)
495:                    else:
496:                        strstart = (lnum, start)           # multiple lines
497:                        contstr = line[start:]
498:                        contline = line
499:                        break
500:                elif initial in single_quoted or \
501:                    token[:2] in single_quoted or \
502:                    token[:3] in single_quoted:
503:                    if token[-1] == '\n':                  # continued string
504:                        strstart = (lnum, start)
505:                        endprog = (endprogs[initial] or endprogs[token[1]] or
506:                                   endprogs[token[2]])
507:                        contstr, needcont = line[start:], 1
508:                        contline = line
509:                        break
510:                    else:                                  # ordinary string
511:                        if stashed:
512:                            yield stashed
513:                            stashed = None
514:                        yield (STRING, token, spos, epos, line)
515:                elif initial.isidentifier():               # ordinary name
516:                    if token in ('async', 'await'):
517:                        if async_is_reserved_keyword or async_def:
518:                            yield (ASYNC if token == 'async' else AWAIT,
519:                                   token, spos, epos, line)
520:                            continue
521:
522:                    tok = (NAME, token, spos, epos, line)
523:                    if token == 'async' and not stashed:
524:                        stashed = tok
525:                        continue
526:
527:                    if token in ('def', 'for'):
528:                        if (stashed
529:                                and stashed[0] == NAME
530:                                and stashed[1] == 'async'):
531:
532:                            if token == 'def':
533:                                async_def = True
534:                                async_def_indent = indents[-1]
535:
536:                            yield (ASYNC, stashed[1],
537:                                   stashed[2], stashed[3],
538:                                   stashed[4])
539:                            stashed = None
540:
541:                    if stashed:
542:                        yield stashed
543:                        stashed = None
544:
545:                    yield tok
546:                elif initial == '\\':                      # continued stmt
547:                    # This yield is new; needed for better idempotency:
548:                    if stashed:
549:                        yield stashed
550:                        stashed = None
551:                    yield (NL, token, spos, (lnum, pos), line)
552:                    continued = 1
553:                else:
554:                    if initial in '([{': parenlev = parenlev + 1
555:                    elif initial in ')]}': parenlev = parenlev - 1
556:                    if stashed:
557:                        yield stashed
558:                        stashed = None
559:                    yield (OP, token, spos, epos, line)
560:            else:
561:                yield (ERRORTOKEN, line[pos],
562:                           (lnum, pos), (lnum, pos+1), line)
563:                pos = pos + 1
564:
565:    if stashed:
566:        yield stashed
567:        stashed = None
568:
569:    for indent in indents[1:]:                 # pop remaining indent levels
570:        yield (DEDENT, '', (lnum, 0), (lnum, 0), '')
571:    yield (ENDMARKER, '', (lnum, 0), (lnum, 0), '')
572:
573:if __name__ == '__main__':                     # testing
574:    import sys
575:    if len(sys.argv) > 1: tokenize(open(sys.argv[1]).readline)
576:    else: tokenize(sys.stdin.readline)

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: In blib2to3/pgen2/tokenize.py, generate_tokens reads config.async_is_reserved_keyword (lines ~364-367) and in the identifier branch (lines ~515-521) emits ASYNC/await tokens when token in ('async','await') and (async_is_reserved_keyword or async_def) is True, otherwise it falls through to NAME. This enables always-keyword behavior for 3.7+ mode while preserving legacy behavior.

Todo item 9 of 15 is correct
description: 'Inspect generate_tokens() tokenization rule for identifiers: when token in ('async','await') and (async_is_reserved_keyword or async_def) is True, yield ASYNC/await tokens; otherwise treat them as NAME. This should enable always-keyword behavior for Python 3.7+ mode while preserving legacy behavior for earlier versions.'

Validating todo item 10 of 15
description: 'Verify Untokenizer.compat() includes ASYNC and AWAIT tokens in the set that receive a trailing space (toknum in (NAME, NUMBER, ASYNC, AWAIT)).'

Calling tool: grepTool
class Untokenizer
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:178:class Untokenizer:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi:22:class Untokenizer:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.py:173:class Untokenizer:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.pyi:18:class Untokenizer:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/3/tokenize.pyi:33:class Untokenizer:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/2/tokenize.pyi:129:class Untokenizer:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/2and3/lib2to3/pgen2/tokenize.pyi:18:class Untokenizer:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
150:    (srow, scol) = xxx_todo_changeme
151:    (erow, ecol) = xxx_todo_changeme1
152:    print("%d,%d-%d,%d:\t%s\t%s" % \
153:        (srow, scol, erow, ecol, tok_name[type], repr(token)))
154:
155:def tokenize(readline, tokeneater=printtoken):
156:    """
157:    The tokenize() function accepts two parameters: one representing the
158:    input stream, and one providing an output mechanism for tokenize().
159:
160:    The first parameter, readline, must be a callable object which provides
161:    the same interface as the readline() method of built-in file objects.
162:    Each call to the function should return one line of input as a string.
163:
164:    The second parameter, tokeneater, must also be a callable object. It is
165:    called once for each token, with five arguments, corresponding to the
166:    tuples generated by generate_tokens().
167:    """
168:    try:
169:        tokenize_loop(readline, tokeneater)
170:    except StopTokenizing:
171:        pass
172:
173:# backwards compatible interface
174:def tokenize_loop(readline, tokeneater):
175:    for token_info in generate_tokens(readline):
176:        tokeneater(*token_info)
177:
178:class Untokenizer:
179:
180:    def __init__(self):
181:        self.tokens = []
182:        self.prev_row = 1
183:        self.prev_col = 0
184:
185:    def add_whitespace(self, start):
186:        row, col = start
187:        assert row <= self.prev_row
188:        col_offset = col - self.prev_col
189:        if col_offset:
190:            self.tokens.append(" " * col_offset)
191:
192:    def untokenize(self, iterable):
193:        for t in iterable:
194:            if len(t) == 2:
195:                self.compat(t, iterable)
196:                break
197:            tok_type, token, start, end, line = t
198:            self.add_whitespace(start)
199:            self.tokens.append(token)
200:            self.prev_row, self.prev_col = end
201:            if tok_type in (NEWLINE, NL):
202:                self.prev_row += 1
203:                self.prev_col = 0
204:        return "".join(self.tokens)
205:
206:    def compat(self, token, iterable):
207:        startline = False
208:        indents = []
209:        toks_append = self.tokens.append
210:        toknum, tokval = token
211:        if toknum in (NAME, NUMBER):
212:            tokval += ' '
213:        if toknum in (NEWLINE, NL):
214:            startline = True
215:        for tok in iterable:
216:            toknum, tokval = tok[:2]
217:
218:            if toknum in (NAME, NUMBER, ASYNC, AWAIT):
219:                tokval += ' '
220:
221:            if toknum == INDENT:
222:                indents.append(tokval)
223:                continue
224:            elif toknum == DEDENT:
225:                indents.pop()
226:                continue
227:            elif toknum in (NEWLINE, NL):
228:                startline = True
229:            elif startline and indents:
230:                toks_append(indents[-1])
231:                startline = False
232:            toks_append(tokval)
233:
234:cookie_re = re.compile(r'^[ \t\f]*#.*?coding[:=][ \t]*([-\w.]+)', re.ASCII)
235:blank_re = re.compile(br'^[ \t\f]*(?:[#\r\n]|$)', re.ASCII)
236:
237:def _get_normal_name(orig_enc):
238:    """Imitates get_normal_name in tokenizer.c."""
239:    # Only care about the first 12 characters.
240:    enc = orig_enc[:12].lower().replace("_", "-")
241:    if enc == "utf-8" or enc.startswith("utf-8-"):
242:        return "utf-8"
243:    if enc in ("latin-1", "iso-8859-1", "iso-latin-1") or \
244:       enc.startswith(("latin-1-", "iso-8859-1-", "iso-latin-1-")):
245:        return "iso-8859-1"
246:    return orig_enc
247:
248:def detect_encoding(readline):
249:    """
250:    The detect_encoding() function is used to detect the encoding that should
251:    be used to decode a Python source file. It requires one argument, readline,
252:    in the same way as the tokenize() generator.
253:
254:    It will call readline a maximum of twice, and return the encoding used
255:    (as a string) and a list of any lines (left as bytes) it has read
256:    in.
257:
258:    It detects the encoding from the presence of a utf-8 bom or an encoding
259:    cookie as specified in pep-0263. If both a bom and a cookie are present, but
260:    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid
261:    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,
262:    'utf-8-sig' is returned.
263:
264:    If no encoding is specified, then the default of 'utf-8' will be returned.
265:    """
266:    bom_found = False
267:    encoding = None
268:    default = 'utf-8'
269:    def read_or_stop():

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.py
160:    called once for each token, with five arguments, corresponding to the
161:    tuples generated by generate_tokens().
162:    """
163:    try:
164:        tokenize_loop(readline, tokeneater)
165:    except StopTokenizing:
166:        pass
167:
168:# backwards compatible interface
169:def tokenize_loop(readline, tokeneater):
170:    for token_info in generate_tokens(readline):
171:        tokeneater(*token_info)
172:
173:class Untokenizer:
174:
175:    def __init__(self):
176:        self.tokens = []
177:        self.prev_row = 1
178:        self.prev_col = 0
179:
180:    def add_whitespace(self, start):
181:        row, col = start
182:        assert row <= self.prev_row
183:        col_offset = col - self.prev_col
184:        if col_offset:
185:            self.tokens.append(" " * col_offset)
186:
187:    def untokenize(self, iterable):
188:        for t in iterable:
189:            if len(t) == 2:
190:                self.compat(t, iterable)
191:                break
192:            tok_type, token, start, end, line = t
193:            self.add_whitespace(start)
194:            self.tokens.append(token)
195:            self.prev_row, self.prev_col = end
196:            if tok_type in (NEWLINE, NL):
197:                self.prev_row += 1
198:                self.prev_col = 0
199:        return "".join(self.tokens)
200:
201:    def compat(self, token, iterable):
202:        startline = False
203:        indents = []
204:        toks_append = self.tokens.append
205:        toknum, tokval = token
206:        if toknum in (NAME, NUMBER):
207:            tokval += ' '
208:        if toknum in (NEWLINE, NL):
209:            startline = True
210:        for tok in iterable:
211:            toknum, tokval = tok[:2]
212:
213:            if toknum in (NAME, NUMBER, ASYNC, AWAIT):
214:                tokval += ' '
215:
216:            if toknum == INDENT:
217:                indents.append(tokval)
218:                continue
219:            elif toknum == DEDENT:
220:                indents.pop()
221:                continue
222:            elif toknum in (NEWLINE, NL):
223:                startline = True
224:            elif startline and indents:
225:                toks_append(indents[-1])
226:                startline = False
227:            toks_append(tokval)
228:
229:cookie_re = re.compile(r'^[ \t\f]*#.*?coding[:=][ \t]*([-\w.]+)', re.ASCII)
230:blank_re = re.compile(br'^[ \t\f]*(?:[#\r\n]|$)', re.ASCII)
231:
232:def _get_normal_name(orig_enc):
233:    """Imitates get_normal_name in tokenizer.c."""
234:    # Only care about the first 12 characters.
235:    enc = orig_enc[:12].lower().replace("_", "-")
236:    if enc == "utf-8" or enc.startswith("utf-8-"):
237:        return "utf-8"
238:    if enc in ("latin-1", "iso-8859-1", "iso-latin-1") or \
239:       enc.startswith(("latin-1-", "iso-8859-1-", "iso-latin-1-")):
240:        return "iso-8859-1"
241:    return orig_enc
242:
243:def detect_encoding(readline):
244:    """
245:    The detect_encoding() function is used to detect the encoding that should
246:    be used to decode a Python source file. It requires one argument, readline,
247:    in the same way as the tokenize() generator.
248:
249:    It will call readline a maximum of twice, and return the encoding used
250:    (as a string) and a list of any lines (left as bytes) it has read
251:    in.
252:
253:    It detects the encoding from the presence of a utf-8 bom or an encoding
254:    cookie as specified in pep-0263. If both a bom and a cookie are present, but
255:    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid
256:    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,
257:    'utf-8-sig' is returned.
258:
259:    If no encoding is specified, then the default of 'utf-8' will be returned.
260:    """
261:    bom_found = False
262:    encoding = None
263:    default = 'utf-8'
264:    def read_or_stop():
265:        try:
266:            return readline()
267:        except StopIteration:
268:            return bytes()
269:
270:    def find_cookie(line):
271:        try:
272:            line_string = line.decode('ascii')
273:        except UnicodeDecodeError:
274:            return None
275:        match = cookie_re.match(line_string)
276:        if not match:
277:            return None
278:        encoding = _get_normal_name(match.group(1))
279:        try:

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: In blib2to3/pgen2/tokenize.py, Untokenizer.compat() adds a trailing space for tokens when toknum in (NAME, NUMBER, ASYNC, AWAIT). Verified at lines ~213-214 where the condition explicitly includes ASYNC and AWAIT.

Todo item 10 of 15 is correct
description: 'Verify Untokenizer.compat() includes ASYNC and AWAIT tokens in the set that receive a trailing space (toknum in (NAME, NUMBER, ASYNC, AWAIT)).'

Validating todo item 11 of 15
description: 'Confirm all internal call sites to tokenize.generate_tokens either pass the new config (driver) or rely on the default (e.g., pgen2/pgen.py). Ensure no remaining internal call sites break due to signature change.'

Calling tool: grepTool
generate_tokens\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:6:generate_tokens(readline) is a generator that breaks a stream of
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:166:    tuples generated by generate_tokens().
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:175:    for token_info in generate_tokens(readline):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:333:        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:336:        t2 = [tok[:2] for tokin generate_tokens(readline)]
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:344:    The generate_tokens() generator requires one argument, readline, which
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py:107:        tokens = tokenize.generate_tokens(stream.readline, config=self.tokenizer_config)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py:121:        tokens = tokenize.generate_tokens(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/pgen.py:19:        self.generator = tokenize.generate_tokens(stream.readline)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi:32:def generate_tokens(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.py:6:generate_tokens(readline) is a generator that breaks a stream of
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.py:161:    tuples generated by generate_tokens().
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.py:170:    for token_info in generate_tokens(readline):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.py:328:        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.py:331:        t2 = [tok[:2] for tokin generate_tokens(readline)]
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.py:337:def generate_tokens(readline):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.py:339:    The generate_tokens() generator requires one argument, readline, which
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/driver.py:100:        tokens = tokenize.generate_tokens(stream.readline)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/driver.py:114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.pyi:28:def generate_tokens(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/pgen.py:19:        self.generator = tokenize.generate_tokens(stream.readline)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/flake8/checker.py:578:        for token in file_processor.generate_tokens():
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/flake8/processor.py:120:                    tokenize.generate_tokens(lambda: next(line_iter))
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/flake8/processor.py:259:    def generate_tokens(self):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/flake8/processor.py:267:            for token in tokenize.generate_tokens(self.next_line):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pycodestyle.py:2026:    def generate_tokens(self):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pycodestyle.py:2030:        tokengen = tokenize.generate_tokens(self.readline)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pycodestyle.py:2088:        for token in self.generate_tokens():
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/sphinx/pycode/parser.py:134:        self.tokens = tokenize.generate_tokens(lambda: next(lines))
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/babel/messages/extract.py:417:    tokens = generate_tokens(next_line)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/2/tokenize.pyi:117:def generate_tokens(readline: Callable[[], str]) -> Generator[_TokenType, None, None]: ...
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/3/tokenize.pyi:46:def generate_tokens(readline: Callable[[], str]) -> Generator[TokenInfo, None, None]: ...  # undocumented
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/2and3/lib2to3/pgen2/tokenize.pyi:28:def generate_tokens(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/coverage/parser.py:138:        tokgen = generate_tokens(self.text)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/coverage/phystokens.py:21:    tokenize.generate_tokens() doesn't return a token for the backslash that
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/coverage/phystokens.py:25:    Returns the same values as generate_tokens()
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/coverage/phystokens.py:95:    tokgen = generate_tokens(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/coverage/phystokens.py:142:    def generate_tokens(self, text):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/coverage/phystokens.py:147:            self.last_tokens = list(tokenize.generate_tokens(readline))

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi
1:# Stubs for lib2to3.pgen2.tokenize (Python 3.6)
2:# NOTE: Only elements from __all__ are present.
3:
4:from typing import Callable, Iterable, Iterator, List, Text, Tuple
5:from attr import dataclass
6:from blib2to3.pgen2.token import *  # noqa
7:
8:
9:_Coord = Tuple[int, int]
10:_TokenEater = Callable[[int, Text, _Coord, _Coord, Text], None]
11:_TokenInfo = Tuple[int, Text, _Coord, _Coord, Text]
12:
13:@dataclass(frozen=True)
14:class TokenizerConfig:
15:    async_is_reserved_keyword: bool = False
16:
17:class TokenError(Exception): ...
18:class StopTokenizing(Exception): ...
19:
20:def tokenize(readline: Callable[[], Text], tokeneater: _TokenEater = ...) -> None: ...
21:
22:class Untokenizer:
23:    tokens: List[Text]
24:    prev_row: int
25:    prev_col: int
26:    def __init__(self) -> None: ...
27:    def add_whitespace(self, start: _Coord) -> None: ...
28:    def untokenize(self, iterable: Iterable[_TokenInfo]) -> Text: ...
29:    def compat(self, token: Tuple[int, Text], iterable: Iterable[_TokenInfo]) -> None: ...
30:
31:def untokenize(iterable: Iterable[_TokenInfo]) -> Text: ...
32:def generate_tokens(
33:    readline: Callable[[], Text]
34:) -> Iterator[_TokenInfo]: ...

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Modifications:
5:# Copyright 2006 Google, Inc. All Rights Reserved.
6:# Licensed to PSF under a Contributor Agreement.
7:
8:"""Parser driver.
9:
10:This provides a high-level interface to parse a file into a syntax tree.
11:
12:"""
13:
14:__author__ = "Guido van Rossum <guido@python.org>"
15:
16:__all__ = ["Driver", "load_grammar"]
17:
18:# Python imports
19:import codecs
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(
33:        self,
34:        grammar,
35:        convert=None,
36:        logger=None,
37:        tokenizer_config=tokenize.TokenizerConfig(),
38:    ):
39:        self.grammar = grammar
40:        if logger is None:
41:            logger = logging.getLogger(__name__)
42:        self.logger = logger
43:        self.convert = convert
44:        self.tokenizer_config = tokenizer_config
45:
46:    def parse_tokens(self, tokens, debug=False):
47:        """Parse a series of tokens and return the syntax tree."""
48:        # XXX Move the prefix computation into a wrapper around tokenize.
49:        p = parse.Parser(self.grammar, self.convert)
50:        p.setup()
51:        lineno = 1
52:        column = 0
53:        indent_columns = []
54:        type = value = start = end = line_text = None
55:        prefix = ""
56:        for quintuple in tokens:
57:            type, value, start, end, line_text = quintuple
58:            if start != (lineno, column):
59:                assert (lineno, column) <= start, ((lineno, column), start)
60:                s_lineno, s_column = start
61:                if lineno < s_lineno:
62:                    prefix += "\n" * (s_lineno - lineno)
63:                    lineno = s_lineno
64:                    column = 0
65:                if column < s_column:
66:                    prefix += line_text[column:s_column]
67:                    column = s_column
68:            if type in (tokenize.COMMENT, tokenize.NL):
69:                prefix += value
70:                lineno, column = end
71:                if value.endswith("\n"):
72:                    lineno += 1
73:                    column = 0
74:                continue
75:            if type == token.OP:
76:                type = grammar.opmap[value]
77:            if debug:
78:                self.logger.debug("%s %r (prefix=%r)",
79:                                  token.tok_name[type], value, prefix)
80:            if type == token.INDENT:
81:                indent_columns.append(len(value))
82:                _prefix = prefix + value
83:                prefix = ""
84:                value = ""
85:            elif type == token.DEDENT:
86:                _indent_col = indent_columns.pop()
87:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
88:            if p.addtoken(type, value, (prefix, start)):
89:                if debug:
90:                    self.logger.debug("Stop.")
91:                break
92:            prefix = ""
93:            if type in {token.INDENT, token.DEDENT}:
94:                prefix = _prefix
95:            lineno, column = end
96:            if value.endswith("\n"):
97:                lineno += 1
98:                column = 0
99:        else:
100:            # We never broke out -- EOF is too soon (how can this happen???)
101:            raise parse.ParseError("incomplete input",
102:                                   type, value, (prefix, start))
103:        return p.rootnode
104:
105:    def parse_stream_raw(self, stream, debug=False):
106:        """Parse a stream and return the syntax tree."""
107:        tokens = tokenize.generate_tokens(stream.readline, config=self.tokenizer_config)
108:        return self.parse_tokens(tokens, debug)
109:
110:    def parse_stream(self, stream, debug=False):
111:        """Parse a stream and return the syntax tree."""
112:        return self.parse_stream_raw(stream, debug)
113:
114:    def parse_file(self, filename, encoding=None, debug=False):
115:        """Parse a file and return the syntax tree."""
116:        with io.open(filename, "r", encoding=encoding) as stream:
117:            return self.parse_stream(stream, debug)
118:
119:    def parse_string(self, text, debug=False):
120:        """Parse a string and return the syntax tree."""
121:        tokens = tokenize.generate_tokens(
122:            io.StringIO(text).readline,
123:            config=self.tokenizer_config,
124:        )
125:        return self.parse_tokens(tokens, debug)
126:
127:    def _partially_consume_prefix(self, prefix, column):
128:        lines = []
129:        current_line = ""
130:        current_column = 0
131:        wait_for_nl = False
132:        for char in prefix:
133:            current_line += char
134:            if wait_for_nl:
135:                if char == '\n':
136:                    if current_line.strip() and current_column < column:
137:                        res = ''.join(lines)
138:                        return res, prefix[len(res):]
139:
140:                    lines.append(current_line)
141:                    current_line = ""
142:                    current_column = 0
143:                    wait_for_nl = False
144:            elif char in ' \t':
145:                current_column += 1
146:            elif char == '\n':
147:                # unexpected empty line
148:                current_column = 0
149:            else:
150:                # indent is finished
151:                wait_for_nl = True
152:        return ''.join(lines), current_line
153:
154:
155:def _generate_pickle_name(gt, cache_dir=None):
156:    head, tail = os.path.splitext(gt)
157:    if tail == ".txt":
158:        tail = ""
159:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
160:    if cache_dir:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/pgen.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Pgen imports
5:from . import grammar, token, tokenize
6:
7:class PgenGrammar(grammar.Grammar):
8:    pass
9:
10:class ParserGenerator(object):
11:
12:    def __init__(self, filename, stream=None):
13:        close_stream = None
14:        if stream is None:
15:            stream = open(filename)
16:            close_stream = stream.close
17:        self.filename = filename
18:        self.stream = stream
19:        self.generator = tokenize.generate_tokens(stream.readline)
20:        self.gettoken() # Initialize lookahead
21:        self.dfas, self.startsymbol = self.parse()
22:        if close_stream is not None:
23:            close_stream()
24:        self.first = {} # map from symbol name to set of tokens
25:        self.addfirstsets()
26:
27:    def make_grammar(self):
28:        c = PgenGrammar()
29:        names = list(self.dfas.keys())
30:        names.sort()
31:        names.remove(self.startsymbol)
32:        names.insert(0, self.startsymbol)
33:        for name in names:
34:            i = 256 + len(c.symbol2number)
35:            c.symbol2number[name] = i
36:            c.number2symbol[i] = name
37:        for name in names:
38:            dfa = self.dfas[name]
39:            states = []
40:            for state in dfa:
41:                arcs = []
42:                for label, next in sorted(state.arcs.items()):
43:                    arcs.append((self.make_label(c, label), dfa.index(next)))
44:                if state.isfinal:
45:                    arcs.append((0, dfa.index(state)))
46:                states.append(arcs)
47:            c.states.append(states)
48:            c.dfas[c.symbol2number[name]] = (states, self.make_first(c, name))
49:        c.start = c.symbol2number[self.startsymbol]
50:        return c
51:
52:    def make_first(self, c, name):
53:        rawfirst = self.first[name]
54:        first = {}
55:        for label in sorted(rawfirst):
56:            ilabel = self.make_label(c, label)
57:            ##assert ilabel not in first # XXX failed on <> ... !=
58:            first[ilabel] = 1
59:        return first
60:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
320:
321:def untokenize(iterable):
322:    """Transform tokens back into Python source code.
323:
324:    Each element returned by the iterable must be a token sequence
325:    with at least two elements, a token number and token value.  If
326:    only two tokens are passed, the resulting output is poor.
327:
328:    Round-trip invariant for full input:
329:        Untokenized source will match input source exactly
330:
331:    Round-trip invariant for limited intput:
332:        # Output text will tokenize the back to the input
333:        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
334:        newcode = untokenize(t1)
335:        readline = iter(newcode.splitlines(1)).next
336:        t2 = [tok[:2] for tokin generate_tokens(readline)]
337:        assert t1 == t2
338:    """
339:    ut = Untokenizer()
340:    return ut.untokenize(iterable)
341:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
343:    """
344:    The generate_tokens() generator requires one argument, readline, which
345:    must be a callable object which provides the same interface as the
346:    readline() method of built-in file objects. Each call to the function
347:    should return one line of input as a string.  Alternately, readline
348:    can be a callable function terminating with StopIteration:
349:        readline = open(myfile).next    # Example of alternate readline
350:
351:    The generator produces 5-tuples with these members: the token type; the
352:    token string; a 2-tuple (srow, scol) of ints specifying the row and
353:    column where the token begins in the source; a 2-tuple (erow, ecol) of
354:    ints specifying the row and column where the token ends in the source;
355:    and the line on which the token was found. The line passed is the
356:    logical line; continuation lines are included.
357:    """
358:    lnum = parenlev = continued = 0
359:    numchars = '0123456789'
360:    contstr, needcont = '', 0
361:    contline = None
362:    indents = [0]
363:
364:    # If we know we're parsing 3.7+, we can unconditionally parse `async` and
365:    # `await` as keywords.
366:    async_is_reserved_keyword = config.async_is_reserved_keyword
367:    # 'stashed' and 'async_*' are used for async/await parsing
368:    stashed = None
369:    async_def = False
370:    async_def_indent = 0
371:    async_def_nl = False
372:
373:    while 1:                                   # loop over lines in stream
374:        try:
375:            line = readline()
376:        except StopIteration:
377:            line = ''
378:        lnum = lnum + 1
379:        pos, max = 0, len(line)

Calling tool: grepTool
from dataclasses import dataclass
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/dataclasses-0.6.dist-info/METADATA:47:  from dataclasses import dataclass
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/cryptography/hazmat/primitives/serialization/ssh.py:14:from dataclasses import dataclass
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pip/_vendor/rich/_windows.py:2:from dataclasses import dataclass
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pip/_vendor/rich/progress.py:8:from dataclasses import dataclass, field
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pip/_vendor/rich/table.py:1:from dataclasses import dataclass, field, replace
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pip/_vendor/rich/pretty.py:9:from dataclasses import dataclass, fields, is_dataclass
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pip/_vendor/rich/_ratio.py:150:    from dataclasses import dataclass
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pip/_vendor/rich/traceback.py:6:from dataclasses import dataclass, field
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pip/_vendor/rich/console.py:9:from dataclasses import dataclass, field
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pip/_internal/self_outdated_check.py:9:from dataclasses import dataclass
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pip/_internal/models/link.py:8:from dataclasses import dataclass
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/pip/_internal/utils/logging.py:8:from dataclasses import dataclass

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
1:import asyncio
2:from asyncio.base_events import BaseEventLoop
3:from concurrent.futures import Executor, ProcessPoolExecutor
4:from datetime import datetime
5:from enum import Enum
6:from functools import lru_cache, partial, wraps
7:import io
8:import itertools
9:import logging
10:from multiprocessing import Manager, freeze_support
11:import os
12:from pathlib import Path
13:import pickle
14:import re
15:import signal
16:import sys
17:import tempfile
18:import tokenize
19:from typing import (
20:    Any,
21:    Callable,
22:    Collection,
23:    Dict,
24:    Generator,
25:    Generic,
26:    Iterable,
27:    Iterator,
28:    List,
29:    Optional,
30:    Pattern,
31:    Sequence,
32:    Set,
33:    Tuple,
34:    TypeVar,
35:    Union,
36:    cast,
37:)
38:
39:from appdirs import user_cache_dir
40:from attr import dataclass, evolve, Factory
41:import click
42:import toml
43:from typed_ast import ast3, ast27
44:
45:# lib2to3 fork
46:from blib2to3.pytree import Node, Leaf, type_repr
47:from blib2to3 import pygram, pytree
48:from blib2to3.pgen2 import driver, token
49:from blib2to3.pgen2.grammar import Grammar
50:from blib2to3.pgen2.parse import ParseError
51:from blib2to3.pgen2.tokenize import TokenizerConfig
52:
53:
54:__version__ = "19.3b0"
55:DEFAULT_LINE_LENGTH = 88
56:DEFAULT_EXCLUDES = (
57:    r"/(\.eggs|\.git|\.hg|\.mypy_cache|\.nox|\.tox|\.venv|_build|buck-out|build|dist)/"
58:)
59:DEFAULT_INCLUDES = r"\.pyi?$"
60:CACHE_DIR = Path(user_cache_dir("black", version=__version__))
61:
62:
63:# types
64:FileContent = str
65:Encoding = str
66:NewLine = str
67:Depth = int
68:NodeType = int
69:LeafID = int
70:Priority = int
71:Index = int
72:LN = Union[Leaf, Node]
73:SplitFunc = Callable[["Line", Collection["Feature"]], Iterator["Line"]]
74:Timestamp = float
75:FileSize = int
76:CacheInfo = Tuple[Timestamp, FileSize]
77:Cache = Dict[Path, CacheInfo]
78:out = partial(click.secho, bold=True, err=True)
79:err = partial(click.secho, fg="red", err=True)
80:
81:pygram.initialize(CACHE_DIR)
82:syms = pygram.python_symbols
83:
84:
85:class NothingChanged(UserWarning):
86:    """Raised when reformatted code is the same as source."""
87:
88:
89:class CannotSplit(Exception):
90:    """A readable split that fits the allotted line length is impossible."""
91:
92:
93:class InvalidInput(ValueError):
94:    """Raised when input source code fails all parse attempts."""
95:
96:
97:class WriteBack(Enum):
98:    NO = 0
99:    YES = 1
100:    DIFF = 2
101:    CHECK = 3
102:
103:    @classmethod
104:    def from_configuration(cls, *, check: bool, diff: bool) -> "WriteBack":
105:        if check and not diff:
106:            return cls.CHECK
107:
108:        return cls.DIFF if diff else cls.YES
109:
110:
111:class Changed(Enum):
112:    NO = 0
113:    CACHED = 1
114:    YES = 2
115:
116:
117:class TargetVersion(Enum):
118:    PY27 = 2
119:    PY33 = 3
120:    PY34 = 4
121:    PY35 = 5
122:    PY36 = 6
123:    PY37 = 7
124:    PY38 = 8
125:
126:    def is_python2(self) -> bool:
127:        return self is TargetVersion.PY27
128:
129:
130:PY36_VERSIONS = {TargetVersion.PY36, TargetVersion.PY37, TargetVersion.PY38}
131:
132:
133:class Feature(Enum):
134:    # All string literals are unicode
135:    UNICODE_LITERALS = 1
136:    F_STRINGS = 2
137:    NUMERIC_UNDERSCORES = 3
138:    TRAILING_COMMA_IN_CALL = 4
139:    TRAILING_COMMA_IN_DEF = 5
140:    # The following two feature-flags are mutually exclusive, and exactly one should be
141:    # set for every version of python.
142:    ASYNC_IS_VALID_IDENTIFIER = 6
143:    ASYNC_IS_RESERVED_KEYWORD = 7
144:
145:
146:VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {
147:    TargetVersion.PY27: {Feature.ASYNC_IS_VALID_IDENTIFIER},
148:    TargetVersion.PY33: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
149:    TargetVersion.PY34: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
150:    TargetVersion.PY35: {
151:        Feature.UNICODE_LITERALS,
152:        Feature.TRAILING_COMMA_IN_CALL,
153:        Feature.ASYNC_IS_VALID_IDENTIFIER,
154:    },
155:    TargetVersion.PY36: {
156:        Feature.UNICODE_LITERALS,
157:        Feature.F_STRINGS,
158:        Feature.NUMERIC_UNDERSCORES,
159:        Feature.TRAILING_COMMA_IN_CALL,
160:        Feature.TRAILING_COMMA_IN_DEF,
161:        Feature.ASYNC_IS_VALID_IDENTIFIER,
162:    },
163:    TargetVersion.PY37: {
164:        Feature.UNICODE_LITERALS,
165:        Feature.F_STRINGS,
166:        Feature.NUMERIC_UNDERSCORES,
167:        Feature.TRAILING_COMMA_IN_CALL,
168:        Feature.TRAILING_COMMA_IN_DEF,
169:        Feature.ASYNC_IS_RESERVED_KEYWORD,
170:    },
171:    TargetVersion.PY38: {
172:        Feature.UNICODE_LITERALS,
173:        Feature.F_STRINGS,
174:        Feature.NUMERIC_UNDERSCORES,
175:        Feature.TRAILING_COMMA_IN_CALL,
176:        Feature.TRAILING_COMMA_IN_DEF,
177:        Feature.ASYNC_IS_RESERVED_KEYWORD,
178:    },
179:}
180:
181:
182:@dataclass
183:class FileMode:
184:    target_versions: Set[TargetVersion] = Factory(set)
185:    line_length: int = DEFAULT_LINE_LENGTH
186:    string_normalization: bool = True
187:    is_pyi: bool = False
188:
189:    def get_cache_key(self) -> str:
190:        if self.target_versions:
191:            version_str = ",".join(
192:                str(version.value)
193:                for version in sorted(self.target_versions, key=lambda v: v.value)
194:            )
195:        else:
196:            version_str = "-"
197:        parts = [
198:            version_str,
199:            str(self.line_length),
200:            str(int(self.string_normalization)),
201:            str(int(self.is_pyi)),
202:        ]
203:        return ".".join(parts)
204:
205:
206:def supports_feature(target_versions: Set[TargetVersion], feature: Feature) -> bool:
207:    return all(feature in VERSION_TO_FEATURES[version] for version in target_versions)
208:
209:
210:def read_pyproject_toml(
211:    ctx: click.Context, param: click.Parameter, value: Union[str, int, bool, None]
212:) -> Optional[str]:
213:    """Inject Black configuration from "pyproject.toml" into defaults in `ctx`.
214:
215:    Returns the path to a successfully found and read configuration file, None
216:    otherwise.
217:    """
218:    assert not isinstance(value, (int, bool)), "Invalid parameter type passed"
219:    if not value:
220:        root = find_project_root(ctx.params.get("src", ()))

Calling tool: grepTool
driver\.Driver\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:827:        drv = driver.Driver(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/black.py:773:        drv = driver.Driver(grammar, pytree.convert)

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: Searched for all internal calls to generate_tokens. Only blib2to3/pgen2/driver.py and blib2to3/pgen2/pgen.py call it. Driver was updated to pass tokenizer_config via keyword, while pgen.py still calls it with only readline, relying on the new default TokenizerConfig(). No other internal call sites exist, so the signature change doesnt break anything.

Todo item 11 of 15 is correct
description: 'Confirm all internal call sites to tokenize.generate_tokens either pass the new config (driver) or rely on the default (e.g., pgen2/pgen.py). Ensure no remaining internal call sites break due to signature change.'

Validating todo item 12 of 15
description: 'Check blib2to3/pgen2/tokenize.pyi is consistent with implementation: it defines TokenizerConfig dataclass (frozen) with async_is_reserved_keyword, but its generate_tokens stub currently lacks the config parameter. Flag this inconsistency and require updating the stub to include the optional config parameter to match runtime signature.'

Calling tool: globTool
**/tokenize.pyi
Matched files:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/src/black/blib2to3/pgen2/tokenize.pyi
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/2/tokenize.pyi
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/3/tokenize.pyi
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/2and3/lib2to3/pgen2/tokenize.pyi

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi
1:# Stubs for lib2to3.pgen2.tokenize (Python 3.6)
2:# NOTE: Only elements from __all__ are present.
3:
4:from typing import Callable, Iterable, Iterator, List, Text, Tuple
5:from attr import dataclass
6:from blib2to3.pgen2.token import *  # noqa
7:
8:
9:_Coord = Tuple[int, int]
10:_TokenEater = Callable[[int, Text, _Coord, _Coord, Text], None]
11:_TokenInfo = Tuple[int, Text, _Coord, _Coord, Text]
12:
13:@dataclass(frozen=True)
14:class TokenizerConfig:
15:    async_is_reserved_keyword: bool = False
16:
17:class TokenError(Exception): ...
18:class StopTokenizing(Exception): ...
19:
20:def tokenize(readline: Callable[[], Text], tokeneater: _TokenEater = ...) -> None: ...
21:
22:class Untokenizer:
23:    tokens: List[Text]
24:    prev_row: int
25:    prev_col: int
26:    def __init__(self) -> None: ...
27:    def add_whitespace(self, start: _Coord) -> None: ...
28:    def untokenize(self, iterable: Iterable[_TokenInfo]) -> Text: ...
29:    def compat(self, token: Tuple[int, Text], iterable: Iterable[_TokenInfo]) -> None: ...
30:
31:def untokenize(iterable: Iterable[_TokenInfo]) -> Text: ...
32:def generate_tokens(
33:    readline: Callable[[], Text]
34:) -> Iterator[_TokenInfo]: ...

Calling tool: grepTool
def generate_tokens\(
Matched content:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
330:
331:    Round-trip invariant for limited intput:
332:        # Output text will tokenize the back to the input
333:        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
334:        newcode = untokenize(t1)
335:        readline = iter(newcode.splitlines(1)).next
336:        t2 = [tok[:2] for tokin generate_tokens(readline)]
337:        assert t1 == t2
338:    """
339:    ut = Untokenizer()
340:    return ut.untokenize(iterable)
341:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
343:    """
344:    The generate_tokens() generator requires one argument, readline, which
345:    must be a callable object which provides the same interface as the
346:    readline() method of built-in file objects. Each call to the function
347:    should return one line of input as a string.  Alternately, readline
348:    can be a callable function terminating with StopIteration:
349:        readline = open(myfile).next    # Example of alternate readline
350:
351:    The generator produces 5-tuples with these members: the token type; the
352:    token string; a 2-tuple (srow, scol) of ints specifying the row and
353:    column where the token begins in the source; a 2-tuple (erow, ecol) of
354:    ints specifying the row and column where the token ends in the source;
355:    and the line on which the token was found. The line passed is the
356:    logical line; continuation lines are included.
357:    """
358:    lnum = parenlev = continued = 0
359:    numchars = '0123456789'
360:    contstr, needcont = '', 0
361:    contline = None
362:    indents = [0]
363:
364:    # If we know we're parsing 3.7+, we can unconditionally parse `async` and
365:    # `await` as keywords.
366:    async_is_reserved_keyword = config.async_is_reserved_keyword
367:    # 'stashed' and 'async_*' are used for async/await parsing
368:    stashed = None
369:    async_def = False

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as INCORRECT
Reason: In implementation, generate_tokens(readline, config: TokenizerConfig = ...) was added, but the stub blib2to3/pgen2/tokenize.pyi still declares generate_tokens(readline) without the config parameter. While TokenizerConfig is defined in the stub, the function signature is inconsistent and needs updating.


=== PHASE 2 COMPLETE ===

