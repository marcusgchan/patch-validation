=== PHASE 1: ANALYSIS ===

[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "test_comment_indentation",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "class Driver", "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black" ]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "_partially_consume_prefix\\(",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "blib2to3\\.pgen2\\.driver",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "blib2to3", "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black" ]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "def fs\\(", "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black" ]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "\\bfs\\(", "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black" ]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "indent_columns", "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black" ]
Calling tool: grepTool
test_comment_indentation
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/bugsinpy_run_test.sh:1:python -m unittest -q tests.test_black.BlackTestCase.test_comment_indentation
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/coverage_bugsinpy.txt:1:coverage run -m unittest -q tests.test_black.BlackTestCase.test_comment_indentation
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:512:    def test_comment_indentation(self) -> None:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py
500:        self.assertFormatEqual(expected, actual)
501:        black.assert_equivalent(source, actual)
502:        black.assert_stable(source, actual, line_length=ll)
503:
504:    @patch("black.dump_to_file", dump_to_stderr)
505:    def test_bracket_match(self) -> None:
506:        source, expected = read_data("bracketmatch")
507:        actual = fs(source)
508:        self.assertFormatEqual(expected, actual)
509:        black.assert_equivalent(source, actual)
510:        black.assert_stable(source, actual, line_length=ll)
511:
512:    def test_comment_indentation(self) -> None:
513:        contents_tab = "if 1:\n\tif 2:\n\t\tpass\n\t# comment\n\tpass\n"
514:        contents_spc = "if 1:\n    if 2:\n        pass\n    # comment\n    pass\n"
515:
516:        self.assertFormatEqual(fs(contents_spc), contents_spc)
517:        self.assertFormatEqual(fs(contents_tab), contents_spc)
518:
519:        contents_tab = "if 1:\n\tif 2:\n\t\tpass\n\t\t# comment\n\tpass\n"
520:        contents_spc = "if 1:\n    if 2:\n        pass\n        # comment\n    pass\n"
521:
522:        self.assertFormatEqual(fs(contents_tab), contents_spc)
523:        self.assertFormatEqual(fs(contents_spc), contents_spc)
524:
525:    def test_report_verbose(self) -> None:
526:        report = black.Report(verbose=True)
527:        out_lines = []
528:        err_lines = []
529:
530:        def out(msg: str, **kwargs: Any) -> None:
531:            out_lines.append(msg)
532:
533:        def err(msg: str, **kwargs: Any) -> None:
534:            err_lines.append(msg)
535:
536:        with patch("black.out", out), patch("black.err", err):
537:            report.done(Path("f1"), black.Changed.NO)
538:            self.assertEqual(len(out_lines), 1)
539:            self.assertEqual(len(err_lines), 0)
540:            self.assertEqual(out_lines[-1], "f1 already well formatted, good job.")
541:            self.assertEqual(unstyle(str(report)), "1 file left unchanged.")
542:            self.assertEqual(report.return_code, 0)
543:            report.done(Path("f2"), black.Changed.YES)
544:            self.assertEqual(len(out_lines), 2)
545:            self.assertEqual(len(err_lines), 0)
546:            self.assertEqual(out_lines[-1], "reformatted f2")
547:            self.assertEqual(
548:                unstyle(str(report)), "1 file reformatted, 1 file left unchanged."
549:            )
550:            report.done(Path("f3"), black.Changed.CACHED)
551:            self.assertEqual(len(out_lines), 3)
552:            self.assertEqual(len(err_lines), 0)
553:            self.assertEqual(
554:                out_lines[-1], "f3 wasn't modified on disk since last run."
555:            )
556:            self.assertEqual(
557:                unstyle(str(report)), "1 file reformatted, 2 files left unchanged."
558:            )
559:            self.assertEqual(report.return_code, 0)
560:            report.check = True
561:            self.assertEqual(report.return_code, 1)
562:            report.check = False
563:            report.failed(Path("e1"), "boom")
564:            self.assertEqual(len(out_lines), 3)
565:            self.assertEqual(len(err_lines), 1)
566:            self.assertEqual(err_lines[-1], "error: cannot format e1: boom")
567:            self.assertEqual(
568:                unstyle(str(report)),
569:                "1 file reformatted, 2 files left unchanged, "
570:                "1 file failed to reformat.",
571:            )
572:            self.assertEqual(report.return_code, 123)
573:            report.done(Path("f3"), black.Changed.YES)
574:            self.assertEqual(len(out_lines), 4)
575:            self.assertEqual(len(err_lines), 1)
576:            self.assertEqual(out_lines[-1], "reformatted f3")
577:            self.assertEqual(
578:                unstyle(str(report)),
579:                "2 files reformatted, 2 files left unchanged, "
580:                "1 file failed to reformat.",
581:            )
582:            self.assertEqual(report.return_code, 123)
583:            report.failed(Path("e2"), "boom")
584:            self.assertEqual(len(out_lines), 4)
585:            self.assertEqual(len(err_lines), 2)
586:            self.assertEqual(err_lines[-1], "error: cannot format e2: boom")
587:            self.assertEqual(
588:                unstyle(str(report)),
589:                "2 files reformatted, 2 files left unchanged, "
590:                "2 files failed to reformat.",
591:            )
592:            self.assertEqual(report.return_code, 123)
593:            report.path_ignored(Path("wat"), "no match")
594:            self.assertEqual(len(out_lines), 5)
595:            self.assertEqual(len(err_lines), 2)
596:            self.assertEqual(out_lines[-1], "wat ignored: no match")
597:            self.assertEqual(
598:                unstyle(str(report)),
599:                "2 files reformatted, 2 files left unchanged, "
600:                "2 files failed to reformat.",
601:            )
602:            self.assertEqual(report.return_code, 123)
603:            report.done(Path("f4"), black.Changed.NO)
604:            self.assertEqual(len(out_lines), 6)
605:            self.assertEqual(len(err_lines), 2)
606:            self.assertEqual(out_lines[-1], "f4 already well formatted, good job.")
607:            self.assertEqual(
608:                unstyle(str(report)),
609:                "2 files reformatted, 3 files left unchanged, "
610:                "2 files failed to reformat.",
611:            )
612:            self.assertEqual(report.return_code, 123)
613:            report.check = True
614:            self.assertEqual(
615:                unstyle(str(report)),
616:                "2 files would be reformatted, 3 files would be left unchanged, "
617:                "2 files would fail to reformat.",
618:            )
619:

Calling tool: grepTool
class Driver
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:30:class Driver(object):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py:30:class Driver(object):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.pyi:13:class Driver:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.pyi:13:class Driver:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/2and3/lib2to3/pgen2/driver.pyi:13:class Driver:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Modifications:
5:# Copyright 2006 Google, Inc. All Rights Reserved.
6:# Licensed to PSF under a Contributor Agreement.
7:
8:"""Parser driver.
9:
10:This provides a high-level interface to parse a file into a syntax tree.
11:
12:"""
13:
14:__author__ = "Guido van Rossum <guido@python.org>"
15:
16:__all__ = ["Driver", "load_grammar"]
17:
18:# Python imports
19:import codecs
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(self, grammar, convert=None, logger=None):
33:        self.grammar = grammar
34:        if logger is None:
35:            logger = logging.getLogger()
36:        self.logger = logger
37:        self.convert = convert
38:
39:    def parse_tokens(self, tokens, debug=False):
40:        """Parse a series of tokens and return the syntax tree."""
41:        # XXX Move the prefix computation into a wrapper around tokenize.
42:        p = parse.Parser(self.grammar, self.convert)
43:        p.setup()
44:        lineno = 1
45:        column = 0
46:        indent_columns = []
47:        type = value = start = end = line_text = None
48:        prefix = ""
49:        for quintuple in tokens:
50:            type, value, start, end, line_text = quintuple
51:            if start != (lineno, column):
52:                assert (lineno, column) <= start, ((lineno, column), start)
53:                s_lineno, s_column = start
54:                if lineno < s_lineno:
55:                    prefix += "\n" * (s_lineno - lineno)
56:                    lineno = s_lineno
57:                    column = 0
58:                if column < s_column:
59:                    prefix += line_text[column:s_column]
60:                    column = s_column
61:            if type in (tokenize.COMMENT, tokenize.NL):
62:                prefix += value
63:                lineno, column = end
64:                if value.endswith("\n"):
65:                    lineno += 1
66:                    column = 0
67:                continue
68:            if type == token.OP:
69:                type = grammar.opmap[value]
70:            if debug:
71:                self.logger.debug("%s %r (prefix=%r)",
72:                                  token.tok_name[type], value, prefix)
73:            if type == token.INDENT:
74:                indent_columns.append(len(value))
75:                _prefix = prefix + value
76:                prefix = ""
77:                value = ""
78:            elif type == token.DEDENT:
79:                _indent_col = indent_columns.pop()
80:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
81:            if p.addtoken(type, value, (prefix, start)):
82:                if debug:
83:                    self.logger.debug("Stop.")
84:                break
85:            prefix = ""
86:            if type in {token.INDENT, token.DEDENT}:
87:                prefix = _prefix
88:            lineno, column = end
89:            if value.endswith("\n"):
90:                lineno += 1
91:                column = 0
92:        else:
93:            # We never broke out -- EOF is too soon (how can this happen???)
94:            raise parse.ParseError("incomplete input",
95:                                   type, value, (prefix, start))
96:        return p.rootnode
97:
98:    def parse_stream_raw(self, stream, debug=False):
99:        """Parse a stream and return the syntax tree."""
100:        tokens = tokenize.generate_tokens(stream.readline)
101:        return self.parse_tokens(tokens, debug)
102:
103:    def parse_stream(self, stream, debug=False):
104:        """Parse a stream and return the syntax tree."""
105:        return self.parse_stream_raw(stream, debug)
106:
107:    def parse_file(self, filename, encoding=None, debug=False):
108:        """Parse a file and return the syntax tree."""
109:        with io.open(filename, "r", encoding=encoding) as stream:
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char in ' \t':
135:                current_column += 1
136:            elif char == '\n':
137:                # unexpected empty line
138:                current_column = 0
139:            else:
140:                # indent is finished
141:                wait_for_nl = True
142:        return ''.join(lines), current_line
143:
144:
145:def _generate_pickle_name(gt, cache_dir=None):
146:    head, tail = os.path.splitext(gt)
147:    if tail == ".txt":
148:        tail = ""
149:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
150:    if cache_dir:
151:        return os.path.join(cache_dir, os.path.basename(name))
152:    else:
153:        return name
154:
155:
156:def load_grammar(gt="Grammar.txt", gp=None,
157:                 save=True, force=False, logger=None):
158:    """Load the grammar (maybe from a pickle)."""
159:    if logger is None:
160:        logger = logging.getLogger()
161:    gp = _generate_pickle_name(gt) if gp is None else gp
162:    if force or not _newer(gp, gt):
163:        logger.info("Generating grammar tables from %s", gt)
164:        g = pgen.generate_grammar(gt)
165:        if save:
166:            logger.info("Writing grammar tables to %s", gp)
167:            try:
168:                g.dump(gp)
169:            except OSError as e:
170:                logger.info("Writing failed: %s", e)
171:    else:
172:        g = grammar.Grammar()
173:        g.load(gp)
174:    return g
175:
176:
177:def _newer(a, b):
178:    """Inquire whether file a was written since file b."""
179:    if not os.path.exists(a):
180:        return False
181:    if not os.path.exists(b):
182:        return True
183:    return os.path.getmtime(a) >= os.path.getmtime(b)
184:
185:
186:def load_packaged_grammar(package, grammar_source, cache_dir=None):
187:    """Normally, loads a pickled grammar by doing
188:        pkgutil.get_data(package, pickled_grammar)
189:    where *pickled_grammar* is computed from *grammar_source* by adding the
190:    Python version and using a ``.pickle`` extension.
191:
192:    However, if *grammar_source* is an extant file, load_grammar(grammar_source)
193:    is called instead. This facilitates using a packaged grammar file when needed
194:    but preserves load_grammar's automatic regeneration behavior when possible.
195:
196:    """
197:    if os.path.isfile(grammar_source):
198:        gp = _generate_pickle_name(grammar_source, cache_dir) if cache_dir else None
199:        return load_grammar(grammar_source, gp=gp)
200:    pickled_name = _generate_pickle_name(os.path.basename(grammar_source), cache_dir)
201:    data = pkgutil.get_data(package, pickled_name)
202:    g = grammar.Grammar()
203:    g.loads(data)
204:    return g
205:
206:
207:def main(*args):
208:    """Main program, when run as a script: produce grammar pickle files.
209:
210:    Calls load_grammar for each argument, a path to a grammar text file.
211:    """
212:    if not args:
213:        args = sys.argv[1:]
214:    logging.basicConfig(level=logging.INFO, stream=sys.stdout,
215:                        format='%(message)s')
216:    for gt in args:
217:        load_grammar(gt, save=True, force=True)
218:    return True
219:
220:if __name__ == "__main__":

Calling tool: grepTool
_partially_consume_prefix\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:80:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:117:    def _partially_consume_prefix(self, prefix, column):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py:80:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py:117:    def _partially_consume_prefix(self, prefix, column):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char == ' ':
135:                current_column += 1
136:            elif char == '\t':
137:                current_column += 4
138:            elif char == '\n':
139:                # unexpected empty line
140:                current_column = 0
141:            else:
142:                # indent is finished
143:                wait_for_nl = True
144:        return ''.join(lines), current_line
145:
146:
147:def _generate_pickle_name(gt, cache_dir=None):
148:    head, tail = os.path.splitext(gt)
149:    if tail == ".txt":
150:        tail = ""
151:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
152:    if cache_dir:
153:        return os.path.join(cache_dir, os.path.basename(name))
154:    else:
155:        return name
156:
157:
158:def load_grammar(gt="Grammar.txt", gp=None,
159:                 save=True, force=False, logger=None):
160:    """Load the grammar (maybe from a pickle)."""
161:    if logger is None:
162:        logger = logging.getLogger()
163:    gp = _generate_pickle_name(gt) if gp is None else gp
164:    if force or not _newer(gp, gt):
165:        logger.info("Generating grammar tables from %s", gt)
166:        g = pgen.generate_grammar(gt)
167:        if save:
168:            logger.info("Writing grammar tables to %s", gp)
169:            try:

Calling tool: grepTool
blib2to3\.pgen2\.driver
No files found. Call grepTool again with a more generic pattern.

Calling tool: grepTool
blib2to3
Showing first 50 results from grep. Consider using a more specific pattern or path.
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/bugsinpy_patchfile.info:1:blib2to3/pgen2/driver.py;
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:34:from blib2to3.pgen2.token import *
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/setup.py:40:    packages=["blib2to3", "blib2to3.pgen2"],
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/setup.py:41:    package_data={"blib2to3": ["*.txt"]},
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/grammar.pyi:3:from blib2to3.pgen2 import _Path
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pygram.pyi:5:from blib2to3.pgen2.grammar import Grammar
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pygram.py:41:    python_grammar = driver.load_packaged_grammar("blib2to3", _GRAMMAR_FILE,
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pygram.py:55:    pattern_grammar = driver.load_packaged_grammar("blib2to3", _PATTERN_GRAMMAR_FILE,
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.pyi:8:from blib2to3.pytree import _Convert, _NL
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.pyi:9:from blib2to3.pgen2 import _Path
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.pyi:10:from blib2to3.pgen2.grammar import Grammar
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.pyi:5:from blib2to3.pgen2.token import *  # noqa
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.pyi:6:from blib2to3.pgen2.grammar import Grammar
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/parse.pyi:5:from blib2to3.pgen2.grammar import Grammar, _DFAS
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/parse.pyi:6:from blib2to3.pytree import _NL, _Convert, _RawNode
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/pgen.pyi:6:from blib2to3.pgen2 import _Path, grammar
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/pgen.pyi:7:from blib2to3.pgen2.tokenize import _TokenInfo
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black.py:45:from blib2to3.pytree import Node, Leaf, type_repr
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black.py:46:from blib2to3 import pygram, pytree
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black.py:47:from blib2to3.pgen2 import driver, token
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black.py:48:from blib2to3.pgen2.parse import ParseError
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black.py:1480:        # In blib2to3 INDENT never holds comments.
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black.py:2031:    It's not a :class:`blib2to3.pytree.Leaf` so that:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/tokenize.py:34:from blib2to3.pgen2.token import *
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/grammar.pyi:3:from blib2to3.pgen2 import _Path
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/pyproject.toml:26:  | blib2to3
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/pyproject.toml:64:    {include = "blib2to3"}
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.pyi:8:from blib2to3.pytree import _Convert, _NL
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.pyi:9:from blib2to3.pgen2 import _Path
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.pyi:10:from blib2to3.pgen2.grammar import Grammar
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/black.py:45:from blib2to3.pytree import Node, Leaf, type_repr
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/black.py:46:from blib2to3 import pygram, pytree
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/black.py:47:from blib2to3.pgen2 import driver, token
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/black.py:48:from blib2to3.pgen2.parse import ParseError
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/black.py:1480:        # In blib2to3 INDENT never holds comments.
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/black.py:2031:    It's not a :class:`blib2to3.pytree.Leaf` so that:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pygram.py:41:    python_grammar = driver.load_packaged_grammar("blib2to3", _GRAMMAR_FILE,
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pygram.py:55:    pattern_grammar = driver.load_packaged_grammar("blib2to3", _PATTERN_GRAMMAR_FILE,
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pytree.pyi:6:from blib2to3.pgen2.grammar import Grammar
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/README.md:588:  | blib2to3
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/tokenize.pyi:5:from blib2to3.pgen2.token import *  # noqa
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/parse.pyi:5:from blib2to3.pgen2.grammar import Grammar, _DFAS
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/parse.pyi:6:from blib2to3.pytree import _NL, _Convert, _RawNode
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pygram.pyi:5:from blib2to3.pgen2.grammar import Grammar
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/pgen.pyi:6:from blib2to3.pgen2 import _Path, grammar
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/pgen.pyi:7:from blib2to3.pgen2.tokenize import _TokenInfo
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/README.md:588:  | blib2to3
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/pyproject.toml:26:  | blib2to3
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/pyproject.toml:64:    {include = "blib2to3"}
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/MANIFEST.in:2:recursive-include blib2to3 *.txt *.py LICENSE

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black.py
1:import asyncio
2:from asyncio.base_events import BaseEventLoop
3:from concurrent.futures import Executor, ProcessPoolExecutor
4:from datetime import datetime
5:from enum import Enum, Flag
6:from functools import lru_cache, partial, wraps
7:import io
8:import itertools
9:import logging
10:from multiprocessing import Manager, freeze_support
11:import os
12:from pathlib import Path
13:import pickle
14:import re
15:import signal
16:import sys
17:import tempfile
18:import tokenize
19:from typing import (
20:    Any,
21:    Callable,
22:    Collection,
23:    Dict,
24:    Generator,
25:    Generic,
26:    Iterable,
27:    Iterator,
28:    List,
29:    Optional,
30:    Pattern,
31:    Sequence,
32:    Set,
33:    Tuple,
34:    TypeVar,
35:    Union,
36:    cast,
37:)
38:
39:from appdirs import user_cache_dir
40:from attr import dataclass, Factory
41:import click
42:import toml
43:
44:# lib2to3 fork
45:from blib2to3.pytree import Node, Leaf, type_repr
46:from blib2to3 import pygram, pytree
47:from blib2to3.pgen2 import driver, token
48:from blib2to3.pgen2.parse import ParseError
49:
50:
51:__version__ = "18.9b0"
52:DEFAULT_LINE_LENGTH = 88
53:DEFAULT_EXCLUDES = (
54:    r"/(\.eggs|\.git|\.hg|\.mypy_cache|\.nox|\.tox|\.venv|_build|buck-out|build|dist)/"
55:)
56:DEFAULT_INCLUDES = r"\.pyi?$"
57:CACHE_DIR = Path(user_cache_dir("black", version=__version__))
58:
59:
60:# types
61:FileContent = str
62:Encoding = str
63:NewLine = str
64:Depth = int
65:NodeType = int
66:LeafID = int
67:Priority = int
68:Index = int
69:LN = Union[Leaf, Node]
70:SplitFunc = Callable[["Line", bool], Iterator["Line"]]

Calling tool: grepTool
def fs\(
No files found. Call grepTool again with a more generic pattern.

Calling tool: grepTool
\bfs\(
Showing first 50 results from grep. Consider using a more specific pattern or path.
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:161:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:180:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:189:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:227:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:236:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:244:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:252:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:299:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:307:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:312:        not_normalized = fs(source, mode=mode)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:320:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:328:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:336:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:344:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:352:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:360:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:368:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:376:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:384:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:392:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:400:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:408:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:416:        actual = fs(source, mode=black.FileMode.PYTHON36)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:427:        actual = fs(source, mode=mode)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:435:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:442:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:450:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:458:        actual = fs(source, mode=mode)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:465:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:475:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:483:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:491:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:499:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:507:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:516:        self.assertFormatEqual(fs(contents_spc), contents_spc)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:517:        self.assertFormatEqual(fs(contents_tab), contents_spc)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:522:        self.assertFormatEqual(fs(contents_tab), contents_spc)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py:523:        self.assertFormatEqual(fs(contents_spc), contents_spc)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/tests/test_black.py:161:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/tests/test_black.py:180:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/tests/test_black.py:189:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/tests/test_black.py:227:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/tests/test_black.py:236:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/tests/test_black.py:244:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/tests/test_black.py:252:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/tests/test_black.py:299:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/tests/test_black.py:307:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/tests/test_black.py:312:        not_normalized = fs(source, mode=mode)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/tests/test_black.py:320:        actual = fs(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/tests/test_black.py:328:        actual = fs(source)

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/tests/test_black.py
1:#!/usr/bin/env python3
2:import asyncio
3:from concurrent.futures import ThreadPoolExecutor
4:from contextlib import contextmanager, redirect_stderr
5:from functools import partial, wraps
6:from io import BytesIO, TextIOWrapper
7:import os
8:from pathlib import Path
9:import re
10:import sys
11:from tempfile import TemporaryDirectory
12:from typing import (
13:    Any,
14:    BinaryIO,
15:    Callable,
16:    Coroutine,
17:    Generator,
18:    List,
19:    Tuple,
20:    Iterator,
21:    TypeVar,
22:)
23:import unittest
24:from unittest.mock import patch, MagicMock
25:
26:from click import unstyle
27:from click.testing import CliRunner
28:
29:import black
30:
31:try:
32:    import blackd
33:    from aiohttp.test_utils import TestClient, TestServer
34:except ImportError:
35:    has_blackd_deps = False
36:else:
37:    has_blackd_deps = True
38:
39:
40:ll = 88
41:ff = partial(black.format_file_in_place, line_length=ll, fast=True)
42:fs = partial(black.format_str, line_length=ll)
43:THIS_FILE = Path(__file__)
44:THIS_DIR = THIS_FILE.parent
45:EMPTY_LINE = "# EMPTY LINE WITH WHITESPACE" + " (this comment will be removed)"
46:T = TypeVar("T")
47:R = TypeVar("R")
48:
49:
50:def dump_to_stderr(*output: str) -> str:
51:    return "\n" + "\n".join(output) + "\n"
52:
53:
54:def read_data(name: str, data: bool = True) -> Tuple[str, str]:
55:    """read_data('test_name') -> 'input', 'output'"""
56:    if not name.endswith((".py", ".pyi", ".out", ".diff")):
57:        name += ".py"
58:    _input: List[str] = []
59:    _output: List[str] = []
60:    base_dir = THIS_DIR / "data" if data else THIS_DIR
61:    with open(base_dir / name, "r", encoding="utf8") as test:
62:        lines = test.readlines()
63:    result = _input
64:    for line in lines:
65:        line = line.replace(EMPTY_LINE, "")
66:        if line.rstrip() == "# output":
67:            result = _output
68:            continue
69:
70:        result.append(line)
71:    if _input and not _output:
72:        # If there's no output marker, treat the entire file as already pre-formatted.
73:        _output = _input[:]
74:    return "".join(_input).strip() + "\n", "".join(_output).strip() + "\n"
75:
76:
77:@contextmanager
78:def cache_dir(exists: bool = True) -> Iterator[Path]:
79:    with TemporaryDirectory() as workspace:
80:        cache_dir = Path(workspace)
81:        if not exists:
82:            cache_dir = cache_dir / "new"
83:        with patch("black.CACHE_DIR", cache_dir):
84:            yield cache_dir
85:
86:
87:@contextmanager
88:def event_loop(close: bool) -> Iterator[None]:
89:    policy = asyncio.get_event_loop_policy()
90:    old_loop = policy.get_event_loop()
91:    loop = policy.new_event_loop()
92:    asyncio.set_event_loop(loop)
93:    try:
94:        yield
95:
96:    finally:
97:        policy.set_event_loop(old_loop)
98:        if close:
99:            loop.close()
100:
101:
102:def async_test(f: Callable[..., Coroutine[Any, None, R]]) -> Callable[..., None]:
103:    @event_loop(close=True)
104:    @wraps(f)
105:    def wrapper(*args: Any, **kwargs: Any) -> None:
106:        asyncio.get_event_loop().run_until_complete(f(*args, **kwargs))
107:
108:    return wrapper
109:
110:
111:class BlackRunner(CliRunner):
112:    """Modify CliRunner so that stderr is not merged with stdout.
113:
114:    This is a hack that can be removed once we depend on Click 7.x"""
115:
116:    def __init__(self) -> None:
117:        self.stderrbuf = BytesIO()
118:        self.stdoutbuf = BytesIO()
119:        self.stdout_bytes = b""
120:        self.stderr_bytes = b""
121:        super().__init__()
122:
123:    @contextmanager
124:    def isolation(self, *args: Any, **kwargs: Any) -> Generator[BinaryIO, None, None]:
125:        with super().isolation(*args, **kwargs) as output:
126:            try:
127:                hold_stderr = sys.stderr
128:                sys.stderr = TextIOWrapper(self.stderrbuf, encoding=self.charset)
129:                yield output
130:            finally:
131:                self.stdout_bytes = sys.stdout.buffer.getvalue()  # type: ignore
132:                self.stderr_bytes = sys.stderr.buffer.getvalue()  # type: ignore
133:                sys.stderr = hold_stderr
134:
135:
136:class BlackTestCase(unittest.TestCase):
137:    maxDiff = None
138:
139:    def assertFormatEqual(self, expected: str, actual: str) -> None:
140:        if actual != expected and not os.environ.get("SKIP_AST_PRINT"):
141:            bdv: black.DebugVisitor[Any]
142:            black.out("Expected tree:", fg="green")
143:            try:
144:                exp_node = black.lib2to3_parse(expected)
145:                bdv = black.DebugVisitor()
146:                list(bdv.visit(exp_node))
147:            except Exception as ve:
148:                black.err(str(ve))
149:            black.out("Actual tree:", fg="red")
150:            try:
151:                exp_node = black.lib2to3_parse(actual)
152:                bdv = black.DebugVisitor()
153:                list(bdv.visit(exp_node))
154:            except Exception as ve:
155:                black.err(str(ve))
156:        self.assertEqual(expected, actual)
157:
158:    @patch("black.dump_to_file", dump_to_stderr)
159:    def test_empty(self) -> None:
160:        source = expected = ""
161:        actual = fs(source)
162:        self.assertFormatEqual(expected, actual)
163:        black.assert_equivalent(source, actual)
164:        black.assert_stable(source, actual, line_length=ll)
165:
166:    def test_empty_ff(self) -> None:
167:        expected = ""
168:        tmp_file = Path(black.dump_to_file())
169:        try:
170:            self.assertFalse(ff(tmp_file, write_back=black.WriteBack.YES))

Calling tool: grepTool
indent_columns
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:46:        indent_columns = []
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:74:                indent_columns.append(len(value))
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:79:                _indent_col = indent_columns.pop()
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py:46:        indent_columns = []
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py:74:                indent_columns.append(len(value))
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py:79:                _indent_col = indent_columns.pop()

Calling tool: createTodo
Todo List Created
Todo List (8 items):
- [ ] Confirm that in blib2to3/pgen2/driver.py, _partially_consume_prefix counts both spaces and tabs as width 1 by checking for `elif char in ' \t': current_column += 1` and that there is no remaining branch setting tabs to 4 in this file. (ID: todo-1)
- [ ] Verify that parse_tokens passes the correct indent column to _partially_consume_prefix: on INDENT, it appends len(value) to indent_columns; on DEDENT, it pops that value and calls _partially_consume_prefix(prefix, _indent_col). Ensure this uses character count (len) consistent with the updated tab=1 logic. (ID: todo-2)
- [ ] Ensure DEDENT handling preserves and reattaches the split prefix properly: after calling _partially_consume_prefix, confirm that `prefix` is set to the returned consumed portion and `_prefix` holds the remainder, and that after `p.addtoken`, when type is INDENT or DEDENT, `prefix = _prefix` is executed. (ID: todo-3)
- [ ] Validate that comment and NL tokens are added to the prefix and skipped in parse_tokens: check that when type in (tokenize.COMMENT, tokenize.NL), the code appends to `prefix` and continues, so that comment indentation is processed by _partially_consume_prefix. (ID: todo-4)
- [ ] Search the codebase for other instances in the root blib2to3/pgen2 module that might still treat tabs as width 4 in indentation calculations and confirm none exist (e.g., no `current_column += 4` for '\t' in the root driver implementation). (ID: todo-5)
- [ ] Identify duplicate copies of driver.py (e.g., under env/src/black/blib2to3/pgen2/driver.py) and verify whether the version used at runtime by black imports the updated root file. If the env copy is not used, note it; if it could be imported, ensure it is updated or that import paths favor the updated implementation. (ID: todo-6)
- [ ] Verify the test case tests/test_black.py::BlackTestCase::test_comment_indentation defines two scenarios with tab-indented inputs and space-indented expected outputs, and asserts `fs(contents_tab) == contents_spc` and `fs(contents_spc) == contents_spc` for both scenarios. (ID: todo-7)
- [ ] Cross-check that the first scenario in test_comment_indentation covers a comment immediately after a dedent (outer-level comment) and the second covers a comment at the inner indentation level, matching the issue description. (ID: todo-8)


=== PHASE 1 COMPLETE ===

=== PHASE 2: VALIDATION ===

Validating todo item 1 of 8
description: 'Confirm that in blib2to3/pgen2/driver.py, _partially_consume_prefix counts both spaces and tabs as width 1 by checking for `elif char in ' \t': current_column += 1` and that there is no remaining branch setting tabs to 4 in this file.'
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "def _partially_consume_prefix\\(",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "\\\\t", "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py" ]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "\\+= 4", "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py" ]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "elif char == '\\\\t':",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py"
]
Calling tool: grepTool
def _partially_consume_prefix\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:117:    def _partially_consume_prefix(self, prefix, column):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py:117:    def _partially_consume_prefix(self, prefix, column):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
100:        tokens = tokenize.generate_tokens(stream.readline)
101:        return self.parse_tokens(tokens, debug)
102:
103:    def parse_stream(self, stream, debug=False):
104:        """Parse a stream and return the syntax tree."""
105:        return self.parse_stream_raw(stream, debug)
106:
107:    def parse_file(self, filename, encoding=None, debug=False):
108:        """Parse a file and return the syntax tree."""
109:        with io.open(filename, "r", encoding=encoding) as stream:
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char in ' \t':
135:                current_column += 1
136:            elif char == '\n':
137:                # unexpected empty line
138:                current_column = 0
139:            else:
140:                # indent is finished
141:                wait_for_nl = True
142:        return ''.join(lines), current_line
143:
144:
145:def _generate_pickle_name(gt, cache_dir=None):
146:    head, tail = os.path.splitext(gt)
147:    if tail == ".txt":
148:        tail = ""
149:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
150:    if cache_dir:
151:        return os.path.join(cache_dir, os.path.basename(name))
152:    else:
153:        return name
154:
155:
156:def load_grammar(gt="Grammar.txt", gp=None,
157:                 save=True, force=False, logger=None):
158:    """Load the grammar (maybe from a pickle)."""
159:    if logger is None:
160:        logger = logging.getLogger()
161:    gp = _generate_pickle_name(gt) if gp is None else gp
162:    if force or not _newer(gp, gt):
163:        logger.info("Generating grammar tables from %s", gt)
164:        g = pgen.generate_grammar(gt)
165:        if save:
166:            logger.info("Writing grammar tables to %s", gp)
167:            try:
168:                g.dump(gp)
169:            except OSError as e:
170:                logger.info("Writing failed: %s", e)
171:    else:
172:        g = grammar.Grammar()
173:        g.load(gp)
174:    return g
175:
176:
177:def _newer(a, b):
178:    """Inquire whether file a was written since file b."""
179:    if not os.path.exists(a):
180:        return False
181:    if not os.path.exists(b):
182:        return True
183:    return os.path.getmtime(a) >= os.path.getmtime(b)
184:
185:
186:def load_packaged_grammar(package, grammar_source, cache_dir=None):
187:    """Normally, loads a pickled grammar by doing
188:        pkgutil.get_data(package, pickled_grammar)
189:    where *pickled_grammar* is computed from *grammar_source* by adding the
190:    Python version and using a ``.pickle`` extension.
191:
192:    However, if *grammar_source* is an extant file, load_grammar(grammar_source)
193:    is called instead. This facilitates using a packaged grammar file when needed
194:    but preserves load_grammar's automatic regeneration behavior when possible.
195:
196:    """
197:    if os.path.isfile(grammar_source):
198:        gp = _generate_pickle_name(grammar_source, cache_dir) if cache_dir else None
199:        return load_grammar(grammar_source, gp=gp)
200:    pickled_name = _generate_pickle_name(os.path.basename(grammar_source), cache_dir)
201:    data = pkgutil.get_data(package, pickled_name)
202:    g = grammar.Grammar()
203:    g.loads(data)
204:    return g
205:
206:
207:def main(*args):
208:    """Main program, when run as a script: produce grammar pickle files.
209:
210:    Calls load_grammar for each argument, a path to a grammar text file.
211:    """
212:    if not args:
213:        args = sys.argv[1:]
214:    logging.basicConfig(level=logging.INFO, stream=sys.stdout,
215:                        format='%(message)s')
216:    for gt in args:
217:        load_grammar(gt, save=True, force=True)
218:    return True
219:

Calling tool: grepTool
\\t
Matched content:
134:            elif char in ' \t':

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py
100:        tokens = tokenize.generate_tokens(stream.readline)
101:        return self.parse_tokens(tokens, debug)
102:
103:    def parse_stream(self, stream, debug=False):
104:        """Parse a stream and return the syntax tree."""
105:        return self.parse_stream_raw(stream, debug)
106:
107:    def parse_file(self, filename, encoding=None, debug=False):
108:        """Parse a file and return the syntax tree."""
109:        with io.open(filename, "r", encoding=encoding) as stream:
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char == ' ':
135:                current_column += 1
136:            elif char == '\t':
137:                current_column += 4
138:            elif char == '\n':
139:                # unexpected empty line
140:                current_column = 0
141:            else:
142:                # indent is finished
143:                wait_for_nl = True
144:        return ''.join(lines), current_line
145:
146:
147:def _generate_pickle_name(gt, cache_dir=None):
148:    head, tail = os.path.splitext(gt)
149:    if tail == ".txt":
150:        tail = ""
151:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
152:    if cache_dir:
153:        return os.path.join(cache_dir, os.path.basename(name))
154:    else:
155:        return name
156:
157:
158:def load_grammar(gt="Grammar.txt", gp=None,
159:                 save=True, force=False, logger=None):
160:    """Load the grammar (maybe from a pickle)."""
161:    if logger is None:
162:        logger = logging.getLogger()
163:    gp = _generate_pickle_name(gt) if gp is None else gp
164:    if force or not _newer(gp, gt):
165:        logger.info("Generating grammar tables from %s", gt)
166:        g = pgen.generate_grammar(gt)
167:        if save:
168:            logger.info("Writing grammar tables to %s", gp)
169:            try:
170:                g.dump(gp)
171:            except OSError as e:
172:                logger.info("Writing failed: %s", e)
173:    else:
174:        g = grammar.Grammar()
175:        g.load(gp)
176:    return g
177:
178:
179:def _newer(a, b):
180:    """Inquire whether file a was written since file b."""
181:    if not os.path.exists(a):
182:        return False
183:    if not os.path.exists(b):
184:        return True
185:    return os.path.getmtime(a) >= os.path.getmtime(b)
186:
187:
188:def load_packaged_grammar(package, grammar_source, cache_dir=None):
189:    """Normally, loads a pickled grammar by doing
190:        pkgutil.get_data(package, pickled_grammar)
191:    where *pickled_grammar* is computed from *grammar_source* by adding the
192:    Python version and using a ``.pickle`` extension.
193:
194:    However, if *grammar_source* is an extant file, load_grammar(grammar_source)
195:    is called instead. This facilitates using a packaged grammar file when needed
196:    but preserves load_grammar's automatic regeneration behavior when possible.
197:
198:    """
199:    if os.path.isfile(grammar_source):
200:        gp = _generate_pickle_name(grammar_source, cache_dir) if cache_dir else None
201:        return load_grammar(grammar_source, gp=gp)
202:    pickled_name = _generate_pickle_name(os.path.basename(grammar_source), cache_dir)
203:    data = pkgutil.get_data(package, pickled_name)
204:    g = grammar.Grammar()
205:    g.loads(data)
206:    return g
207:
208:
209:def main(*args):
210:    """Main program, when run as a script: produce grammar pickle files.
211:
212:    Calls load_grammar for each argument, a path to a grammar text file.
213:    """
214:    if not args:
215:        args = sys.argv[1:]
216:    logging.basicConfig(level=logging.INFO, stream=sys.stdout,
217:                        format='%(message)s')
218:    for gt in args:
219:        load_grammar(gt, save=True, force=True)

Calling tool: grepTool
\+= 4
No files found. Call grepTool again with a more generic pattern.

Calling tool: grepTool
elif char == '\\t':
No files found. Call grepTool again with a more generic pattern.

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: Verified in blib2to3/pgen2/driver.py _partially_consume_prefix: it uses `elif char in ' \t': current_column += 1` (lines ~134-136) and there is no remaining `elif char == '\t': current_column += 4` branch or any `+= 4` occurrences in this file.


Todo item 1 of 8 is correct
description: 'Confirm that in blib2to3/pgen2/driver.py, _partially_consume_prefix counts both spaces and tabs as width 1 by checking for `elif char in ' \t': current_column += 1` and that there is no remaining branch setting tabs to 4 in this file.'
Validating todo item 2 of 8
description: 'Verify that parse_tokens passes the correct indent column to _partially_consume_prefix: on INDENT, it appends len(value) to indent_columns; on DEDENT, it pops that value and calls _partially_consume_prefix(prefix, _indent_col). Ensure this uses character count (len) consistent with the updated tab=1 logic.'
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "def parse_tokens\\(",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "_partially_consume_prefix\\(",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black"
]
Calling tool: grepTool
def parse_tokens\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:39:    def parse_tokens(self, tokens, debug=False):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.pyi:18:    def parse_tokens(self, tokens: Iterable[Any], debug: bool = ...) -> _NL: ...
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py:39:    def parse_tokens(self, tokens, debug=False):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.pyi:18:    def parse_tokens(self, tokens: Iterable[Any], debug: bool = ...) -> _NL: ...
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/2and3/lib2to3/pgen2/driver.pyi:18:    def parse_tokens(self, tokens: Iterable[Any], debug: bool = ...) -> _NL: ...

Calling tool: grepTool
_partially_consume_prefix\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:80:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:117:    def _partially_consume_prefix(self, prefix, column):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py:80:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py:117:    def _partially_consume_prefix(self, prefix, column):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Modifications:
5:# Copyright 2006 Google, Inc. All Rights Reserved.
6:# Licensed to PSF under a Contributor Agreement.
7:
8:"""Parser driver.
9:
10:This provides a high-level interface to parse a file into a syntax tree.
11:
12:"""
13:
14:__author__ = "Guido van Rossum <guido@python.org>"
15:
16:__all__ = ["Driver", "load_grammar"]
17:
18:# Python imports
19:import codecs
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(self, grammar, convert=None, logger=None):
33:        self.grammar = grammar
34:        if logger is None:
35:            logger = logging.getLogger()
36:        self.logger = logger
37:        self.convert = convert
38:
39:    def parse_tokens(self, tokens, debug=False):
40:        """Parse a series of tokens and return the syntax tree."""
41:        # XXX Move the prefix computation into a wrapper around tokenize.
42:        p = parse.Parser(self.grammar, self.convert)
43:        p.setup()
44:        lineno = 1
45:        column = 0
46:        indent_columns = []
47:        type = value = start = end = line_text = None
48:        prefix = ""
49:        for quintuple in tokens:
50:            type, value, start, end, line_text = quintuple
51:            if start != (lineno, column):
52:                assert (lineno, column) <= start, ((lineno, column), start)
53:                s_lineno, s_column = start
54:                if lineno < s_lineno:
55:                    prefix += "\n" * (s_lineno - lineno)
56:                    lineno = s_lineno
57:                    column = 0
58:                if column < s_column:
59:                    prefix += line_text[column:s_column]
60:                    column = s_column
61:            if type in (tokenize.COMMENT, tokenize.NL):
62:                prefix += value
63:                lineno, column = end
64:                if value.endswith("\n"):
65:                    lineno += 1
66:                    column = 0
67:                continue
68:            if type == token.OP:
69:                type = grammar.opmap[value]
70:            if debug:
71:                self.logger.debug("%s %r (prefix=%r)",
72:                                  token.tok_name[type], value, prefix)
73:            if type == token.INDENT:
74:                indent_columns.append(len(value))
75:                _prefix = prefix + value
76:                prefix = ""
77:                value = ""
78:            elif type == token.DEDENT:
79:                _indent_col = indent_columns.pop()
80:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
81:            if p.addtoken(type, value, (prefix, start)):
82:                if debug:
83:                    self.logger.debug("Stop.")
84:                break
85:            prefix = ""
86:            if type in {token.INDENT, token.DEDENT}:
87:                prefix = _prefix
88:            lineno, column = end
89:            if value.endswith("\n"):
90:                lineno += 1
91:                column = 0
92:        else:
93:            # We never broke out -- EOF is too soon (how can this happen???)
94:            raise parse.ParseError("incomplete input",
95:                                   type, value, (prefix, start))
96:        return p.rootnode
97:
98:    def parse_stream_raw(self, stream, debug=False):
99:        """Parse a stream and return the syntax tree."""
100:        tokens = tokenize.generate_tokens(stream.readline)
101:        return self.parse_tokens(tokens, debug)
102:
103:    def parse_stream(self, stream, debug=False):
104:        """Parse a stream and return the syntax tree."""
105:        return self.parse_stream_raw(stream, debug)
106:
107:    def parse_file(self, filename, encoding=None, debug=False):
108:        """Parse a file and return the syntax tree."""
109:        with io.open(filename, "r", encoding=encoding) as stream:
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char in ' \t':
135:                current_column += 1
136:            elif char == '\n':
137:                # unexpected empty line
138:                current_column = 0
139:            else:
140:                # indent is finished
141:                wait_for_nl = True
142:        return ''.join(lines), current_line
143:
144:
145:def _generate_pickle_name(gt, cache_dir=None):
146:    head, tail = os.path.splitext(gt)
147:    if tail == ".txt":
148:        tail = ""
149:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
150:    if cache_dir:
151:        return os.path.join(cache_dir, os.path.basename(name))
152:    else:
153:        return name
154:
155:
156:def load_grammar(gt="Grammar.txt", gp=None,
157:                 save=True, force=False, logger=None):
158:    """Load the grammar (maybe from a pickle)."""
159:    if logger is None:
160:        logger = logging.getLogger()
161:    gp = _generate_pickle_name(gt) if gp is None else gp
162:    if force or not _newer(gp, gt):
163:        logger.info("Generating grammar tables from %s", gt)
164:        g = pgen.generate_grammar(gt)
165:        if save:
166:            logger.info("Writing grammar tables to %s", gp)
167:            try:
168:                g.dump(gp)
169:            except OSError as e:
170:                logger.info("Writing failed: %s", e)
171:    else:
172:        g = grammar.Grammar()
173:        g.load(gp)
174:    return g
175:
176:
177:def _newer(a, b):
178:    """Inquire whether file a was written since file b."""
179:    if not os.path.exists(a):
180:        return False

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: In parse_tokens, INDENT appends len(value) to indent_columns (line 74) and on DEDENT it pops that value and calls _partially_consume_prefix(prefix, _indent_col) (lines 79-81). _partially_consume_prefix now counts both spaces and tabs as 1 column (line 134), so the character-count (len) matches the updated tab=1 logic.


Todo item 2 of 8 is correct
description: 'Verify that parse_tokens passes the correct indent column to _partially_consume_prefix: on INDENT, it appends len(value) to indent_columns; on DEDENT, it pops that value and calls _partially_consume_prefix(prefix, _indent_col). Ensure this uses character count (len) consistent with the updated tab=1 logic.'
Validating todo item 3 of 8
description: 'Ensure DEDENT handling preserves and reattaches the split prefix properly: after calling _partially_consume_prefix, confirm that `prefix` is set to the returned consumed portion and `_prefix` holds the remainder, and that after `p.addtoken`, when type is INDENT or DEDENT, `prefix = _prefix` is executed.'
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "_partially_consume_prefix\\(",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "p\\.addtoken\\(", "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black" ]
Calling tool: grepTool
_partially_consume_prefix\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:80:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:117:    def _partially_consume_prefix(self, prefix, column):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py:80:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py:117:    def _partially_consume_prefix(self, prefix, column):

Calling tool: grepTool
p\.addtoken\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/parse.py:35:        if p.addtoken(...):           # parse a token; may raise ParseError
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py:81:            if p.addtoken(type, value, (prefix, start)):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:81:            if p.addtoken(type, value, (prefix, start)):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/parse.py:35:        if p.addtoken(...):           # parse a token; may raise ParseError

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Modifications:
5:# Copyright 2006 Google, Inc. All Rights Reserved.
6:# Licensed to PSF under a Contributor Agreement.
7:
8:"""Parser driver.
9:
10:This provides a high-level interface to parse a file into a syntax tree.
11:
12:"""
13:
14:__author__ = "Guido van Rossum <guido@python.org>"
15:
16:__all__ = ["Driver", "load_grammar"]
17:
18:# Python imports
19:import codecs
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(self, grammar, convert=None, logger=None):
33:        self.grammar = grammar
34:        if logger is None:
35:            logger = logging.getLogger()
36:        self.logger = logger
37:        self.convert = convert
38:
39:    def parse_tokens(self, tokens, debug=False):
40:        """Parse a series of tokens and return the syntax tree."""
41:        # XXX Move the prefix computation into a wrapper around tokenize.
42:        p = parse.Parser(self.grammar, self.convert)
43:        p.setup()
44:        lineno = 1
45:        column = 0
46:        indent_columns = []
47:        type = value = start = end = line_text = None
48:        prefix = ""
49:        for quintuple in tokens:
50:            type, value, start, end, line_text = quintuple
51:            if start != (lineno, column):
52:                assert (lineno, column) <= start, ((lineno, column), start)
53:                s_lineno, s_column = start
54:                if lineno < s_lineno:
55:                    prefix += "\n" * (s_lineno - lineno)
56:                    lineno = s_lineno
57:                    column = 0
58:                if column < s_column:
59:                    prefix += line_text[column:s_column]
60:                    column = s_column
61:            if type in (tokenize.COMMENT, tokenize.NL):
62:                prefix += value
63:                lineno, column = end
64:                if value.endswith("\n"):
65:                    lineno += 1
66:                    column = 0
67:                continue
68:            if type == token.OP:
69:                type = grammar.opmap[value]
70:            if debug:
71:                self.logger.debug("%s %r (prefix=%r)",
72:                                  token.tok_name[type], value, prefix)
73:            if type == token.INDENT:
74:                indent_columns.append(len(value))
75:                _prefix = prefix + value
76:                prefix = ""
77:                value = ""
78:            elif type == token.DEDENT:
79:                _indent_col = indent_columns.pop()
80:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
81:            if p.addtoken(type, value, (prefix, start)):
82:                if debug:
83:                    self.logger.debug("Stop.")
84:                break
85:            prefix = ""
86:            if type in {token.INDENT, token.DEDENT}:
87:                prefix = _prefix
88:            lineno, column = end
89:            if value.endswith("\n"):
90:                lineno += 1
91:                column = 0
92:        else:
93:            # We never broke out -- EOF is too soon (how can this happen???)
94:            raise parse.ParseError("incomplete input",
95:                                   type, value, (prefix, start))
96:        return p.rootnode
97:
98:    def parse_stream_raw(self, stream, debug=False):
99:        """Parse a stream and return the syntax tree."""
100:        tokens = tokenize.generate_tokens(stream.readline)
101:        return self.parse_tokens(tokens, debug)
102:
103:    def parse_stream(self, stream, debug=False):
104:        """Parse a stream and return the syntax tree."""
105:        return self.parse_stream_raw(stream, debug)
106:
107:    def parse_file(self, filename, encoding=None, debug=False):
108:        """Parse a file and return the syntax tree."""
109:        with io.open(filename, "r", encoding=encoding) as stream:
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char in ' \t':
135:                current_column += 1
136:            elif char == '\n':
137:                # unexpected empty line
138:                current_column = 0
139:            else:
140:                # indent is finished
141:                wait_for_nl = True
142:        return ''.join(lines), current_line
143:
144:
145:def _generate_pickle_name(gt, cache_dir=None):
146:    head, tail = os.path.splitext(gt)
147:    if tail == ".txt":
148:        tail = ""
149:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
150:    if cache_dir:
151:        return os.path.join(cache_dir, os.path.basename(name))
152:    else:
153:        return name
154:
155:
156:def load_grammar(gt="Grammar.txt", gp=None,
157:                 save=True, force=False, logger=None):
158:    """Load the grammar (maybe from a pickle)."""
159:    if logger is None:
160:        logger = logging.getLogger()

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py
60:                    column = s_column
61:            if type in (tokenize.COMMENT, tokenize.NL):
62:                prefix += value
63:                lineno, column = end
64:                if value.endswith("\n"):
65:                    lineno += 1
66:                    column = 0
67:                continue
68:            if type == token.OP:
69:                type = grammar.opmap[value]
70:            if debug:
71:                self.logger.debug("%s %r (prefix=%r)",
72:                                  token.tok_name[type], value, prefix)
73:            if type == token.INDENT:
74:                indent_columns.append(len(value))
75:                _prefix = prefix + value
76:                prefix = ""
77:                value = ""
78:            elif type == token.DEDENT:
79:                _indent_col = indent_columns.pop()
80:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
81:            if p.addtoken(type, value, (prefix, start)):
82:                if debug:
83:                    self.logger.debug("Stop.")
84:                break
85:            prefix = ""
86:            if type in {token.INDENT, token.DEDENT}:
87:                prefix = _prefix
88:            lineno, column = end
89:            if value.endswith("\n"):
90:                lineno += 1
91:                column = 0
92:        else:
93:            # We never broke out -- EOF is too soon (how can this happen???)
94:            raise parse.ParseError("incomplete input",
95:                                   type, value, (prefix, start))
96:        return p.rootnode
97:
98:    def parse_stream_raw(self, stream, debug=False):
99:        """Parse a stream and return the syntax tree."""
100:        tokens = tokenize.generate_tokens(stream.readline)
101:        return self.parse_tokens(tokens, debug)
102:
103:    def parse_stream(self, stream, debug=False):
104:        """Parse a stream and return the syntax tree."""
105:        return self.parse_stream_raw(stream, debug)
106:
107:    def parse_file(self, filename, encoding=None, debug=False):
108:        """Parse a file and return the syntax tree."""
109:        with io.open(filename, "r", encoding=encoding) as stream:
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char == ' ':
135:                current_column += 1
136:            elif char == '\t':
137:                current_column += 4
138:            elif char == '\n':
139:                # unexpected empty line
140:                current_column = 0
141:            else:
142:                # indent is finished
143:                wait_for_nl = True
144:        return ''.join(lines), current_line
145:
146:
147:def _generate_pickle_name(gt, cache_dir=None):
148:    head, tail = os.path.splitext(gt)
149:    if tail == ".txt":
150:        tail = ""
151:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
152:    if cache_dir:
153:        return os.path.join(cache_dir, os.path.basename(name))
154:    else:
155:        return name
156:
157:
158:def load_grammar(gt="Grammar.txt", gp=None,
159:                 save=True, force=False, logger=None):
160:    """Load the grammar (maybe from a pickle)."""
161:    if logger is None:
162:        logger = logging.getLogger()
163:    gp = _generate_pickle_name(gt) if gp is None else gp
164:    if force or not _newer(gp, gt):
165:        logger.info("Generating grammar tables from %s", gt)
166:        g = pgen.generate_grammar(gt)
167:        if save:
168:            logger.info("Writing grammar tables to %s", gp)
169:            try:
170:                g.dump(gp)
171:            except OSError as e:
172:                logger.info("Writing failed: %s", e)
173:    else:
174:        g = grammar.Grammar()
175:        g.load(gp)
176:    return g

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: In blib2to3/pgen2/driver.py, within parse_tokens: on DEDENT, it assigns `prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)` (lines ~79-80), so `prefix` gets the consumed part and `_prefix` the remainder. After `p.addtoken(...)`, it executes `if type in {token.INDENT, token.DEDENT}: prefix = _prefix` (lines ~85-87), reattaching the remainder as required.


Todo item 3 of 8 is correct
description: 'Ensure DEDENT handling preserves and reattaches the split prefix properly: after calling _partially_consume_prefix, confirm that `prefix` is set to the returned consumed portion and `_prefix` holds the remainder, and that after `p.addtoken`, when type is INDENT or DEDENT, `prefix = _prefix` is executed.'
Validating todo item 4 of 8
description: 'Validate that comment and NL tokens are added to the prefix and skipped in parse_tokens: check that when type in (tokenize.COMMENT, tokenize.NL), the code appends to `prefix` and continues, so that comment indentation is processed by _partially_consume_prefix.'
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "class Driver", "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/{{FOLDER_PATH}}" ]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "parse_tokens\\(", "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black" ]
Calling tool: grepTool
class Driver
Grep command failed with exit code 2: rg: .rgignore: No such file or directory (os error 2)
rg: /Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/{{FOLDER_PATH}}: IO error for operation on /Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/{{FOLDER_PATH}}: No such file or directory (os error 2)


Calling tool: grepTool
parse_tokens\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black.py:2008:    in `pgen2/driver.py:Driver.parse_tokens()`.  This was a brilliant implementation
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:39:    def parse_tokens(self, tokens, debug=False):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:101:        return self.parse_tokens(tokens, debug)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:115:        return self.parse_tokens(tokens, debug)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.pyi:18:    def parse_tokens(self, tokens: Iterable[Any], debug: bool = ...) -> _NL: ...
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py:39:    def parse_tokens(self, tokens, debug=False):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py:101:        return self.parse_tokens(tokens, debug)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py:115:        return self.parse_tokens(tokens, debug)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.pyi:18:    def parse_tokens(self, tokens: Iterable[Any], debug: bool = ...) -> _NL: ...
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/black.py:2008:    in `pgen2/driver.py:Driver.parse_tokens()`.  This was a brilliant implementation
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/2and3/lib2to3/pgen2/driver.pyi:18:    def parse_tokens(self, tokens: Iterable[Any], debug: bool = ...) -> _NL: ...

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Modifications:
5:# Copyright 2006 Google, Inc. All Rights Reserved.
6:# Licensed to PSF under a Contributor Agreement.
7:
8:"""Parser driver.
9:
10:This provides a high-level interface to parse a file into a syntax tree.
11:
12:"""
13:
14:__author__ = "Guido van Rossum <guido@python.org>"
15:
16:__all__ = ["Driver", "load_grammar"]
17:
18:# Python imports
19:import codecs
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(self, grammar, convert=None, logger=None):
33:        self.grammar = grammar
34:        if logger is None:
35:            logger = logging.getLogger()
36:        self.logger = logger
37:        self.convert = convert
38:
39:    def parse_tokens(self, tokens, debug=False):
40:        """Parse a series of tokens and return the syntax tree."""
41:        # XXX Move the prefix computation into a wrapper around tokenize.
42:        p = parse.Parser(self.grammar, self.convert)
43:        p.setup()
44:        lineno = 1
45:        column = 0
46:        indent_columns = []
47:        type = value = start = end = line_text = None
48:        prefix = ""
49:        for quintuple in tokens:
50:            type, value, start, end, line_text = quintuple
51:            if start != (lineno, column):
52:                assert (lineno, column) <= start, ((lineno, column), start)
53:                s_lineno, s_column = start
54:                if lineno < s_lineno:
55:                    prefix += "\n" * (s_lineno - lineno)
56:                    lineno = s_lineno
57:                    column = 0
58:                if column < s_column:
59:                    prefix += line_text[column:s_column]
60:                    column = s_column
61:            if type in (tokenize.COMMENT, tokenize.NL):
62:                prefix += value
63:                lineno, column = end
64:                if value.endswith("\n"):
65:                    lineno += 1
66:                    column = 0
67:                continue
68:            if type == token.OP:
69:                type = grammar.opmap[value]
70:            if debug:
71:                self.logger.debug("%s %r (prefix=%r)",
72:                                  token.tok_name[type], value, prefix)
73:            if type == token.INDENT:
74:                indent_columns.append(len(value))
75:                _prefix = prefix + value
76:                prefix = ""
77:                value = ""
78:            elif type == token.DEDENT:
79:                _indent_col = indent_columns.pop()
80:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
81:            if p.addtoken(type, value, (prefix, start)):
82:                if debug:
83:                    self.logger.debug("Stop.")
84:                break
85:            prefix = ""
86:            if type in {token.INDENT, token.DEDENT}:
87:                prefix = _prefix
88:            lineno, column = end
89:            if value.endswith("\n"):
90:                lineno += 1
91:                column = 0
92:        else:
93:            # We never broke out -- EOF is too soon (how can this happen???)
94:            raise parse.ParseError("incomplete input",
95:                                   type, value, (prefix, start))
96:        return p.rootnode
97:
98:    def parse_stream_raw(self, stream, debug=False):
99:        """Parse a stream and return the syntax tree."""
100:        tokens = tokenize.generate_tokens(stream.readline)
101:        return self.parse_tokens(tokens, debug)
102:
103:    def parse_stream(self, stream, debug=False):
104:        """Parse a stream and return the syntax tree."""
105:        return self.parse_stream_raw(stream, debug)
106:
107:    def parse_file(self, filename, encoding=None, debug=False):
108:        """Parse a file and return the syntax tree."""
109:        with io.open(filename, "r", encoding=encoding) as stream:
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char in ' \t':
135:                current_column += 1
136:            elif char == '\n':
137:                # unexpected empty line
138:                current_column = 0
139:            else:
140:                # indent is finished
141:                wait_for_nl = True
142:        return ''.join(lines), current_line
143:
144:
145:def _generate_pickle_name(gt, cache_dir=None):
146:    head, tail = os.path.splitext(gt)
147:    if tail == ".txt":
148:        tail = ""
149:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
150:    if cache_dir:
151:        return os.path.join(cache_dir, os.path.basename(name))
152:    else:
153:        return name
154:
155:
156:def load_grammar(gt="Grammar.txt", gp=None,
157:                 save=True, force=False, logger=None):
158:    """Load the grammar (maybe from a pickle)."""
159:    if logger is None:
160:        logger = logging.getLogger()
161:    gp = _generate_pickle_name(gt) if gp is None else gp
162:    if force or not _newer(gp, gt):
163:        logger.info("Generating grammar tables from %s", gt)
164:        g = pgen.generate_grammar(gt)
165:        if save:
166:            logger.info("Writing grammar tables to %s", gp)
167:            try:
168:                g.dump(gp)
169:            except OSError as e:
170:                logger.info("Writing failed: %s", e)
171:    else:
172:        g = grammar.Grammar()
173:        g.load(gp)
174:    return g
175:
176:
177:def _newer(a, b):
178:    """Inquire whether file a was written since file b."""
179:    if not os.path.exists(a):
180:        return False
181:    if not os.path.exists(b):
182:        return True
183:    return os.path.getmtime(a) >= os.path.getmtime(b)
184:
185:
186:def load_packaged_grammar(package, grammar_source, cache_dir=None):
187:    """Normally, loads a pickled grammar by doing
188:        pkgutil.get_data(package, pickled_grammar)
189:    where *pickled_grammar* is computed from *grammar_source* by adding the
190:    Python version and using a ``.pickle`` extension.
191:
192:    However, if *grammar_source* is an extant file, load_grammar(grammar_source)
193:    is called instead. This facilitates using a packaged grammar file when needed
194:    but preserves load_grammar's automatic regeneration behavior when possible.
195:
196:    """
197:    if os.path.isfile(grammar_source):
198:        gp = _generate_pickle_name(grammar_source, cache_dir) if cache_dir else None
199:        return load_grammar(grammar_source, gp=gp)
200:    pickled_name = _generate_pickle_name(os.path.basename(grammar_source), cache_dir)

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: In blib2to3/pgen2/driver.py:parse_tokens(), lines 61-67 explicitly handle tokenize.COMMENT and tokenize.NL by appending value to prefix and continuing, ensuring they are skipped from parsing and included in prefix. This allows subsequent _partially_consume_prefix() (lines 117-142) to process comment indentation appropriately during DEDENT handling.


Todo item 4 of 8 is correct
description: 'Validate that comment and NL tokens are added to the prefix and skipped in parse_tokens: check that when type in (tokenize.COMMENT, tokenize.NL), the code appends to `prefix` and continues, so that comment indentation is processed by _partially_consume_prefix.'
Validating todo item 5 of 8
description: 'Search the codebase for other instances in the root blib2to3/pgen2 module that might still treat tabs as width 4 in indentation calculations and confirm none exist (e.g., no `current_column += 4` for '\t' in the root driver implementation).'
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "(?i)\\\\t", "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2" ]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "current_column", "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2" ]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "\\+=\\s*4", "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2" ]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "tabsize", "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py" ]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "(?i).*\\\\t.*4|4.*\\\\t.*",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "(?i)indent", "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2" ]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "column\\s*([+\\-*/%]|//=|\\+=|-=|\\*=|/=|//=|%=).*[4]|4.*column",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "char\\s*==\\s*'\\\\t'.*current_column\\s*\\+=\\s*4",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "current_column\\s*\\+=",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2"
]
Calling tool: grepTool
(?i)\\t
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:56:Whitespace = r'[ \f\t]*'
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:147:    print("%d,%d-%d,%d:\t%s\t%s" % \
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:229:cookie_re = re.compile(r'^[ \t\f]*#.*?coding[:=][ \t]*([-\w.]+)', re.ASCII)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:230:blank_re = re.compile(br'^[ \t\f]*(?:[#\r\n]|$)', re.ASCII)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:399:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:134:            elif char in ' \t':
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/conv.py:228:        assert line == "\tdfas,\n", (lineno, line)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/literals.py:13:                  "t": "\t",

Calling tool: grepTool
current_column
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:120:        current_column = 0
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:126:                    if current_line.strip() and current_column < column:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:132:                    current_column = 0
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:135:                current_column += 1
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:138:                current_column = 0

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
100:        tokens = tokenize.generate_tokens(stream.readline)
101:        return self.parse_tokens(tokens, debug)
102:
103:    def parse_stream(self, stream, debug=False):
104:        """Parse a stream and return the syntax tree."""
105:        return self.parse_stream_raw(stream, debug)
106:
107:    def parse_file(self, filename, encoding=None, debug=False):
108:        """Parse a file and return the syntax tree."""
109:        with io.open(filename, "r", encoding=encoding) as stream:
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char in ' \t':
135:                current_column += 1
136:            elif char == '\n':
137:                # unexpected empty line
138:                current_column = 0
139:            else:
140:                # indent is finished
141:                wait_for_nl = True
142:        return ''.join(lines), current_line
143:
144:
145:def _generate_pickle_name(gt, cache_dir=None):
146:    head, tail = os.path.splitext(gt)
147:    if tail == ".txt":
148:        tail = ""
149:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
150:    if cache_dir:
151:        return os.path.join(cache_dir, os.path.basename(name))
152:    else:
153:        return name
154:
155:
156:def load_grammar(gt="Grammar.txt", gp=None,
157:                 save=True, force=False, logger=None):
158:    """Load the grammar (maybe from a pickle)."""
159:    if logger is None:
160:        logger = logging.getLogger()
161:    gp = _generate_pickle_name(gt) if gp is None else gp
162:    if force or not _newer(gp, gt):
163:        logger.info("Generating grammar tables from %s", gt)
164:        g = pgen.generate_grammar(gt)
165:        if save:
166:            logger.info("Writing grammar tables to %s", gp)
167:            try:
168:                g.dump(gp)
169:            except OSError as e:
170:                logger.info("Writing failed: %s", e)
171:    else:
172:        g = grammar.Grammar()
173:        g.load(gp)
174:    return g
175:
176:
177:def _newer(a, b):
178:    """Inquire whether file a was written since file b."""
179:    if not os.path.exists(a):

Calling tool: grepTool
\+=\s*4
No files found. Call grepTool again with a more generic pattern.

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py
360:    stashed = None
361:    async_def = False
362:    async_def_indent = 0
363:    async_def_nl = False
364:
365:    while 1:                                   # loop over lines in stream
366:        try:
367:            line = readline()
368:        except StopIteration:
369:            line = ''
370:        lnum = lnum + 1
371:        pos, max = 0, len(line)
372:
373:        if contstr:                            # continued string
374:            if not line:
375:                raise TokenError("EOF in multi-line string", strstart)
376:            endmatch = endprog.match(line)
377:            if endmatch:
378:                pos = end = endmatch.end(0)
379:                yield (STRING, contstr + line[:end],
380:                       strstart, (lnum, end), contline + line)
381:                contstr, needcont = '', 0
382:                contline = None
383:            elif needcont and line[-2:] != '\\\n' and line[-3:] != '\\\r\n':
384:                yield (ERRORTOKEN, contstr + line,
385:                           strstart, (lnum, len(line)), contline)
386:                contstr = ''
387:                contline = None
388:                continue
389:            else:
390:                contstr = contstr + line
391:                contline = contline + line
392:                continue
393:
394:        elif parenlev == 0 and not continued:  # new statement
395:            if not line: break
396:            column = 0
397:            while pos < max:                   # measure leading whitespace
398:                if line[pos] == ' ': column = column + 1
399:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize
400:                elif line[pos] == '\f': column = 0
401:                else: break
402:                pos = pos + 1
403:            if pos == max: break
404:
405:            if stashed:
406:                yield stashed
407:                stashed = None
408:
409:            if line[pos] in '\r\n':            # skip blank lines
410:                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)
411:                continue
412:
413:            if line[pos] == '#':               # skip comments
414:                comment_token = line[pos:].rstrip('\r\n')
415:                nl_pos = pos + len(comment_token)
416:                yield (COMMENT, comment_token,
417:                        (lnum, pos), (lnum, pos + len(comment_token)), line)
418:                yield (NL, line[nl_pos:],
419:                        (lnum, nl_pos), (lnum, len(line)), line)
420:                continue
421:
422:            if column > indents[-1]:           # count indents
423:                indents.append(column)
424:                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)
425:
426:            while column < indents[-1]:        # count dedents
427:                if column not in indents:
428:                    raise IndentationError(
429:                        "unindent does not match any outer indentation level",
430:                        ("<tokenize>", lnum, pos, line))
431:                indents = indents[:-1]
432:
433:                if async_def and async_def_indent >= indents[-1]:
434:                    async_def = False
435:                    async_def_nl = False
436:                    async_def_indent = 0
437:
438:                yield (DEDENT, '', (lnum, pos), (lnum, pos), line)
439:

Calling tool: grepTool
tabsize
Matched content:
138:tabsize = 8
399:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize

Calling tool: grepTool
(?i).*\\t.*4|4.*\\t.*
No files found. Call grepTool again with a more generic pattern.

Calling tool: grepTool
(?i)indent
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:203:        indents = []
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:216:            if toknum == INDENT:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:217:                indents.append(tokval)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:220:                indents.pop()
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:224:            elif startline and indents:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:225:                toks_append(indents[-1])
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:357:    indents = [0]
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:362:    async_def_indent = 0
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:422:            if column > indents[-1]:           # count indents
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:423:                indents.append(column)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:424:                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:426:            while column < indents[-1]:        # count dedents
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:427:                if column not in indents:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:428:                    raise IndentationError(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:429:                        "unindent does not match any outer indentation level",
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:431:                indents = indents[:-1]
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:433:                if async_def and async_def_indent >= indents[-1]:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:436:                    async_def_indent = 0
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:440:            if async_def and async_def_nl and async_def_indent >= indents[-1]:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:443:                async_def_indent = 0
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:526:                                async_def_indent = indents[-1]
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:561:    for indent in indents[1:]:                 # pop remaining indent levels
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:46:        indent_columns = []
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:73:            if type == token.INDENT:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:74:                indent_columns.append(len(value))
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:79:                _indent_col = indent_columns.pop()
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:80:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:86:            if type in {token.INDENT, token.DEDENT}:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:140:                # indent is finished
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/token.pyi:11:INDENT: int
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/token.py:12:INDENT = 5

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Modifications:
5:# Copyright 2006 Google, Inc. All Rights Reserved.
6:# Licensed to PSF under a Contributor Agreement.
7:
8:"""Parser driver.
9:
10:This provides a high-level interface to parse a file into a syntax tree.
11:
12:"""
13:
14:__author__ = "Guido van Rossum <guido@python.org>"
15:
16:__all__ = ["Driver", "load_grammar"]
17:
18:# Python imports
19:import codecs
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(self, grammar, convert=None, logger=None):
33:        self.grammar = grammar
34:        if logger is None:
35:            logger = logging.getLogger()
36:        self.logger = logger
37:        self.convert = convert
38:
39:    def parse_tokens(self, tokens, debug=False):
40:        """Parse a series of tokens and return the syntax tree."""
41:        # XXX Move the prefix computation into a wrapper around tokenize.
42:        p = parse.Parser(self.grammar, self.convert)
43:        p.setup()
44:        lineno = 1
45:        column = 0
46:        indent_columns = []
47:        type = value = start = end = line_text = None
48:        prefix = ""
49:        for quintuple in tokens:
50:            type, value, start, end, line_text = quintuple
51:            if start != (lineno, column):
52:                assert (lineno, column) <= start, ((lineno, column), start)
53:                s_lineno, s_column = start
54:                if lineno < s_lineno:
55:                    prefix += "\n" * (s_lineno - lineno)
56:                    lineno = s_lineno
57:                    column = 0
58:                if column < s_column:
59:                    prefix += line_text[column:s_column]
60:                    column = s_column
61:            if type in (tokenize.COMMENT, tokenize.NL):
62:                prefix += value
63:                lineno, column = end
64:                if value.endswith("\n"):
65:                    lineno += 1
66:                    column = 0
67:                continue
68:            if type == token.OP:
69:                type = grammar.opmap[value]
70:            if debug:
71:                self.logger.debug("%s %r (prefix=%r)",
72:                                  token.tok_name[type], value, prefix)
73:            if type == token.INDENT:
74:                indent_columns.append(len(value))
75:                _prefix = prefix + value
76:                prefix = ""
77:                value = ""
78:            elif type == token.DEDENT:
79:                _indent_col = indent_columns.pop()
80:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
81:            if p.addtoken(type, value, (prefix, start)):
82:                if debug:
83:                    self.logger.debug("Stop.")
84:                break
85:            prefix = ""
86:            if type in {token.INDENT, token.DEDENT}:
87:                prefix = _prefix
88:            lineno, column = end
89:            if value.endswith("\n"):
90:                lineno += 1
91:                column = 0
92:        else:
93:            # We never broke out -- EOF is too soon (how can this happen???)
94:            raise parse.ParseError("incomplete input",
95:                                   type, value, (prefix, start))
96:        return p.rootnode
97:
98:    def parse_stream_raw(self, stream, debug=False):
99:        """Parse a stream and return the syntax tree."""
100:        tokens = tokenize.generate_tokens(stream.readline)
101:        return self.parse_tokens(tokens, debug)
102:
103:    def parse_stream(self, stream, debug=False):
104:        """Parse a stream and return the syntax tree."""
105:        return self.parse_stream_raw(stream, debug)
106:
107:    def parse_file(self, filename, encoding=None, debug=False):
108:        """Parse a file and return the syntax tree."""
109:        with io.open(filename, "r", encoding=encoding) as stream:
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0

Calling tool: globTool
**/*
Matched files:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/grammar.pyi
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/parse.py
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/literals.pyi
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/conv.py
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.pyi
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/token.pyi
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/grammar.py
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/literals.py
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/__pycache__/parse.cpython-38.pyc
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/__pycache__/pgen.cpython-38.pyc
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/__pycache__/grammar.cpython-38.pyc
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/__pycache__/token.cpython-38.pyc
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/__pycache__/tokenize.cpython-38.pyc
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/__pycache__/driver.cpython-38.pyc
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/__pycache__/__init__.cpython-38.pyc
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/__init__.py
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.pyi
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/parse.pyi
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/pgen.py
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/pgen.pyi
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/__init__.pyi
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/token.py

Calling tool: grepTool
column\s*([+\-*/%]|//=|\+=|-=|\*=|/=|//=|%=).*[4]|4.*column
No files found. Call grepTool again with a more generic pattern.

Calling tool: grepTool
char\s*==\s*'\\t'.*current_column\s*\+=\s*4
No files found. Call grepTool again with a more generic pattern.

Calling tool: grepTool
current_column\s*\+=
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:135:                current_column += 1

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: Searched blib2to3/pgen2 for any handling of tabs as width 4. In driver.py, _partially_consume_prefix now treats both space and tab identically (current_column += 1). Grep found no occurrences of current_column += 4 or similar. Other references (tokenize.py) use tabsize=8, not 4, and conv/literals only contain string literals. Thus no remaining 4-width tab handling exists in the module.


Todo item 5 of 8 is correct
description: 'Search the codebase for other instances in the root blib2to3/pgen2 module that might still treat tabs as width 4 in indentation calculations and confirm none exist (e.g., no `current_column += 4` for '\t' in the root driver implementation).'
Validating todo item 6 of 8
description: 'Identify duplicate copies of driver.py (e.g., under env/src/black/blib2to3/pgen2/driver.py) and verify whether the version used at runtime by black imports the updated root file. If the env copy is not used, note it; if it could be imported, ensure it is updated or that import paths favor the updated implementation.'
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "blib2to3\\.pgen2\\.driver|pgen2\\.driver|from blib2to3|import blib2to3",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "elif char == '\\\\t'|elif char in ' \\\\t'|current_column \\+=",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "_partially_consume_prefix|elif char in ' \\\\t'|elif char == '\\\\t'",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py"
]
[ "rg", "--no-heading", "--line-number", "--ignore-file", ".rgignore", "sys\\.path|PYTHONPATH|insert\\(0",
  "/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black"
]
Calling tool: globTool
**/driver.py
Matched files:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py

Calling tool: grepTool
blib2to3\.pgen2\.driver|pgen2\.driver|from blib2to3|import blib2to3
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black.py:45:from blib2to3.pytree import Node, Leaf, type_repr
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black.py:46:from blib2to3 import pygram, pytree
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black.py:47:from blib2to3.pgen2 import driver, token
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black.py:48:from blib2to3.pgen2.parse import ParseError
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.pyi:6:from blib2to3.pgen2.grammar import Grammar
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:34:from blib2to3.pgen2.token import *
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.pyi:5:from blib2to3.pgen2.token import *  # noqa
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/parse.pyi:5:from blib2to3.pgen2.grammar import Grammar, _DFAS
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/parse.pyi:6:from blib2to3.pytree import _NL, _Convert, _RawNode
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/grammar.pyi:3:from blib2to3.pgen2 import _Path
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/pgen.pyi:6:from blib2to3.pgen2 import _Path, grammar
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/pgen.pyi:7:from blib2to3.pgen2.tokenize import _TokenInfo
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pygram.pyi:5:from blib2to3.pgen2.grammar import Grammar
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.pyi:1:# Stubs for lib2to3.pgen2.driver (Python 3.6)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.pyi:8:from blib2to3.pytree import _Convert, _NL
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.pyi:9:from blib2to3.pgen2 import _Path
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.pyi:10:from blib2to3.pgen2.grammar import Grammar
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/parse.pyi:5:from blib2to3.pgen2.grammar import Grammar, _DFAS
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/parse.pyi:6:from blib2to3.pytree import _NL, _Convert, _RawNode
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.pyi:1:# Stubs for lib2to3.pgen2.driver (Python 3.6)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.pyi:8:from blib2to3.pytree import _Convert, _NL
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.pyi:9:from blib2to3.pgen2 import _Path
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.pyi:10:from blib2to3.pgen2.grammar import Grammar
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/tokenize.py:34:from blib2to3.pgen2.token import *
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/pgen.pyi:6:from blib2to3.pgen2 import _Path, grammar
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/pgen.pyi:7:from blib2to3.pgen2.tokenize import _TokenInfo
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/grammar.pyi:3:from blib2to3.pgen2 import _Path
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pygram.pyi:5:from blib2to3.pgen2.grammar import Grammar
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pytree.pyi:6:from blib2to3.pgen2.grammar import Grammar
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/tokenize.pyi:5:from blib2to3.pgen2.token import *  # noqa
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/black.py:45:from blib2to3.pytree import Node, Leaf, type_repr
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/black.py:46:from blib2to3 import pygram, pytree
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/black.py:47:from blib2to3.pgen2 import driver, token
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/black.py:48:from blib2to3.pgen2.parse import ParseError
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/2and3/lib2to3/pgen2/driver.pyi:1:# Stubs for lib2to3.pgen2.driver (Python 3.6)

Calling tool: grepTool
elif char == '\\t'|elif char in ' \\t'|current_column \+=
Matched content:
134:            elif char in ' \t':
135:                current_column += 1

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char in ' \t':
135:                current_column += 1
136:            elif char == '\n':
137:                # unexpected empty line
138:                current_column = 0
139:            else:
140:                # indent is finished
141:                wait_for_nl = True
142:        return ''.join(lines), current_line
143:
144:
145:def _generate_pickle_name(gt, cache_dir=None):
146:    head, tail = os.path.splitext(gt)
147:    if tail == ".txt":
148:        tail = ""
149:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
150:    if cache_dir:
151:        return os.path.join(cache_dir, os.path.basename(name))
152:    else:
153:        return name
154:
155:
156:def load_grammar(gt="Grammar.txt", gp=None,
157:                 save=True, force=False, logger=None):
158:    """Load the grammar (maybe from a pickle)."""
159:    if logger is None:
160:        logger = logging.getLogger()
161:    gp = _generate_pickle_name(gt) if gp is None else gp
162:    if force or not _newer(gp, gt):
163:        logger.info("Generating grammar tables from %s", gt)
164:        g = pgen.generate_grammar(gt)
165:        if save:
166:            logger.info("Writing grammar tables to %s", gp)
167:            try:
168:                g.dump(gp)
169:            except OSError as e:

Calling tool: grepTool
_partially_consume_prefix|elif char in ' \\t'|elif char == '\\t'
Matched content:
80:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
117:    def _partially_consume_prefix(self, prefix, column):
136:            elif char == '\t':

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char == ' ':
135:                current_column += 1
136:            elif char == '\t':
137:                current_column += 4
138:            elif char == '\n':
139:                # unexpected empty line
140:                current_column = 0
141:            else:
142:                # indent is finished
143:                wait_for_nl = True
144:        return ''.join(lines), current_line
145:
146:
147:def _generate_pickle_name(gt, cache_dir=None):
148:    head, tail = os.path.splitext(gt)
149:    if tail == ".txt":
150:        tail = ""
151:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
152:    if cache_dir:
153:        return os.path.join(cache_dir, os.path.basename(name))
154:    else:
155:        return name
156:
157:
158:def load_grammar(gt="Grammar.txt", gp=None,
159:                 save=True, force=False, logger=None):
160:    """Load the grammar (maybe from a pickle)."""
161:    if logger is None:
162:        logger = logging.getLogger()
163:    gp = _generate_pickle_name(gt) if gp is None else gp
164:    if force or not _newer(gp, gt):
165:        logger.info("Generating grammar tables from %s", gt)
166:        g = pgen.generate_grammar(gt)
167:        if save:
168:            logger.info("Writing grammar tables to %s", gp)
169:            try:

Calling tool: grepTool
sys\.path|PYTHONPATH|insert\(0
Showing first 50 results from grep. Consider using a more specific pattern or path.
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/plugin/black.vim:90:  if sys.path[0] != virtualenv_site_packages:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/plugin/black.vim:91:    sys.path.insert(0, virtualenv_site_packages)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/docs/conf.py:12:# add these directories to sys.path here. If the directory is relative to the
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/pgen.py:32:        names.insert(0, self.startsymbol)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/bin/mypyc:50:    env['PYTHONPATH'] = base_path + os.pathsep + env.get('PYTHONPATH', '')
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/pgen.py:32:        names.insert(0, self.startsymbol)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/docs/conf.py:12:# add these directories to sys.path here. If the directory is relative to the
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/pyflakes/test/test_api.py:709:        env['PYTHONPATH'] = os.pathsep.join(sys.path)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/plugin/black.vim:90:  if sys.path[0] != virtualenv_site_packages:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/plugin/black.vim:91:    sys.path.insert(0, virtualenv_site_packages)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/pathspec/patterns/gitwildmatch.py:103:					pattern_segs.insert(0, '**')
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/platformdirs/android.py:145:        # and if that fails, too, find an android folder looking at path on the sys.path
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/platformdirs/android.py:148:        for path in sys.path:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/platformdirs/android.py:155:        # one last try: find an android folder looking at path on the sys.path taking adopted storage paths into
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/platformdirs/android.py:158:        for path in sys.path:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/flake8/main/application.py:187:        sys.path.extend(self.local_plugins.paths)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/cffi/recompiler.py:802:            arguments.insert(0, arg)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/pkginfo/installed.py:39:                for entry in sys.path:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/cffi/_embedding.h:240:            PyFile_WriteString("\nsys.path: ", f);
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/docutils/nodes.py:1532:            self.insert(0, header())
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/aiohttp/web_urldispatcher.py:234:        self._apps.insert(0, app)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/cffi/_imp_emulation.py:53:                path = sys.path
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/docutils/transforms/parts.py:63:                title.insert(0, generated)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/cffi/vengine_gen.py:31:                path = sys.path
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/cffi/vengine_gen.py:161:            arglist.insert(0, tpresult.get_c_name(' *r', context))
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/cffi/vengine_gen.py:209:                    indirect_args.insert(0, indirect_result)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/cffi/vengine_gen.py:210:                    indirections.insert(0, ("result", indirect_result))
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/docutils/transforms/writer_aux.py:87:                admonition.insert(0, title)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/pkginfo/tests/test_installed.py:108:        oldpath = sys.path[:]
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/pkginfo/tests/test_installed.py:110:            sys.path.append(wonky)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/pkginfo/tests/test_installed.py:117:            sys.path[:] = oldpath
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/pkginfo/tests/test_installed.py:127:        oldpath = sys.path[:]
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/pkginfo/tests/test_installed.py:129:            sys.path.append(manky)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/pkginfo/tests/test_installed.py:136:            sys.path[:] = oldpath
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/docutils/transforms/references.py:512:            footnote.insert(0, nodes.label('', label))
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/docutils/transforms/references.py:569:            footnote.insert(0, nodes.label('', labeltext))
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/docutils/transforms/references.py:824:                    reflist.insert(0, nodes.inline(text=' ', Classes=self.classes))
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/docutils/transforms/references.py:826:                    reflist.insert(0, nodes.Text(' '))
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/distlib/wheel.py:745:        if pathname in sys.path:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/distlib/wheel.py:749:                sys.path.append(pathname)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/distlib/wheel.py:751:                sys.path.insert(0, pathname)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/distlib/wheel.py:760:        if pathname not in sys.path:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/distlib/wheel.py:763:            sys.path.remove(pathname)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/distlib/wheel.py:942:        abis.insert(0, ABI)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/mypyc/genexpr.py:186:                arg_kinds.insert(0, ARG_POS)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/mypyc/genexpr.py:187:                arg_names.insert(0, None)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/mypyc/genexpr.py:254:            arg_values.insert(0, vself)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/mypyc/genexpr.py:255:            arg_kinds.insert(0, ARG_POS)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/mypyc/genexpr.py:256:            arg_names.insert(0, None)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/pip/__main__.py:6:# of sys.path, if present to avoid using current directory

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as INCORRECT
Reason: Two copies of driver.py exist: root blib2to3/pgen2/driver.py (updated with "elif char in ' \t': current_column += 1") and env/src/black/blib2to3/pgen2/driver.py (still has the old tab logic adding 4). Black imports from "blib2to3.pgen2 import driver" (top-level), so at runtime it may import the stale env copy depending on sys.path. No changes ensure import paths favor the updated root file or update the env copy.


Validating todo item 1 of 8
description: 'Confirm that in blib2to3/pgen2/driver.py, _partially_consume_prefix counts both spaces and tabs as width 1 by checking for `elif char in ' \t': current_column += 1` and that there is no remaining branch setting tabs to 4 in this file.'

Calling tool: grepTool
def _partially_consume_prefix\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:117:    def _partially_consume_prefix(self, prefix, column):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py:117:    def _partially_consume_prefix(self, prefix, column):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
100:        tokens = tokenize.generate_tokens(stream.readline)
101:        return self.parse_tokens(tokens, debug)
102:
103:    def parse_stream(self, stream, debug=False):
104:        """Parse a stream and return the syntax tree."""
105:        return self.parse_stream_raw(stream, debug)
106:
107:    def parse_file(self, filename, encoding=None, debug=False):
108:        """Parse a file and return the syntax tree."""
109:        with io.open(filename, "r", encoding=encoding) as stream:
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char in ' \t':
135:                current_column += 1
136:            elif char == '\n':
137:                # unexpected empty line
138:                current_column = 0
139:            else:
140:                # indent is finished
141:                wait_for_nl = True
142:        return ''.join(lines), current_line
143:
144:
145:def _generate_pickle_name(gt, cache_dir=None):
146:    head, tail = os.path.splitext(gt)
147:    if tail == ".txt":
148:        tail = ""
149:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
150:    if cache_dir:
151:        return os.path.join(cache_dir, os.path.basename(name))
152:    else:
153:        return name
154:
155:
156:def load_grammar(gt="Grammar.txt", gp=None,
157:                 save=True, force=False, logger=None):
158:    """Load the grammar (maybe from a pickle)."""
159:    if logger is None:
160:        logger = logging.getLogger()
161:    gp = _generate_pickle_name(gt) if gp is None else gp
162:    if force or not _newer(gp, gt):
163:        logger.info("Generating grammar tables from %s", gt)
164:        g = pgen.generate_grammar(gt)
165:        if save:
166:            logger.info("Writing grammar tables to %s", gp)
167:            try:
168:                g.dump(gp)
169:            except OSError as e:
170:                logger.info("Writing failed: %s", e)
171:    else:
172:        g = grammar.Grammar()
173:        g.load(gp)
174:    return g
175:
176:
177:def _newer(a, b):
178:    """Inquire whether file a was written since file b."""
179:    if not os.path.exists(a):
180:        return False
181:    if not os.path.exists(b):
182:        return True
183:    return os.path.getmtime(a) >= os.path.getmtime(b)
184:
185:
186:def load_packaged_grammar(package, grammar_source, cache_dir=None):
187:    """Normally, loads a pickled grammar by doing
188:        pkgutil.get_data(package, pickled_grammar)
189:    where *pickled_grammar* is computed from *grammar_source* by adding the
190:    Python version and using a ``.pickle`` extension.
191:
192:    However, if *grammar_source* is an extant file, load_grammar(grammar_source)
193:    is called instead. This facilitates using a packaged grammar file when needed
194:    but preserves load_grammar's automatic regeneration behavior when possible.
195:
196:    """
197:    if os.path.isfile(grammar_source):
198:        gp = _generate_pickle_name(grammar_source, cache_dir) if cache_dir else None
199:        return load_grammar(grammar_source, gp=gp)
200:    pickled_name = _generate_pickle_name(os.path.basename(grammar_source), cache_dir)
201:    data = pkgutil.get_data(package, pickled_name)
202:    g = grammar.Grammar()
203:    g.loads(data)
204:    return g
205:
206:
207:def main(*args):
208:    """Main program, when run as a script: produce grammar pickle files.
209:
210:    Calls load_grammar for each argument, a path to a grammar text file.
211:    """
212:    if not args:
213:        args = sys.argv[1:]
214:    logging.basicConfig(level=logging.INFO, stream=sys.stdout,
215:                        format='%(message)s')
216:    for gt in args:
217:        load_grammar(gt, save=True, force=True)
218:    return True
219:

Calling tool: grepTool
\\t
Matched content:
134:            elif char in ' \t':

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py
100:        tokens = tokenize.generate_tokens(stream.readline)
101:        return self.parse_tokens(tokens, debug)
102:
103:    def parse_stream(self, stream, debug=False):
104:        """Parse a stream and return the syntax tree."""
105:        return self.parse_stream_raw(stream, debug)
106:
107:    def parse_file(self, filename, encoding=None, debug=False):
108:        """Parse a file and return the syntax tree."""
109:        with io.open(filename, "r", encoding=encoding) as stream:
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char == ' ':
135:                current_column += 1
136:            elif char == '\t':
137:                current_column += 4
138:            elif char == '\n':
139:                # unexpected empty line
140:                current_column = 0
141:            else:
142:                # indent is finished
143:                wait_for_nl = True
144:        return ''.join(lines), current_line
145:
146:
147:def _generate_pickle_name(gt, cache_dir=None):
148:    head, tail = os.path.splitext(gt)
149:    if tail == ".txt":
150:        tail = ""
151:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
152:    if cache_dir:
153:        return os.path.join(cache_dir, os.path.basename(name))
154:    else:
155:        return name
156:
157:
158:def load_grammar(gt="Grammar.txt", gp=None,
159:                 save=True, force=False, logger=None):
160:    """Load the grammar (maybe from a pickle)."""
161:    if logger is None:
162:        logger = logging.getLogger()
163:    gp = _generate_pickle_name(gt) if gp is None else gp
164:    if force or not _newer(gp, gt):
165:        logger.info("Generating grammar tables from %s", gt)
166:        g = pgen.generate_grammar(gt)
167:        if save:
168:            logger.info("Writing grammar tables to %s", gp)
169:            try:
170:                g.dump(gp)
171:            except OSError as e:
172:                logger.info("Writing failed: %s", e)
173:    else:
174:        g = grammar.Grammar()
175:        g.load(gp)
176:    return g
177:
178:
179:def _newer(a, b):
180:    """Inquire whether file a was written since file b."""
181:    if not os.path.exists(a):
182:        return False
183:    if not os.path.exists(b):
184:        return True
185:    return os.path.getmtime(a) >= os.path.getmtime(b)
186:
187:
188:def load_packaged_grammar(package, grammar_source, cache_dir=None):
189:    """Normally, loads a pickled grammar by doing
190:        pkgutil.get_data(package, pickled_grammar)
191:    where *pickled_grammar* is computed from *grammar_source* by adding the
192:    Python version and using a ``.pickle`` extension.
193:
194:    However, if *grammar_source* is an extant file, load_grammar(grammar_source)
195:    is called instead. This facilitates using a packaged grammar file when needed
196:    but preserves load_grammar's automatic regeneration behavior when possible.
197:
198:    """
199:    if os.path.isfile(grammar_source):
200:        gp = _generate_pickle_name(grammar_source, cache_dir) if cache_dir else None
201:        return load_grammar(grammar_source, gp=gp)
202:    pickled_name = _generate_pickle_name(os.path.basename(grammar_source), cache_dir)
203:    data = pkgutil.get_data(package, pickled_name)
204:    g = grammar.Grammar()
205:    g.loads(data)
206:    return g
207:
208:
209:def main(*args):
210:    """Main program, when run as a script: produce grammar pickle files.
211:
212:    Calls load_grammar for each argument, a path to a grammar text file.
213:    """
214:    if not args:
215:        args = sys.argv[1:]
216:    logging.basicConfig(level=logging.INFO, stream=sys.stdout,
217:                        format='%(message)s')
218:    for gt in args:
219:        load_grammar(gt, save=True, force=True)

Calling tool: grepTool
\+= 4
No files found. Call grepTool again with a more generic pattern.

Calling tool: grepTool
elif char == '\\t':
No files found. Call grepTool again with a more generic pattern.

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: Verified in blib2to3/pgen2/driver.py _partially_consume_prefix: it uses `elif char in ' \t': current_column += 1` (lines ~134-136) and there is no remaining `elif char == '\t': current_column += 4` branch or any `+= 4` occurrences in this file.

Todo item 1 of 8 is correct
description: 'Confirm that in blib2to3/pgen2/driver.py, _partially_consume_prefix counts both spaces and tabs as width 1 by checking for `elif char in ' \t': current_column += 1` and that there is no remaining branch setting tabs to 4 in this file.'

Validating todo item 2 of 8
description: 'Verify that parse_tokens passes the correct indent column to _partially_consume_prefix: on INDENT, it appends len(value) to indent_columns; on DEDENT, it pops that value and calls _partially_consume_prefix(prefix, _indent_col). Ensure this uses character count (len) consistent with the updated tab=1 logic.'

Calling tool: grepTool
def parse_tokens\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:39:    def parse_tokens(self, tokens, debug=False):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.pyi:18:    def parse_tokens(self, tokens: Iterable[Any], debug: bool = ...) -> _NL: ...
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py:39:    def parse_tokens(self, tokens, debug=False):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.pyi:18:    def parse_tokens(self, tokens: Iterable[Any], debug: bool = ...) -> _NL: ...
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/2and3/lib2to3/pgen2/driver.pyi:18:    def parse_tokens(self, tokens: Iterable[Any], debug: bool = ...) -> _NL: ...

Calling tool: grepTool
_partially_consume_prefix\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:80:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:117:    def _partially_consume_prefix(self, prefix, column):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py:80:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py:117:    def _partially_consume_prefix(self, prefix, column):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Modifications:
5:# Copyright 2006 Google, Inc. All Rights Reserved.
6:# Licensed to PSF under a Contributor Agreement.
7:
8:"""Parser driver.
9:
10:This provides a high-level interface to parse a file into a syntax tree.
11:
12:"""
13:
14:__author__ = "Guido van Rossum <guido@python.org>"
15:
16:__all__ = ["Driver", "load_grammar"]
17:
18:# Python imports
19:import codecs
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(self, grammar, convert=None, logger=None):
33:        self.grammar = grammar
34:        if logger is None:
35:            logger = logging.getLogger()
36:        self.logger = logger
37:        self.convert = convert
38:
39:    def parse_tokens(self, tokens, debug=False):
40:        """Parse a series of tokens and return the syntax tree."""
41:        # XXX Move the prefix computation into a wrapper around tokenize.
42:        p = parse.Parser(self.grammar, self.convert)
43:        p.setup()
44:        lineno = 1
45:        column = 0
46:        indent_columns = []
47:        type = value = start = end = line_text = None
48:        prefix = ""
49:        for quintuple in tokens:
50:            type, value, start, end, line_text = quintuple
51:            if start != (lineno, column):
52:                assert (lineno, column) <= start, ((lineno, column), start)
53:                s_lineno, s_column = start
54:                if lineno < s_lineno:
55:                    prefix += "\n" * (s_lineno - lineno)
56:                    lineno = s_lineno
57:                    column = 0
58:                if column < s_column:
59:                    prefix += line_text[column:s_column]
60:                    column = s_column
61:            if type in (tokenize.COMMENT, tokenize.NL):
62:                prefix += value
63:                lineno, column = end
64:                if value.endswith("\n"):
65:                    lineno += 1
66:                    column = 0
67:                continue
68:            if type == token.OP:
69:                type = grammar.opmap[value]
70:            if debug:
71:                self.logger.debug("%s %r (prefix=%r)",
72:                                  token.tok_name[type], value, prefix)
73:            if type == token.INDENT:
74:                indent_columns.append(len(value))
75:                _prefix = prefix + value
76:                prefix = ""
77:                value = ""
78:            elif type == token.DEDENT:
79:                _indent_col = indent_columns.pop()
80:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
81:            if p.addtoken(type, value, (prefix, start)):
82:                if debug:
83:                    self.logger.debug("Stop.")
84:                break
85:            prefix = ""
86:            if type in {token.INDENT, token.DEDENT}:
87:                prefix = _prefix
88:            lineno, column = end
89:            if value.endswith("\n"):
90:                lineno += 1
91:                column = 0
92:        else:
93:            # We never broke out -- EOF is too soon (how can this happen???)
94:            raise parse.ParseError("incomplete input",
95:                                   type, value, (prefix, start))
96:        return p.rootnode
97:
98:    def parse_stream_raw(self, stream, debug=False):
99:        """Parse a stream and return the syntax tree."""
100:        tokens = tokenize.generate_tokens(stream.readline)
101:        return self.parse_tokens(tokens, debug)
102:
103:    def parse_stream(self, stream, debug=False):
104:        """Parse a stream and return the syntax tree."""
105:        return self.parse_stream_raw(stream, debug)
106:
107:    def parse_file(self, filename, encoding=None, debug=False):
108:        """Parse a file and return the syntax tree."""
109:        with io.open(filename, "r", encoding=encoding) as stream:
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char in ' \t':
135:                current_column += 1
136:            elif char == '\n':
137:                # unexpected empty line
138:                current_column = 0
139:            else:
140:                # indent is finished
141:                wait_for_nl = True
142:        return ''.join(lines), current_line
143:
144:
145:def _generate_pickle_name(gt, cache_dir=None):
146:    head, tail = os.path.splitext(gt)
147:    if tail == ".txt":
148:        tail = ""
149:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
150:    if cache_dir:
151:        return os.path.join(cache_dir, os.path.basename(name))
152:    else:
153:        return name
154:
155:
156:def load_grammar(gt="Grammar.txt", gp=None,
157:                 save=True, force=False, logger=None):
158:    """Load the grammar (maybe from a pickle)."""
159:    if logger is None:
160:        logger = logging.getLogger()
161:    gp = _generate_pickle_name(gt) if gp is None else gp
162:    if force or not _newer(gp, gt):
163:        logger.info("Generating grammar tables from %s", gt)
164:        g = pgen.generate_grammar(gt)
165:        if save:
166:            logger.info("Writing grammar tables to %s", gp)
167:            try:
168:                g.dump(gp)
169:            except OSError as e:
170:                logger.info("Writing failed: %s", e)
171:    else:
172:        g = grammar.Grammar()
173:        g.load(gp)
174:    return g
175:
176:
177:def _newer(a, b):
178:    """Inquire whether file a was written since file b."""
179:    if not os.path.exists(a):
180:        return False

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: In parse_tokens, INDENT appends len(value) to indent_columns (line 74) and on DEDENT it pops that value and calls _partially_consume_prefix(prefix, _indent_col) (lines 79-81). _partially_consume_prefix now counts both spaces and tabs as 1 column (line 134), so the character-count (len) matches the updated tab=1 logic.

Todo item 2 of 8 is correct
description: 'Verify that parse_tokens passes the correct indent column to _partially_consume_prefix: on INDENT, it appends len(value) to indent_columns; on DEDENT, it pops that value and calls _partially_consume_prefix(prefix, _indent_col). Ensure this uses character count (len) consistent with the updated tab=1 logic.'

Validating todo item 3 of 8
description: 'Ensure DEDENT handling preserves and reattaches the split prefix properly: after calling _partially_consume_prefix, confirm that `prefix` is set to the returned consumed portion and `_prefix` holds the remainder, and that after `p.addtoken`, when type is INDENT or DEDENT, `prefix = _prefix` is executed.'

Calling tool: grepTool
_partially_consume_prefix\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:80:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:117:    def _partially_consume_prefix(self, prefix, column):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py:80:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py:117:    def _partially_consume_prefix(self, prefix, column):

Calling tool: grepTool
p\.addtoken\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/parse.py:35:        if p.addtoken(...):           # parse a token; may raise ParseError
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py:81:            if p.addtoken(type, value, (prefix, start)):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:81:            if p.addtoken(type, value, (prefix, start)):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/parse.py:35:        if p.addtoken(...):           # parse a token; may raise ParseError

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Modifications:
5:# Copyright 2006 Google, Inc. All Rights Reserved.
6:# Licensed to PSF under a Contributor Agreement.
7:
8:"""Parser driver.
9:
10:This provides a high-level interface to parse a file into a syntax tree.
11:
12:"""
13:
14:__author__ = "Guido van Rossum <guido@python.org>"
15:
16:__all__ = ["Driver", "load_grammar"]
17:
18:# Python imports
19:import codecs
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(self, grammar, convert=None, logger=None):
33:        self.grammar = grammar
34:        if logger is None:
35:            logger = logging.getLogger()
36:        self.logger = logger
37:        self.convert = convert
38:
39:    def parse_tokens(self, tokens, debug=False):
40:        """Parse a series of tokens and return the syntax tree."""
41:        # XXX Move the prefix computation into a wrapper around tokenize.
42:        p = parse.Parser(self.grammar, self.convert)
43:        p.setup()
44:        lineno = 1
45:        column = 0
46:        indent_columns = []
47:        type = value = start = end = line_text = None
48:        prefix = ""
49:        for quintuple in tokens:
50:            type, value, start, end, line_text = quintuple
51:            if start != (lineno, column):
52:                assert (lineno, column) <= start, ((lineno, column), start)
53:                s_lineno, s_column = start
54:                if lineno < s_lineno:
55:                    prefix += "\n" * (s_lineno - lineno)
56:                    lineno = s_lineno
57:                    column = 0
58:                if column < s_column:
59:                    prefix += line_text[column:s_column]
60:                    column = s_column
61:            if type in (tokenize.COMMENT, tokenize.NL):
62:                prefix += value
63:                lineno, column = end
64:                if value.endswith("\n"):
65:                    lineno += 1
66:                    column = 0
67:                continue
68:            if type == token.OP:
69:                type = grammar.opmap[value]
70:            if debug:
71:                self.logger.debug("%s %r (prefix=%r)",
72:                                  token.tok_name[type], value, prefix)
73:            if type == token.INDENT:
74:                indent_columns.append(len(value))
75:                _prefix = prefix + value
76:                prefix = ""
77:                value = ""
78:            elif type == token.DEDENT:
79:                _indent_col = indent_columns.pop()
80:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
81:            if p.addtoken(type, value, (prefix, start)):
82:                if debug:
83:                    self.logger.debug("Stop.")
84:                break
85:            prefix = ""
86:            if type in {token.INDENT, token.DEDENT}:
87:                prefix = _prefix
88:            lineno, column = end
89:            if value.endswith("\n"):
90:                lineno += 1
91:                column = 0
92:        else:
93:            # We never broke out -- EOF is too soon (how can this happen???)
94:            raise parse.ParseError("incomplete input",
95:                                   type, value, (prefix, start))
96:        return p.rootnode
97:
98:    def parse_stream_raw(self, stream, debug=False):
99:        """Parse a stream and return the syntax tree."""
100:        tokens = tokenize.generate_tokens(stream.readline)
101:        return self.parse_tokens(tokens, debug)
102:
103:    def parse_stream(self, stream, debug=False):
104:        """Parse a stream and return the syntax tree."""
105:        return self.parse_stream_raw(stream, debug)
106:
107:    def parse_file(self, filename, encoding=None, debug=False):
108:        """Parse a file and return the syntax tree."""
109:        with io.open(filename, "r", encoding=encoding) as stream:
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char in ' \t':
135:                current_column += 1
136:            elif char == '\n':
137:                # unexpected empty line
138:                current_column = 0
139:            else:
140:                # indent is finished
141:                wait_for_nl = True
142:        return ''.join(lines), current_line
143:
144:
145:def _generate_pickle_name(gt, cache_dir=None):
146:    head, tail = os.path.splitext(gt)
147:    if tail == ".txt":
148:        tail = ""
149:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
150:    if cache_dir:
151:        return os.path.join(cache_dir, os.path.basename(name))
152:    else:
153:        return name
154:
155:
156:def load_grammar(gt="Grammar.txt", gp=None,
157:                 save=True, force=False, logger=None):
158:    """Load the grammar (maybe from a pickle)."""
159:    if logger is None:
160:        logger = logging.getLogger()

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py
60:                    column = s_column
61:            if type in (tokenize.COMMENT, tokenize.NL):
62:                prefix += value
63:                lineno, column = end
64:                if value.endswith("\n"):
65:                    lineno += 1
66:                    column = 0
67:                continue
68:            if type == token.OP:
69:                type = grammar.opmap[value]
70:            if debug:
71:                self.logger.debug("%s %r (prefix=%r)",
72:                                  token.tok_name[type], value, prefix)
73:            if type == token.INDENT:
74:                indent_columns.append(len(value))
75:                _prefix = prefix + value
76:                prefix = ""
77:                value = ""
78:            elif type == token.DEDENT:
79:                _indent_col = indent_columns.pop()
80:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
81:            if p.addtoken(type, value, (prefix, start)):
82:                if debug:
83:                    self.logger.debug("Stop.")
84:                break
85:            prefix = ""
86:            if type in {token.INDENT, token.DEDENT}:
87:                prefix = _prefix
88:            lineno, column = end
89:            if value.endswith("\n"):
90:                lineno += 1
91:                column = 0
92:        else:
93:            # We never broke out -- EOF is too soon (how can this happen???)
94:            raise parse.ParseError("incomplete input",
95:                                   type, value, (prefix, start))
96:        return p.rootnode
97:
98:    def parse_stream_raw(self, stream, debug=False):
99:        """Parse a stream and return the syntax tree."""
100:        tokens = tokenize.generate_tokens(stream.readline)
101:        return self.parse_tokens(tokens, debug)
102:
103:    def parse_stream(self, stream, debug=False):
104:        """Parse a stream and return the syntax tree."""
105:        return self.parse_stream_raw(stream, debug)
106:
107:    def parse_file(self, filename, encoding=None, debug=False):
108:        """Parse a file and return the syntax tree."""
109:        with io.open(filename, "r", encoding=encoding) as stream:
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char == ' ':
135:                current_column += 1
136:            elif char == '\t':
137:                current_column += 4
138:            elif char == '\n':
139:                # unexpected empty line
140:                current_column = 0
141:            else:
142:                # indent is finished
143:                wait_for_nl = True
144:        return ''.join(lines), current_line
145:
146:
147:def _generate_pickle_name(gt, cache_dir=None):
148:    head, tail = os.path.splitext(gt)
149:    if tail == ".txt":
150:        tail = ""
151:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
152:    if cache_dir:
153:        return os.path.join(cache_dir, os.path.basename(name))
154:    else:
155:        return name
156:
157:
158:def load_grammar(gt="Grammar.txt", gp=None,
159:                 save=True, force=False, logger=None):
160:    """Load the grammar (maybe from a pickle)."""
161:    if logger is None:
162:        logger = logging.getLogger()
163:    gp = _generate_pickle_name(gt) if gp is None else gp
164:    if force or not _newer(gp, gt):
165:        logger.info("Generating grammar tables from %s", gt)
166:        g = pgen.generate_grammar(gt)
167:        if save:
168:            logger.info("Writing grammar tables to %s", gp)
169:            try:
170:                g.dump(gp)
171:            except OSError as e:
172:                logger.info("Writing failed: %s", e)
173:    else:
174:        g = grammar.Grammar()
175:        g.load(gp)
176:    return g

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: In blib2to3/pgen2/driver.py, within parse_tokens: on DEDENT, it assigns `prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)` (lines ~79-80), so `prefix` gets the consumed part and `_prefix` the remainder. After `p.addtoken(...)`, it executes `if type in {token.INDENT, token.DEDENT}: prefix = _prefix` (lines ~85-87), reattaching the remainder as required.

Todo item 3 of 8 is correct
description: 'Ensure DEDENT handling preserves and reattaches the split prefix properly: after calling _partially_consume_prefix, confirm that `prefix` is set to the returned consumed portion and `_prefix` holds the remainder, and that after `p.addtoken`, when type is INDENT or DEDENT, `prefix = _prefix` is executed.'

Validating todo item 4 of 8
description: 'Validate that comment and NL tokens are added to the prefix and skipped in parse_tokens: check that when type in (tokenize.COMMENT, tokenize.NL), the code appends to `prefix` and continues, so that comment indentation is processed by _partially_consume_prefix.'

Calling tool: grepTool
class Driver
Grep command failed with exit code 2: rg: .rgignore: No such file or directory (os error 2)
rg: /Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/{{FOLDER_PATH}}: IO error for operation on /Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/{{FOLDER_PATH}}: No such file or directory (os error 2)


Calling tool: grepTool
parse_tokens\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black.py:2008:    in `pgen2/driver.py:Driver.parse_tokens()`.  This was a brilliant implementation
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:39:    def parse_tokens(self, tokens, debug=False):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:101:        return self.parse_tokens(tokens, debug)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:115:        return self.parse_tokens(tokens, debug)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.pyi:18:    def parse_tokens(self, tokens: Iterable[Any], debug: bool = ...) -> _NL: ...
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py:39:    def parse_tokens(self, tokens, debug=False):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py:101:        return self.parse_tokens(tokens, debug)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py:115:        return self.parse_tokens(tokens, debug)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.pyi:18:    def parse_tokens(self, tokens: Iterable[Any], debug: bool = ...) -> _NL: ...
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/black.py:2008:    in `pgen2/driver.py:Driver.parse_tokens()`.  This was a brilliant implementation
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/2and3/lib2to3/pgen2/driver.pyi:18:    def parse_tokens(self, tokens: Iterable[Any], debug: bool = ...) -> _NL: ...

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Modifications:
5:# Copyright 2006 Google, Inc. All Rights Reserved.
6:# Licensed to PSF under a Contributor Agreement.
7:
8:"""Parser driver.
9:
10:This provides a high-level interface to parse a file into a syntax tree.
11:
12:"""
13:
14:__author__ = "Guido van Rossum <guido@python.org>"
15:
16:__all__ = ["Driver", "load_grammar"]
17:
18:# Python imports
19:import codecs
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(self, grammar, convert=None, logger=None):
33:        self.grammar = grammar
34:        if logger is None:
35:            logger = logging.getLogger()
36:        self.logger = logger
37:        self.convert = convert
38:
39:    def parse_tokens(self, tokens, debug=False):
40:        """Parse a series of tokens and return the syntax tree."""
41:        # XXX Move the prefix computation into a wrapper around tokenize.
42:        p = parse.Parser(self.grammar, self.convert)
43:        p.setup()
44:        lineno = 1
45:        column = 0
46:        indent_columns = []
47:        type = value = start = end = line_text = None
48:        prefix = ""
49:        for quintuple in tokens:
50:            type, value, start, end, line_text = quintuple
51:            if start != (lineno, column):
52:                assert (lineno, column) <= start, ((lineno, column), start)
53:                s_lineno, s_column = start
54:                if lineno < s_lineno:
55:                    prefix += "\n" * (s_lineno - lineno)
56:                    lineno = s_lineno
57:                    column = 0
58:                if column < s_column:
59:                    prefix += line_text[column:s_column]
60:                    column = s_column
61:            if type in (tokenize.COMMENT, tokenize.NL):
62:                prefix += value
63:                lineno, column = end
64:                if value.endswith("\n"):
65:                    lineno += 1
66:                    column = 0
67:                continue
68:            if type == token.OP:
69:                type = grammar.opmap[value]
70:            if debug:
71:                self.logger.debug("%s %r (prefix=%r)",
72:                                  token.tok_name[type], value, prefix)
73:            if type == token.INDENT:
74:                indent_columns.append(len(value))
75:                _prefix = prefix + value
76:                prefix = ""
77:                value = ""
78:            elif type == token.DEDENT:
79:                _indent_col = indent_columns.pop()
80:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
81:            if p.addtoken(type, value, (prefix, start)):
82:                if debug:
83:                    self.logger.debug("Stop.")
84:                break
85:            prefix = ""
86:            if type in {token.INDENT, token.DEDENT}:
87:                prefix = _prefix
88:            lineno, column = end
89:            if value.endswith("\n"):
90:                lineno += 1
91:                column = 0
92:        else:
93:            # We never broke out -- EOF is too soon (how can this happen???)
94:            raise parse.ParseError("incomplete input",
95:                                   type, value, (prefix, start))
96:        return p.rootnode
97:
98:    def parse_stream_raw(self, stream, debug=False):
99:        """Parse a stream and return the syntax tree."""
100:        tokens = tokenize.generate_tokens(stream.readline)
101:        return self.parse_tokens(tokens, debug)
102:
103:    def parse_stream(self, stream, debug=False):
104:        """Parse a stream and return the syntax tree."""
105:        return self.parse_stream_raw(stream, debug)
106:
107:    def parse_file(self, filename, encoding=None, debug=False):
108:        """Parse a file and return the syntax tree."""
109:        with io.open(filename, "r", encoding=encoding) as stream:
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char in ' \t':
135:                current_column += 1
136:            elif char == '\n':
137:                # unexpected empty line
138:                current_column = 0
139:            else:
140:                # indent is finished
141:                wait_for_nl = True
142:        return ''.join(lines), current_line
143:
144:
145:def _generate_pickle_name(gt, cache_dir=None):
146:    head, tail = os.path.splitext(gt)
147:    if tail == ".txt":
148:        tail = ""
149:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
150:    if cache_dir:
151:        return os.path.join(cache_dir, os.path.basename(name))
152:    else:
153:        return name
154:
155:
156:def load_grammar(gt="Grammar.txt", gp=None,
157:                 save=True, force=False, logger=None):
158:    """Load the grammar (maybe from a pickle)."""
159:    if logger is None:
160:        logger = logging.getLogger()
161:    gp = _generate_pickle_name(gt) if gp is None else gp
162:    if force or not _newer(gp, gt):
163:        logger.info("Generating grammar tables from %s", gt)
164:        g = pgen.generate_grammar(gt)
165:        if save:
166:            logger.info("Writing grammar tables to %s", gp)
167:            try:
168:                g.dump(gp)
169:            except OSError as e:
170:                logger.info("Writing failed: %s", e)
171:    else:
172:        g = grammar.Grammar()
173:        g.load(gp)
174:    return g
175:
176:
177:def _newer(a, b):
178:    """Inquire whether file a was written since file b."""
179:    if not os.path.exists(a):
180:        return False
181:    if not os.path.exists(b):
182:        return True
183:    return os.path.getmtime(a) >= os.path.getmtime(b)
184:
185:
186:def load_packaged_grammar(package, grammar_source, cache_dir=None):
187:    """Normally, loads a pickled grammar by doing
188:        pkgutil.get_data(package, pickled_grammar)
189:    where *pickled_grammar* is computed from *grammar_source* by adding the
190:    Python version and using a ``.pickle`` extension.
191:
192:    However, if *grammar_source* is an extant file, load_grammar(grammar_source)
193:    is called instead. This facilitates using a packaged grammar file when needed
194:    but preserves load_grammar's automatic regeneration behavior when possible.
195:
196:    """
197:    if os.path.isfile(grammar_source):
198:        gp = _generate_pickle_name(grammar_source, cache_dir) if cache_dir else None
199:        return load_grammar(grammar_source, gp=gp)
200:    pickled_name = _generate_pickle_name(os.path.basename(grammar_source), cache_dir)

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: In blib2to3/pgen2/driver.py:parse_tokens(), lines 61-67 explicitly handle tokenize.COMMENT and tokenize.NL by appending value to prefix and continuing, ensuring they are skipped from parsing and included in prefix. This allows subsequent _partially_consume_prefix() (lines 117-142) to process comment indentation appropriately during DEDENT handling.

Todo item 4 of 8 is correct
description: 'Validate that comment and NL tokens are added to the prefix and skipped in parse_tokens: check that when type in (tokenize.COMMENT, tokenize.NL), the code appends to `prefix` and continues, so that comment indentation is processed by _partially_consume_prefix.'

Validating todo item 5 of 8
description: 'Search the codebase for other instances in the root blib2to3/pgen2 module that might still treat tabs as width 4 in indentation calculations and confirm none exist (e.g., no `current_column += 4` for '\t' in the root driver implementation).'

Calling tool: grepTool
(?i)\\t
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:56:Whitespace = r'[ \f\t]*'
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:147:    print("%d,%d-%d,%d:\t%s\t%s" % \
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:229:cookie_re = re.compile(r'^[ \t\f]*#.*?coding[:=][ \t]*([-\w.]+)', re.ASCII)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:230:blank_re = re.compile(br'^[ \t\f]*(?:[#\r\n]|$)', re.ASCII)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:399:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:134:            elif char in ' \t':
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/conv.py:228:        assert line == "\tdfas,\n", (lineno, line)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/literals.py:13:                  "t": "\t",

Calling tool: grepTool
current_column
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:120:        current_column = 0
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:126:                    if current_line.strip() and current_column < column:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:132:                    current_column = 0
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:135:                current_column += 1
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:138:                current_column = 0

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
100:        tokens = tokenize.generate_tokens(stream.readline)
101:        return self.parse_tokens(tokens, debug)
102:
103:    def parse_stream(self, stream, debug=False):
104:        """Parse a stream and return the syntax tree."""
105:        return self.parse_stream_raw(stream, debug)
106:
107:    def parse_file(self, filename, encoding=None, debug=False):
108:        """Parse a file and return the syntax tree."""
109:        with io.open(filename, "r", encoding=encoding) as stream:
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char in ' \t':
135:                current_column += 1
136:            elif char == '\n':
137:                # unexpected empty line
138:                current_column = 0
139:            else:
140:                # indent is finished
141:                wait_for_nl = True
142:        return ''.join(lines), current_line
143:
144:
145:def _generate_pickle_name(gt, cache_dir=None):
146:    head, tail = os.path.splitext(gt)
147:    if tail == ".txt":
148:        tail = ""
149:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
150:    if cache_dir:
151:        return os.path.join(cache_dir, os.path.basename(name))
152:    else:
153:        return name
154:
155:
156:def load_grammar(gt="Grammar.txt", gp=None,
157:                 save=True, force=False, logger=None):
158:    """Load the grammar (maybe from a pickle)."""
159:    if logger is None:
160:        logger = logging.getLogger()
161:    gp = _generate_pickle_name(gt) if gp is None else gp
162:    if force or not _newer(gp, gt):
163:        logger.info("Generating grammar tables from %s", gt)
164:        g = pgen.generate_grammar(gt)
165:        if save:
166:            logger.info("Writing grammar tables to %s", gp)
167:            try:
168:                g.dump(gp)
169:            except OSError as e:
170:                logger.info("Writing failed: %s", e)
171:    else:
172:        g = grammar.Grammar()
173:        g.load(gp)
174:    return g
175:
176:
177:def _newer(a, b):
178:    """Inquire whether file a was written since file b."""
179:    if not os.path.exists(a):

Calling tool: grepTool
\+=\s*4
No files found. Call grepTool again with a more generic pattern.

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py
360:    stashed = None
361:    async_def = False
362:    async_def_indent = 0
363:    async_def_nl = False
364:
365:    while 1:                                   # loop over lines in stream
366:        try:
367:            line = readline()
368:        except StopIteration:
369:            line = ''
370:        lnum = lnum + 1
371:        pos, max = 0, len(line)
372:
373:        if contstr:                            # continued string
374:            if not line:
375:                raise TokenError("EOF in multi-line string", strstart)
376:            endmatch = endprog.match(line)
377:            if endmatch:
378:                pos = end = endmatch.end(0)
379:                yield (STRING, contstr + line[:end],
380:                       strstart, (lnum, end), contline + line)
381:                contstr, needcont = '', 0
382:                contline = None
383:            elif needcont and line[-2:] != '\\\n' and line[-3:] != '\\\r\n':
384:                yield (ERRORTOKEN, contstr + line,
385:                           strstart, (lnum, len(line)), contline)
386:                contstr = ''
387:                contline = None
388:                continue
389:            else:
390:                contstr = contstr + line
391:                contline = contline + line
392:                continue
393:
394:        elif parenlev == 0 and not continued:  # new statement
395:            if not line: break
396:            column = 0
397:            while pos < max:                   # measure leading whitespace
398:                if line[pos] == ' ': column = column + 1
399:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize
400:                elif line[pos] == '\f': column = 0
401:                else: break
402:                pos = pos + 1
403:            if pos == max: break
404:
405:            if stashed:
406:                yield stashed
407:                stashed = None
408:
409:            if line[pos] in '\r\n':            # skip blank lines
410:                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)
411:                continue
412:
413:            if line[pos] == '#':               # skip comments
414:                comment_token = line[pos:].rstrip('\r\n')
415:                nl_pos = pos + len(comment_token)
416:                yield (COMMENT, comment_token,
417:                        (lnum, pos), (lnum, pos + len(comment_token)), line)
418:                yield (NL, line[nl_pos:],
419:                        (lnum, nl_pos), (lnum, len(line)), line)
420:                continue
421:
422:            if column > indents[-1]:           # count indents
423:                indents.append(column)
424:                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)
425:
426:            while column < indents[-1]:        # count dedents
427:                if column not in indents:
428:                    raise IndentationError(
429:                        "unindent does not match any outer indentation level",
430:                        ("<tokenize>", lnum, pos, line))
431:                indents = indents[:-1]
432:
433:                if async_def and async_def_indent >= indents[-1]:
434:                    async_def = False
435:                    async_def_nl = False
436:                    async_def_indent = 0
437:
438:                yield (DEDENT, '', (lnum, pos), (lnum, pos), line)
439:

Calling tool: grepTool
tabsize
Matched content:
138:tabsize = 8
399:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize

Calling tool: grepTool
(?i).*\\t.*4|4.*\\t.*
No files found. Call grepTool again with a more generic pattern.

Calling tool: grepTool
(?i)indent
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:203:        indents = []
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:216:            if toknum == INDENT:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:217:                indents.append(tokval)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:220:                indents.pop()
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:224:            elif startline and indents:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:225:                toks_append(indents[-1])
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:357:    indents = [0]
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:362:    async_def_indent = 0
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:422:            if column > indents[-1]:           # count indents
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:423:                indents.append(column)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:424:                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:426:            while column < indents[-1]:        # count dedents
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:427:                if column not in indents:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:428:                    raise IndentationError(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:429:                        "unindent does not match any outer indentation level",
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:431:                indents = indents[:-1]
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:433:                if async_def and async_def_indent >= indents[-1]:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:436:                    async_def_indent = 0
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:440:            if async_def and async_def_nl and async_def_indent >= indents[-1]:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:443:                async_def_indent = 0
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:526:                                async_def_indent = indents[-1]
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:561:    for indent in indents[1:]:                 # pop remaining indent levels
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:46:        indent_columns = []
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:73:            if type == token.INDENT:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:74:                indent_columns.append(len(value))
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:79:                _indent_col = indent_columns.pop()
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:80:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:86:            if type in {token.INDENT, token.DEDENT}:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:140:                # indent is finished
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/token.pyi:11:INDENT: int
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/token.py:12:INDENT = 5

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Modifications:
5:# Copyright 2006 Google, Inc. All Rights Reserved.
6:# Licensed to PSF under a Contributor Agreement.
7:
8:"""Parser driver.
9:
10:This provides a high-level interface to parse a file into a syntax tree.
11:
12:"""
13:
14:__author__ = "Guido van Rossum <guido@python.org>"
15:
16:__all__ = ["Driver", "load_grammar"]
17:
18:# Python imports
19:import codecs
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(self, grammar, convert=None, logger=None):
33:        self.grammar = grammar
34:        if logger is None:
35:            logger = logging.getLogger()
36:        self.logger = logger
37:        self.convert = convert
38:
39:    def parse_tokens(self, tokens, debug=False):
40:        """Parse a series of tokens and return the syntax tree."""
41:        # XXX Move the prefix computation into a wrapper around tokenize.
42:        p = parse.Parser(self.grammar, self.convert)
43:        p.setup()
44:        lineno = 1
45:        column = 0
46:        indent_columns = []
47:        type = value = start = end = line_text = None
48:        prefix = ""
49:        for quintuple in tokens:
50:            type, value, start, end, line_text = quintuple
51:            if start != (lineno, column):
52:                assert (lineno, column) <= start, ((lineno, column), start)
53:                s_lineno, s_column = start
54:                if lineno < s_lineno:
55:                    prefix += "\n" * (s_lineno - lineno)
56:                    lineno = s_lineno
57:                    column = 0
58:                if column < s_column:
59:                    prefix += line_text[column:s_column]
60:                    column = s_column
61:            if type in (tokenize.COMMENT, tokenize.NL):
62:                prefix += value
63:                lineno, column = end
64:                if value.endswith("\n"):
65:                    lineno += 1
66:                    column = 0
67:                continue
68:            if type == token.OP:
69:                type = grammar.opmap[value]
70:            if debug:
71:                self.logger.debug("%s %r (prefix=%r)",
72:                                  token.tok_name[type], value, prefix)
73:            if type == token.INDENT:
74:                indent_columns.append(len(value))
75:                _prefix = prefix + value
76:                prefix = ""
77:                value = ""
78:            elif type == token.DEDENT:
79:                _indent_col = indent_columns.pop()
80:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
81:            if p.addtoken(type, value, (prefix, start)):
82:                if debug:
83:                    self.logger.debug("Stop.")
84:                break
85:            prefix = ""
86:            if type in {token.INDENT, token.DEDENT}:
87:                prefix = _prefix
88:            lineno, column = end
89:            if value.endswith("\n"):
90:                lineno += 1
91:                column = 0
92:        else:
93:            # We never broke out -- EOF is too soon (how can this happen???)
94:            raise parse.ParseError("incomplete input",
95:                                   type, value, (prefix, start))
96:        return p.rootnode
97:
98:    def parse_stream_raw(self, stream, debug=False):
99:        """Parse a stream and return the syntax tree."""
100:        tokens = tokenize.generate_tokens(stream.readline)
101:        return self.parse_tokens(tokens, debug)
102:
103:    def parse_stream(self, stream, debug=False):
104:        """Parse a stream and return the syntax tree."""
105:        return self.parse_stream_raw(stream, debug)
106:
107:    def parse_file(self, filename, encoding=None, debug=False):
108:        """Parse a file and return the syntax tree."""
109:        with io.open(filename, "r", encoding=encoding) as stream:
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0

Calling tool: globTool
**/*
Matched files:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/grammar.pyi
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/parse.py
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/literals.pyi
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/conv.py
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.pyi
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/token.pyi
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/grammar.py
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/literals.py
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/__pycache__/parse.cpython-38.pyc
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/__pycache__/pgen.cpython-38.pyc
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/__pycache__/grammar.cpython-38.pyc
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/__pycache__/token.cpython-38.pyc
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/__pycache__/tokenize.cpython-38.pyc
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/__pycache__/driver.cpython-38.pyc
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/__pycache__/__init__.cpython-38.pyc
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/__init__.py
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.pyi
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/parse.pyi
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/pgen.py
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/pgen.pyi
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/__init__.pyi
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/token.py

Calling tool: grepTool
column\s*([+\-*/%]|//=|\+=|-=|\*=|/=|//=|%=).*[4]|4.*column
No files found. Call grepTool again with a more generic pattern.

Calling tool: grepTool
char\s*==\s*'\\t'.*current_column\s*\+=\s*4
No files found. Call grepTool again with a more generic pattern.

Calling tool: grepTool
current_column\s*\+=
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py:135:                current_column += 1

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: Searched blib2to3/pgen2 for any handling of tabs as width 4. In driver.py, _partially_consume_prefix now treats both space and tab identically (current_column += 1). Grep found no occurrences of current_column += 4 or similar. Other references (tokenize.py) use tabsize=8, not 4, and conv/literals only contain string literals. Thus no remaining 4-width tab handling exists in the module.

Todo item 5 of 8 is correct
description: 'Search the codebase for other instances in the root blib2to3/pgen2 module that might still treat tabs as width 4 in indentation calculations and confirm none exist (e.g., no `current_column += 4` for '\t' in the root driver implementation).'

Validating todo item 6 of 8
description: 'Identify duplicate copies of driver.py (e.g., under env/src/black/blib2to3/pgen2/driver.py) and verify whether the version used at runtime by black imports the updated root file. If the env copy is not used, note it; if it could be imported, ensure it is updated or that import paths favor the updated implementation.'

Calling tool: globTool
**/driver.py
Matched files:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py

Calling tool: grepTool
blib2to3\.pgen2\.driver|pgen2\.driver|from blib2to3|import blib2to3
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black.py:45:from blib2to3.pytree import Node, Leaf, type_repr
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black.py:46:from blib2to3 import pygram, pytree
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black.py:47:from blib2to3.pgen2 import driver, token
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/black.py:48:from blib2to3.pgen2.parse import ParseError
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pytree.pyi:6:from blib2to3.pgen2.grammar import Grammar
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.py:34:from blib2to3.pgen2.token import *
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/tokenize.pyi:5:from blib2to3.pgen2.token import *  # noqa
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/parse.pyi:5:from blib2to3.pgen2.grammar import Grammar, _DFAS
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/parse.pyi:6:from blib2to3.pytree import _NL, _Convert, _RawNode
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/grammar.pyi:3:from blib2to3.pgen2 import _Path
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/pgen.pyi:6:from blib2to3.pgen2 import _Path, grammar
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/pgen.pyi:7:from blib2to3.pgen2.tokenize import _TokenInfo
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pygram.pyi:5:from blib2to3.pgen2.grammar import Grammar
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.pyi:1:# Stubs for lib2to3.pgen2.driver (Python 3.6)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.pyi:8:from blib2to3.pytree import _Convert, _NL
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.pyi:9:from blib2to3.pgen2 import _Path
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.pyi:10:from blib2to3.pgen2.grammar import Grammar
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/parse.pyi:5:from blib2to3.pgen2.grammar import Grammar, _DFAS
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/parse.pyi:6:from blib2to3.pytree import _NL, _Convert, _RawNode
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.pyi:1:# Stubs for lib2to3.pgen2.driver (Python 3.6)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.pyi:8:from blib2to3.pytree import _Convert, _NL
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.pyi:9:from blib2to3.pgen2 import _Path
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.pyi:10:from blib2to3.pgen2.grammar import Grammar
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/tokenize.py:34:from blib2to3.pgen2.token import *
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/pgen.pyi:6:from blib2to3.pgen2 import _Path, grammar
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/pgen.pyi:7:from blib2to3.pgen2.tokenize import _TokenInfo
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/grammar.pyi:3:from blib2to3.pgen2 import _Path
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pygram.pyi:5:from blib2to3.pgen2.grammar import Grammar
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pytree.pyi:6:from blib2to3.pgen2.grammar import Grammar
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/tokenize.pyi:5:from blib2to3.pgen2.token import *  # noqa
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/black.py:45:from blib2to3.pytree import Node, Leaf, type_repr
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/black.py:46:from blib2to3 import pygram, pytree
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/black.py:47:from blib2to3.pgen2 import driver, token
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/black.py:48:from blib2to3.pgen2.parse import ParseError
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/mypy/typeshed/stdlib/2and3/lib2to3/pgen2/driver.pyi:1:# Stubs for lib2to3.pgen2.driver (Python 3.6)

Calling tool: grepTool
elif char == '\\t'|elif char in ' \\t'|current_column \+=
Matched content:
134:            elif char in ' \t':
135:                current_column += 1

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/driver.py
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char in ' \t':
135:                current_column += 1
136:            elif char == '\n':
137:                # unexpected empty line
138:                current_column = 0
139:            else:
140:                # indent is finished
141:                wait_for_nl = True
142:        return ''.join(lines), current_line
143:
144:
145:def _generate_pickle_name(gt, cache_dir=None):
146:    head, tail = os.path.splitext(gt)
147:    if tail == ".txt":
148:        tail = ""
149:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
150:    if cache_dir:
151:        return os.path.join(cache_dir, os.path.basename(name))
152:    else:
153:        return name
154:
155:
156:def load_grammar(gt="Grammar.txt", gp=None,
157:                 save=True, force=False, logger=None):
158:    """Load the grammar (maybe from a pickle)."""
159:    if logger is None:
160:        logger = logging.getLogger()
161:    gp = _generate_pickle_name(gt) if gp is None else gp
162:    if force or not _newer(gp, gt):
163:        logger.info("Generating grammar tables from %s", gt)
164:        g = pgen.generate_grammar(gt)
165:        if save:
166:            logger.info("Writing grammar tables to %s", gp)
167:            try:
168:                g.dump(gp)
169:            except OSError as e:

Calling tool: grepTool
_partially_consume_prefix|elif char in ' \\t'|elif char == '\\t'
Matched content:
80:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
117:    def _partially_consume_prefix(self, prefix, column):
136:            elif char == '\t':

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/driver.py
110:            return self.parse_stream(stream, debug)
111:
112:    def parse_string(self, text, debug=False):
113:        """Parse a string and return the syntax tree."""
114:        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
115:        return self.parse_tokens(tokens, debug)
116:
117:    def _partially_consume_prefix(self, prefix, column):
118:        lines = []
119:        current_line = ""
120:        current_column = 0
121:        wait_for_nl = False
122:        for char in prefix:
123:            current_line += char
124:            if wait_for_nl:
125:                if char == '\n':
126:                    if current_line.strip() and current_column < column:
127:                        res = ''.join(lines)
128:                        return res, prefix[len(res):]
129:
130:                    lines.append(current_line)
131:                    current_line = ""
132:                    current_column = 0
133:                    wait_for_nl = False
134:            elif char == ' ':
135:                current_column += 1
136:            elif char == '\t':
137:                current_column += 4
138:            elif char == '\n':
139:                # unexpected empty line
140:                current_column = 0
141:            else:
142:                # indent is finished
143:                wait_for_nl = True
144:        return ''.join(lines), current_line
145:
146:
147:def _generate_pickle_name(gt, cache_dir=None):
148:    head, tail = os.path.splitext(gt)
149:    if tail == ".txt":
150:        tail = ""
151:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
152:    if cache_dir:
153:        return os.path.join(cache_dir, os.path.basename(name))
154:    else:
155:        return name
156:
157:
158:def load_grammar(gt="Grammar.txt", gp=None,
159:                 save=True, force=False, logger=None):
160:    """Load the grammar (maybe from a pickle)."""
161:    if logger is None:
162:        logger = logging.getLogger()
163:    gp = _generate_pickle_name(gt) if gp is None else gp
164:    if force or not _newer(gp, gt):
165:        logger.info("Generating grammar tables from %s", gt)
166:        g = pgen.generate_grammar(gt)
167:        if save:
168:            logger.info("Writing grammar tables to %s", gp)
169:            try:

Calling tool: grepTool
sys\.path|PYTHONPATH|insert\(0
Showing first 50 results from grep. Consider using a more specific pattern or path.
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/plugin/black.vim:90:  if sys.path[0] != virtualenv_site_packages:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/plugin/black.vim:91:    sys.path.insert(0, virtualenv_site_packages)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/docs/conf.py:12:# add these directories to sys.path here. If the directory is relative to the
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/blib2to3/pgen2/pgen.py:32:        names.insert(0, self.startsymbol)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/bin/mypyc:50:    env['PYTHONPATH'] = base_path + os.pathsep + env.get('PYTHONPATH', '')
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/blib2to3/pgen2/pgen.py:32:        names.insert(0, self.startsymbol)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/docs/conf.py:12:# add these directories to sys.path here. If the directory is relative to the
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/pyflakes/test/test_api.py:709:        env['PYTHONPATH'] = os.pathsep.join(sys.path)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/plugin/black.vim:90:  if sys.path[0] != virtualenv_site_packages:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/src/black/plugin/black.vim:91:    sys.path.insert(0, virtualenv_site_packages)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/pathspec/patterns/gitwildmatch.py:103:					pattern_segs.insert(0, '**')
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/platformdirs/android.py:145:        # and if that fails, too, find an android folder looking at path on the sys.path
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/platformdirs/android.py:148:        for path in sys.path:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/platformdirs/android.py:155:        # one last try: find an android folder looking at path on the sys.path taking adopted storage paths into
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/platformdirs/android.py:158:        for path in sys.path:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/flake8/main/application.py:187:        sys.path.extend(self.local_plugins.paths)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/cffi/recompiler.py:802:            arguments.insert(0, arg)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/pkginfo/installed.py:39:                for entry in sys.path:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/cffi/_embedding.h:240:            PyFile_WriteString("\nsys.path: ", f);
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/docutils/nodes.py:1532:            self.insert(0, header())
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/aiohttp/web_urldispatcher.py:234:        self._apps.insert(0, app)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/cffi/_imp_emulation.py:53:                path = sys.path
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/docutils/transforms/parts.py:63:                title.insert(0, generated)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/cffi/vengine_gen.py:31:                path = sys.path
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/cffi/vengine_gen.py:161:            arglist.insert(0, tpresult.get_c_name(' *r', context))
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/cffi/vengine_gen.py:209:                    indirect_args.insert(0, indirect_result)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/cffi/vengine_gen.py:210:                    indirections.insert(0, ("result", indirect_result))
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/docutils/transforms/writer_aux.py:87:                admonition.insert(0, title)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/pkginfo/tests/test_installed.py:108:        oldpath = sys.path[:]
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/pkginfo/tests/test_installed.py:110:            sys.path.append(wonky)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/pkginfo/tests/test_installed.py:117:            sys.path[:] = oldpath
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/pkginfo/tests/test_installed.py:127:        oldpath = sys.path[:]
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/pkginfo/tests/test_installed.py:129:            sys.path.append(manky)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/pkginfo/tests/test_installed.py:136:            sys.path[:] = oldpath
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/docutils/transforms/references.py:512:            footnote.insert(0, nodes.label('', label))
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/docutils/transforms/references.py:569:            footnote.insert(0, nodes.label('', labeltext))
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/docutils/transforms/references.py:824:                    reflist.insert(0, nodes.inline(text=' ', Classes=self.classes))
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/docutils/transforms/references.py:826:                    reflist.insert(0, nodes.Text(' '))
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/distlib/wheel.py:745:        if pathname in sys.path:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/distlib/wheel.py:749:                sys.path.append(pathname)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/distlib/wheel.py:751:                sys.path.insert(0, pathname)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/distlib/wheel.py:760:        if pathname not in sys.path:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/distlib/wheel.py:763:            sys.path.remove(pathname)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/distlib/wheel.py:942:        abis.insert(0, ABI)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/mypyc/genexpr.py:186:                arg_kinds.insert(0, ARG_POS)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/mypyc/genexpr.py:187:                arg_names.insert(0, None)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/mypyc/genexpr.py:254:            arg_values.insert(0, vself)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/mypyc/genexpr.py:255:            arg_kinds.insert(0, ARG_POS)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/mypyc/genexpr.py:256:            arg_names.insert(0, None)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-10/good/black/env/lib/python3.8/site-packages/pip/__main__.py:6:# of sys.path, if present to avoid using current directory

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as INCORRECT
Reason: Two copies of driver.py exist: root blib2to3/pgen2/driver.py (updated with "elif char in ' \t': current_column += 1") and env/src/black/blib2to3/pgen2/driver.py (still has the old tab logic adding 4). Black imports from "blib2to3.pgen2 import driver" (top-level), so at runtime it may import the stale env copy depending on sys.path. No changes ensure import paths favor the updated root file or update the env copy.


=== PHASE 2 COMPLETE ===

