=== PHASE 1: ANALYSIS ===

Calling tool: grepTool
test_async_as_identifier
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/bugsinpy_run_test.sh:1:python -m unittest -q tests.test_black.BlackTestCase.test_async_as_identifier
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:506:    def test_async_as_identifier(self) -> None:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py
480:    @patch("black.dump_to_file", dump_to_stderr)
481:    def test_python2_print_function(self) -> None:
482:        source, expected = read_data("python2_print_function")
483:        mode = black.FileMode(target_versions={TargetVersion.PY27})
484:        actual = fs(source, mode=mode)
485:        self.assertFormatEqual(expected, actual)
486:        black.assert_equivalent(source, actual)
487:        black.assert_stable(source, actual, mode)
488:
489:    @patch("black.dump_to_file", dump_to_stderr)
490:    def test_python2_unicode_literals(self) -> None:
491:        source, expected = read_data("python2_unicode_literals")
492:        actual = fs(source)
493:        self.assertFormatEqual(expected, actual)
494:        black.assert_equivalent(source, actual)
495:        black.assert_stable(source, actual, black.FileMode())
496:
497:    @patch("black.dump_to_file", dump_to_stderr)
498:    def test_stub(self) -> None:
499:        mode = black.FileMode(is_pyi=True)
500:        source, expected = read_data("stub.pyi")
501:        actual = fs(source, mode=mode)
502:        self.assertFormatEqual(expected, actual)
503:        black.assert_stable(source, actual, mode)
504:
505:    @patch("black.dump_to_file", dump_to_stderr)
506:    def test_async_as_identifier(self) -> None:
507:        source_path = (THIS_DIR / "data" / "async_as_identifier.py").resolve()
508:        source, expected = read_data("async_as_identifier")
509:        actual = fs(source)
510:        self.assertFormatEqual(expected, actual)
511:        major, minor = sys.version_info[:2]
512:        if major < 3 or (major <= 3 and minor < 7):
513:            black.assert_equivalent(source, actual)
514:        black.assert_stable(source, actual, black.FileMode())
515:        # ensure black can parse this when the target is 3.6
516:        self.invokeBlack([str(source_path), "--target-version", "py36"])
517:        # but not on 3.7, because async/await is no longer an identifier
518:        self.invokeBlack([str(source_path), "--target-version", "py37"], exit_code=123)
519:
520:    @patch("black.dump_to_file", dump_to_stderr)
521:    def test_python37(self) -> None:
522:        source_path = (THIS_DIR / "data" / "python37.py").resolve()
523:        source, expected = read_data("python37")
524:        actual = fs(source)
525:        self.assertFormatEqual(expected, actual)
526:        major, minor = sys.version_info[:2]
527:        if major > 3 or (major == 3 and minor >= 7):
528:            black.assert_equivalent(source, actual)
529:        black.assert_stable(source, actual, black.FileMode())
530:        # ensure black can parse this when the target is 3.7
531:        self.invokeBlack([str(source_path), "--target-version", "py37"])
532:        # but not on 3.6, because we use async as a reserved keyword
533:        self.invokeBlack([str(source_path), "--target-version", "py36"], exit_code=123)
534:
535:    @patch("black.dump_to_file", dump_to_stderr)
536:    def test_fmtonoff(self) -> None:
537:        source, expected = read_data("fmtonoff")
538:        actual = fs(source)
539:        self.assertFormatEqual(expected, actual)
540:        black.assert_equivalent(source, actual)
541:        black.assert_stable(source, actual, black.FileMode())
542:
543:    @patch("black.dump_to_file", dump_to_stderr)
544:    def test_fmtonoff2(self) -> None:
545:        source, expected = read_data("fmtonoff2")
546:        actual = fs(source)
547:        self.assertFormatEqual(expected, actual)
548:        black.assert_equivalent(source, actual)
549:        black.assert_stable(source, actual, black.FileMode())
550:
551:    @patch("black.dump_to_file", dump_to_stderr)
552:    def test_remove_empty_parentheses_after_class(self) -> None:
553:        source, expected = read_data("class_blank_parentheses")
554:        actual = fs(source)
555:        self.assertFormatEqual(expected, actual)
556:        black.assert_equivalent(source, actual)
557:        black.assert_stable(source, actual, black.FileMode())
558:
559:    @patch("black.dump_to_file", dump_to_stderr)
560:    def test_new_line_between_class_and_code(self) -> None:
561:        source, expected = read_data("class_methods_new_line")
562:        actual = fs(source)
563:        self.assertFormatEqual(expected, actual)
564:        black.assert_equivalent(source, actual)
565:        black.assert_stable(source, actual, black.FileMode())
566:
567:    @patch("black.dump_to_file", dump_to_stderr)
568:    def test_bracket_match(self) -> None:
569:        source, expected = read_data("bracketmatch")
570:        actual = fs(source)
571:        self.assertFormatEqual(expected, actual)
572:        black.assert_equivalent(source, actual)
573:        black.assert_stable(source, actual, black.FileMode())
574:
575:    @patch("black.dump_to_file", dump_to_stderr)
576:    def test_tuple_assign(self) -> None:
577:        source, expected = read_data("tupleassign")
578:        actual = fs(source)
579:        self.assertFormatEqual(expected, actual)
580:        black.assert_equivalent(source, actual)
581:        black.assert_stable(source, actual, black.FileMode())
582:
583:    def test_tab_comment_indentation(self) -> None:
584:        contents_tab = "if 1:\n\tif 2:\n\t\tpass\n\t# comment\n\tpass\n"
585:        contents_spc = "if 1:\n    if 2:\n        pass\n    # comment\n    pass\n"
586:        self.assertFormatEqual(contents_spc, fs(contents_spc))
587:        self.assertFormatEqual(contents_spc, fs(contents_tab))
588:
589:        contents_tab = "if 1:\n\tif 2:\n\t\tpass\n\t\t# comment\n\tpass\n"
590:        contents_spc = "if 1:\n    if 2:\n        pass\n        # comment\n    pass\n"
591:        self.assertFormatEqual(contents_spc, fs(contents_spc))
592:        self.assertFormatEqual(contents_spc, fs(contents_tab))
593:
594:        # mixed tabs and spaces (valid Python 2 code)
595:        contents_tab = "if 1:\n        if 2:\n\t\tpass\n\t# comment\n        pass\n"
596:        contents_spc = "if 1:\n    if 2:\n        pass\n    # comment\n    pass\n"
597:        self.assertFormatEqual(contents_spc, fs(contents_spc))
598:        self.assertFormatEqual(contents_spc, fs(contents_tab))
599:

Calling tool: globTool
**/tests/data/async_as_identifier*
Matched files:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/data/async_as_identifier.py

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/data/async_as_identifier.py
1:def async():
2:    pass
3:
4:
5:def await():
6:    pass
7:
8:
9:await = lambda: None
10:async = lambda: None
11:async()
12:await()
13:
14:
15:def sync_fn():
16:    await = lambda: None
17:    async = lambda: None
18:    async()
19:    await()
20:
21:
22:async def async_fn():
23:    await async_fn()
24:
25:
26:# output
27:def async():
28:    pass
29:
30:
31:def await():
32:    pass
33:
34:
35:await = lambda: None
36:async = lambda: None
37:async()
38:await()
39:
40:
41:def sync_fn():
42:    await = lambda: None
43:    async = lambda: None
44:    async()
45:    await()
46:
47:
48:async def async_fn():
49:    await async_fn()

Calling tool: grepTool
dataclass\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:763:@dataclass(frozen=True)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:141:@dataclass(frozen=True)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi:13:@dataclass(frozen=True)

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
1:import asyncio
2:from asyncio.base_events import BaseEventLoop
3:from concurrent.futures import Executor, ProcessPoolExecutor
4:from datetime import datetime
5:from enum import Enum
6:from functools import lru_cache, partial, wraps
7:import io
8:import itertools
9:import logging
10:from multiprocessing import Manager, freeze_support
11:import os
12:from pathlib import Path
13:import pickle
14:import re
15:import signal
16:import sys
17:import tempfile
18:import tokenize
19:from typing import (
20:    Any,
21:    Callable,
22:    Collection,
23:    Dict,
24:    Generator,
25:    Generic,
26:    Iterable,
27:    Iterator,
28:    List,
29:    Optional,
30:    Pattern,
31:    Sequence,
32:    Set,
33:    Tuple,
34:    TypeVar,
35:    Union,
36:    cast,
37:)
38:
39:from appdirs import user_cache_dir
40:from attr import dataclass, evolve, Factory
41:import click
42:import toml
43:from typed_ast import ast3, ast27
44:
45:# lib2to3 fork
46:from blib2to3.pytree import Node, Leaf, type_repr
47:from blib2to3 import pygram, pytree
48:from blib2to3.pgen2 import driver, token
49:from blib2to3.pgen2.grammar import Grammar
50:from blib2to3.pgen2.parse import ParseError
51:from blib2to3.pgen2.tokenize import TokenizerConfig
52:
53:
54:__version__ = "19.3b0"
55:DEFAULT_LINE_LENGTH = 88
56:DEFAULT_EXCLUDES = (
57:    r"/(\.eggs|\.git|\.hg|\.mypy_cache|\.nox|\.tox|\.venv|_build|buck-out|build|dist)/"
58:)
59:DEFAULT_INCLUDES = r"\.pyi?$"
60:CACHE_DIR = Path(user_cache_dir("black", version=__version__))
61:
62:
63:# types
64:FileContent = str
65:Encoding = str
66:NewLine = str
67:Depth = int
68:NodeType = int
69:LeafID = int
70:Priority = int
71:Index = int
72:LN = Union[Leaf, Node]
73:SplitFunc = Callable[["Line", Collection["Feature"]], Iterator["Line"]]
74:Timestamp = float
75:FileSize = int
76:CacheInfo = Tuple[Timestamp, FileSize]
77:Cache = Dict[Path, CacheInfo]
78:out = partial(click.secho, bold=True, err=True)
79:err = partial(click.secho, fg="red", err=True)
80:
81:pygram.initialize(CACHE_DIR)
82:syms = pygram.python_symbols
83:
84:
85:class NothingChanged(UserWarning):
86:    """Raised when reformatted code is the same as source."""
87:
88:
89:class CannotSplit(Exception):
90:    """A readable split that fits the allotted line length is impossible."""
91:
92:
93:class InvalidInput(ValueError):
94:    """Raised when input source code fails all parse attempts."""
95:
96:
97:class WriteBack(Enum):
98:    NO = 0
99:    YES = 1
100:    DIFF = 2
101:    CHECK = 3
102:
103:    @classmethod
104:    def from_configuration(cls, *, check: bool, diff: bool) -> "WriteBack":
105:        if check and not diff:
106:            return cls.CHECK
107:
108:        return cls.DIFF if diff else cls.YES
109:
110:
111:class Changed(Enum):
112:    NO = 0
113:    CACHED = 1
114:    YES = 2
115:
116:
117:class TargetVersion(Enum):
118:    PY27 = 2
119:    PY33 = 3
120:    PY34 = 4

Calling tool: grepTool
get_parser_configs\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:826:    for parser_config in get_parser_configs(set(target_versions)):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
720:        remove_u_prefix="unicode_literals" in future_imports
721:        or supports_feature(versions, Feature.UNICODE_LITERALS),
722:        is_pyi=mode.is_pyi,
723:        normalize_strings=mode.string_normalization,
724:    )
725:    elt = EmptyLineTracker(is_pyi=mode.is_pyi)
726:    empty_line = Line()
727:    after = 0
728:    split_line_features = {
729:        feature
730:        for feature in {Feature.TRAILING_COMMA_IN_CALL, Feature.TRAILING_COMMA_IN_DEF}
731:        if supports_feature(versions, feature)
732:    }
733:    for current_line in lines.visit(src_node):
734:        for _ in range(after):
735:            dst_contents += str(empty_line)
736:        before, after = elt.maybe_empty_lines(current_line)
737:        for _ in range(before):
738:            dst_contents += str(empty_line)
739:        for line in split_line(
740:            current_line, line_length=mode.line_length, features=split_line_features
741:        ):
742:            dst_contents += str(line)
743:    return dst_contents
744:
745:
746:def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:
747:    """Return a tuple of (decoded_contents, encoding, newline).
748:
749:    `newline` is either CRLF or LF but `decoded_contents` is decoded with
750:    universal newlines (i.e. only contains LF).
751:    """
752:    srcbuf = io.BytesIO(src)
753:    encoding, lines = tokenize.detect_encoding(srcbuf.readline)
754:    if not lines:
755:        return "", encoding, "\n"
756:
757:    newline = "\r\n" if b"\r\n" == lines[0][-2:] else "\n"
758:    srcbuf.seek(0)
759:    with io.TextIOWrapper(srcbuf, encoding) as tiow:
760:        return tiow.read(), encoding, newline
761:
762:
763:@dataclass(frozen=True)
764:class ParserConfig:
765:    grammar: Grammar
766:    tokenizer_config: TokenizerConfig = TokenizerConfig()
767:
768:
769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:
770:    if not target_versions:
771:        # No target_version specified, so try all grammars.
772:        return [
773:            # Python 3.7+
774:            ParserConfig(
775:                pygram.python_grammar_no_print_statement_no_exec_statement,
776:                TokenizerConfig(async_is_reserved_keyword=True),
777:            ),
778:            # Python 3.0-3.6
779:            ParserConfig(
780:                pygram.python_grammar_no_print_statement_no_exec_statement,
781:                TokenizerConfig(async_is_reserved_keyword=False),
782:            ),
783:            # Python 2.7 with future print_function import
784:            ParserConfig(pygram.python_grammar_no_print_statement),
785:            # Python 2.7
786:            ParserConfig(pygram.python_grammar),
787:        ]
788:    elif all(version.is_python2() for version in target_versions):
789:        # Python 2-only code, so try Python 2 grammars.
790:        return [
791:            # Python 2.7 with future print_function import
792:            ParserConfig(pygram.python_grammar_no_print_statement),
793:            # Python 2.7
794:            ParserConfig(pygram.python_grammar),
795:        ]
796:    else:
797:        # Python 3-compatible code, so only try Python 3 grammar.
798:        configs = []
799:        # If we have to parse both, try to parse async as a keyword first
800:        if not supports_feature(target_versions, Feature.ASYNC_IS_VALID_IDENTIFIER):
801:            # Python 3.7+
802:            configs.append(
803:                ParserConfig(
804:                    pygram.python_grammar_no_print_statement_no_exec_statement,
805:                    TokenizerConfig(async_is_reserved_keyword=True),
806:                )
807:            )
808:        if not supports_feature(target_versions, Feature.ASYNC_IS_RESERVED_KEYWORD):
809:            # Python 3.0-3.6
810:            configs.append(
811:                ParserConfig(
812:                    pygram.python_grammar_no_print_statement_no_exec_statement,
813:                    TokenizerConfig(async_is_reserved_keyword=False),
814:                )
815:            )
816:        # At least one of the above branches must have been taken, because every Python
817:        # version has exactly one of the two 'ASYNC_IS_*' flags
818:        return configs
819:
820:
821:def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:
822:    """Given a string with source, return the lib2to3 Node."""
823:    if src_txt[-1:] != "\n":
824:        src_txt += "\n"
825:
826:    for parser_config in get_parser_configs(set(target_versions)):
827:        drv = driver.Driver(
828:            parser_config.grammar,
829:            pytree.convert,
830:            tokenizer_config=parser_config.tokenizer_config,
831:        )
832:        try:
833:            result = drv.parse_string(src_txt, True)
834:            break
835:
836:        except ParseError as pe:
837:            lineno, column = pe.context[1]
838:            lines = src_txt.splitlines()
839:            try:
840:                faulty_line = lines[lineno - 1]
841:            except IndexError:
842:                faulty_line = "<line number missing in source>"
843:            exc = InvalidInput(f"Cannot parse: {lineno}:{column}: {faulty_line}")
844:    else:
845:        raise exc from None
846:
847:    if isinstance(result, Leaf):
848:        result = Node(syms.file_input, [result])
849:    return result
850:
851:
852:def lib2to3_unparse(node: Node) -> str:
853:    """Given a lib2to3 node, return its string representation."""
854:    code = str(node)
855:    return code
856:
857:
858:T = TypeVar("T")
859:

Calling tool: grepTool
class Feature\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:133:class Feature(Enum):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
128:
129:
130:PY36_VERSIONS = {TargetVersion.PY36, TargetVersion.PY37, TargetVersion.PY38}
131:
132:
133:class Feature(Enum):
134:    # All string literals are unicode
135:    UNICODE_LITERALS = 1
136:    F_STRINGS = 2
137:    NUMERIC_UNDERSCORES = 3
138:    TRAILING_COMMA_IN_CALL = 4
139:    TRAILING_COMMA_IN_DEF = 5
140:    # The following two feature-flags are mutually exclusive, and exactly one should be
141:    # set for every version of python.
142:    ASYNC_IS_VALID_IDENTIFIER = 6
143:    ASYNC_IS_RESERVED_KEYWORD = 7
144:
145:
146:VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {
147:    TargetVersion.PY27: {Feature.ASYNC_IS_VALID_IDENTIFIER},
148:    TargetVersion.PY33: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
149:    TargetVersion.PY34: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
150:    TargetVersion.PY35: {
151:        Feature.UNICODE_LITERALS,
152:        Feature.TRAILING_COMMA_IN_CALL,
153:        Feature.ASYNC_IS_VALID_IDENTIFIER,
154:    },
155:    TargetVersion.PY36: {
156:        Feature.UNICODE_LITERALS,
157:        Feature.F_STRINGS,
158:        Feature.NUMERIC_UNDERSCORES,
159:        Feature.TRAILING_COMMA_IN_CALL,
160:        Feature.TRAILING_COMMA_IN_DEF,
161:        Feature.ASYNC_IS_VALID_IDENTIFIER,
162:    },
163:    TargetVersion.PY37: {
164:        Feature.UNICODE_LITERALS,
165:        Feature.F_STRINGS,
166:        Feature.NUMERIC_UNDERSCORES,
167:        Feature.TRAILING_COMMA_IN_CALL,
168:        Feature.TRAILING_COMMA_IN_DEF,
169:        Feature.ASYNC_IS_RESERVED_KEYWORD,
170:    },
171:    TargetVersion.PY38: {
172:        Feature.UNICODE_LITERALS,
173:        Feature.F_STRINGS,
174:        Feature.NUMERIC_UNDERSCORES,
175:        Feature.TRAILING_COMMA_IN_CALL,
176:        Feature.TRAILING_COMMA_IN_DEF,
177:        Feature.ASYNC_IS_RESERVED_KEYWORD,
178:    },
179:}
180:
181:
182:@dataclass
183:class FileMode:
184:    target_versions: Set[TargetVersion] = Factory(set)
185:    line_length: int = DEFAULT_LINE_LENGTH
186:    string_normalization: bool = True
187:    is_pyi: bool = False
188:
189:    def get_cache_key(self) -> str:
190:        if self.target_versions:
191:            version_str = ",".join(
192:                str(version.value)
193:                for version in sorted(self.target_versions, key=lambda v: v.value)
194:            )
195:        else:
196:            version_str = "-"
197:        parts = [
198:            version_str,
199:            str(self.line_length),
200:            str(int(self.string_normalization)),
201:            str(int(self.is_pyi)),
202:        ]
203:        return ".".join(parts)
204:
205:
206:def supports_feature(target_versions: Set[TargetVersion], feature: Feature) -> bool:
207:    return all(feature in VERSION_TO_FEATURES[version] for version in target_versions)

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Modifications:
5:# Copyright 2006 Google, Inc. All Rights Reserved.
6:# Licensed to PSF under a Contributor Agreement.
7:
8:"""Parser driver.
9:
10:This provides a high-level interface to parse a file into a syntax tree.
11:
12:"""
13:
14:__author__ = "Guido van Rossum <guido@python.org>"
15:
16:__all__ = ["Driver", "load_grammar"]
17:
18:# Python imports
19:import codecs
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(
33:        self,
34:        grammar,
35:        convert=None,
36:        logger=None,
37:        tokenizer_config=tokenize.TokenizerConfig(),
38:    ):
39:        self.grammar = grammar
40:        if logger is None:
41:            logger = logging.getLogger(__name__)
42:        self.logger = logger
43:        self.convert = convert
44:        self.tokenizer_config = tokenizer_config
45:
46:    def parse_tokens(self, tokens, debug=False):
47:        """Parse a series of tokens and return the syntax tree."""
48:        # XXX Move the prefix computation into a wrapper around tokenize.
49:        p = parse.Parser(self.grammar, self.convert)
50:        p.setup()
51:        lineno = 1
52:        column = 0
53:        indent_columns = []
54:        type = value = start = end = line_text = None
55:        prefix = ""
56:        for quintuple in tokens:
57:            type, value, start, end, line_text = quintuple
58:            if start != (lineno, column):
59:                assert (lineno, column) <= start, ((lineno, column), start)
60:                s_lineno, s_column = start
61:                if lineno < s_lineno:
62:                    prefix += "\n" * (s_lineno - lineno)
63:                    lineno = s_lineno
64:                    column = 0
65:                if column < s_column:
66:                    prefix += line_text[column:s_column]
67:                    column = s_column
68:            if type in (tokenize.COMMENT, tokenize.NL):
69:                prefix += value
70:                lineno, column = end
71:                if value.endswith("\n"):
72:                    lineno += 1
73:                    column = 0
74:                continue
75:            if type == token.OP:
76:                type = grammar.opmap[value]
77:            if debug:
78:                self.logger.debug("%s %r (prefix=%r)",
79:                                  token.tok_name[type], value, prefix)
80:            if type == token.INDENT:
81:                indent_columns.append(len(value))
82:                _prefix = prefix + value
83:                prefix = ""
84:                value = ""
85:            elif type == token.DEDENT:
86:                _indent_col = indent_columns.pop()
87:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
88:            if p.addtoken(type, value, (prefix, start)):
89:                if debug:
90:                    self.logger.debug("Stop.")
91:                break
92:            prefix = ""
93:            if type in {token.INDENT, token.DEDENT}:
94:                prefix = _prefix
95:            lineno, column = end
96:            if value.endswith("\n"):
97:                lineno += 1
98:                column = 0
99:        else:
100:            # We never broke out -- EOF is too soon (how can this happen???)
101:            raise parse.ParseError("incomplete input",
102:                                   type, value, (prefix, start))
103:        return p.rootnode
104:
105:    def parse_stream_raw(self, stream, debug=False):
106:        """Parse a stream and return the syntax tree."""
107:        tokens = tokenize.generate_tokens(stream.readline, config=self.tokenizer_config)
108:        return self.parse_tokens(tokens, debug)
109:
110:    def parse_stream(self, stream, debug=False):
111:        """Parse a stream and return the syntax tree."""
112:        return self.parse_stream_raw(stream, debug)
113:
114:    def parse_file(self, filename, encoding=None, debug=False):
115:        """Parse a file and return the syntax tree."""
116:        with io.open(filename, "r", encoding=encoding) as stream:
117:            return self.parse_stream(stream, debug)
118:
119:    def parse_string(self, text, debug=False):
120:        """Parse a string and return the syntax tree."""
121:        tokens = tokenize.generate_tokens(
122:            io.StringIO(text).readline,
123:            config=self.tokenizer_config,
124:        )
125:        return self.parse_tokens(tokens, debug)
126:
127:    def _partially_consume_prefix(self, prefix, column):
128:        lines = []
129:        current_line = ""
130:        current_column = 0
131:        wait_for_nl = False
132:        for char in prefix:
133:            current_line += char
134:            if wait_for_nl:
135:                if char == '\n':
136:                    if current_line.strip() and current_column < column:
137:                        res = ''.join(lines)
138:                        return res, prefix[len(res):]
139:
140:                    lines.append(current_line)
141:                    current_line = ""
142:                    current_column = 0
143:                    wait_for_nl = False
144:            elif char in ' \t':
145:                current_column += 1
146:            elif char == '\n':
147:                # unexpected empty line
148:                current_column = 0
149:            else:
150:                # indent is finished
151:                wait_for_nl = True
152:        return ''.join(lines), current_line
153:
154:
155:def _generate_pickle_name(gt, cache_dir=None):
156:    head, tail = os.path.splitext(gt)
157:    if tail == ".txt":
158:        tail = ""
159:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
160:    if cache_dir:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
1:# Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006 Python Software Foundation.
2:# All rights reserved.
3:
4:"""Tokenization help for Python programs.
5:
6:generate_tokens(readline) is a generator that breaks a stream of
7:text into Python tokens.  It accepts a readline-like method which is called
8:repeatedly to get the next line of input (or "" for EOF).  It generates
9:5-tuples with these members:
10:
11:    the token type (see token.py)
12:    the token (a string)
13:    the starting (row, column) indices of the token (a 2-tuple of ints)
14:    the ending (row, column) indices of the token (a 2-tuple of ints)
15:    the original line (string)
16:
17:It is designed to match the working of the Python tokenizer exactly, except
18:that it produces COMMENT tokens for comments and gives type OP for all
19:operators
20:
21:Older entry points
22:    tokenize_loop(readline, tokeneater)
23:    tokenize(readline, tokeneater=printtoken)
24:are the same, except instead of generating tokens, tokeneater is a callback
25:function to which the 5 fields described above are passed as 5 arguments,
26:each time a new token is found."""
27:
28:__author__ = 'Ka-Ping Yee <ping@lfw.org>'
29:__credits__ = \
30:    'GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro'
31:
32:import re
33:from codecs import BOM_UTF8, lookup
34:from attr import dataclass
35:from blib2to3.pgen2.token import *
36:
37:from . import token
38:__all__ = [x for x in dir(token) if x[0] != '_'] + ["tokenize",
39:           "generate_tokens", "untokenize"]
40:del token
41:
42:try:
43:    bytes
44:except NameError:
45:    # Support bytes type in Python <= 2.5, so 2to3 turns itself into
46:    # valid Python 3 code.
47:    bytes = str
48:
49:def group(*choices): return '(' + '|'.join(choices) + ')'
50:def any(*choices): return group(*choices) + '*'
51:def maybe(*choices): return group(*choices) + '?'
52:def _combinations(*l):
53:    return set(
54:        x + y for x in l for y in l + ("",) if x.casefold() != y.casefold()
55:    )
56:
57:Whitespace = r'[ \f\t]*'
58:Comment = r'#[^\r\n]*'
59:Ignore = Whitespace + any(r'\\\r?\n' + Whitespace) + maybe(Comment)
60:Name = r'\w+'  # this is invalid but it's fine because Name comes after Number in all groups
61:
62:Binnumber = r'0[bB]_?[01]+(?:_[01]+)*'
63:Hexnumber = r'0[xX]_?[\da-fA-F]+(?:_[\da-fA-F]+)*[lL]?'
64:Octnumber = r'0[oO]?_?[0-7]+(?:_[0-7]+)*[lL]?'
65:Decnumber = group(r'[1-9]\d*(?:_\d+)*[lL]?', '0[lL]?')
66:Intnumber = group(Binnumber, Hexnumber, Octnumber, Decnumber)
67:Exponent = r'[eE][-+]?\d+(?:_\d+)*'
68:Pointfloat = group(r'\d+(?:_\d+)*\.(?:\d+(?:_\d+)*)?', r'\.\d+(?:_\d+)*') + maybe(Exponent)
69:Expfloat = r'\d+(?:_\d+)*' + Exponent
70:Floatnumber = group(Pointfloat, Expfloat)
71:Imagnumber = group(r'\d+(?:_\d+)*[jJ]', Floatnumber + r'[jJ]')
72:Number = group(Imagnumber, Floatnumber, Intnumber)
73:
74:# Tail end of ' string.
75:Single = r"[^'\\]*(?:\\.[^'\\]*)*'"
76:# Tail end of " string.
77:Double = r'[^"\\]*(?:\\.[^"\\]*)*"'
78:# Tail end of ''' string.
79:Single3 = r"[^'\\]*(?:(?:\\.|'(?!''))[^'\\]*)*'''"
80:# Tail end of """ string.
81:Double3 = r'[^"\\]*(?:(?:\\.|"(?!""))[^"\\]*)*"""'
82:_litprefix = r"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?"
83:Triple = group(_litprefix + "'''", _litprefix + '"""')
84:# Single-line ' or " string.
85:String = group(_litprefix + r"'[^\n'\\]*(?:\\.[^\n'\\]*)*'",
86:               _litprefix + r'"[^\n"\\]*(?:\\.[^\n"\\]*)*"')
87:
88:# Because of leftmost-then-longest match semantics, be sure to put the
89:# longest operators first (e.g., if = came before ==, == would get
90:# recognized as two instances of =).
91:Operator = group(r"\*\*=?", r">>=?", r"<<=?", r"<>", r"!=",
92:                 r"//=?", r"->",
93:                 r"[+\-*/%&@|^=<>]=?",
94:                 r"~")
95:
96:Bracket = '[][(){}]'
97:Special = group(r'\r?\n', r'[:;.,`@]')
98:Funny = group(Operator, Bracket, Special)
99:
100:PlainToken = group(Number, Funny, String, Name)
101:Token = Ignore + PlainToken
102:
103:# First (or only) line of ' or " string.
104:ContStr = group(_litprefix + r"'[^\n'\\]*(?:\\.[^\n'\\]*)*" +
105:                group("'", r'\\\r?\n'),
106:                _litprefix + r'"[^\n"\\]*(?:\\.[^\n"\\]*)*' +
107:                group('"', r'\\\r?\n'))
108:PseudoExtras = group(r'\\\r?\n', Comment, Triple)
109:PseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)
110:
111:tokenprog = re.compile(Token, re.UNICODE)
112:pseudoprog = re.compile(PseudoToken, re.UNICODE)
113:single3prog = re.compile(Single3)
114:double3prog = re.compile(Double3)
115:
116:_strprefixes = (
117:    _combinations('r', 'R', 'f', 'F') |
118:    _combinations('r', 'R', 'b', 'B') |
119:    {'u', 'U', 'ur', 'uR', 'Ur', 'UR'}
120:)
121:
122:endprogs = {"'": re.compile(Single), '"': re.compile(Double),
123:            "'''": single3prog, '"""': double3prog,
124:            **{f"{prefix}'''": single3prog for prefix in _strprefixes},
125:            **{f'{prefix}"""': double3prog for prefix in _strprefixes},
126:            **{prefix: None for prefix in _strprefixes}}
127:
128:triple_quoted = (
129:    {"'''", '"""'} |
130:    {f"{prefix}'''" for prefix in _strprefixes} |
131:    {f'{prefix}"""' for prefix in _strprefixes}
132:)
133:single_quoted = (
134:    {"'", '"'} |
135:    {f"{prefix}'" for prefix in _strprefixes} |
136:    {f'{prefix}"' for prefix in _strprefixes}
137:)
138:
139:tabsize = 8
140:
141:@dataclass(frozen=True)
142:class TokenizerConfig:
143:    async_is_reserved_keyword: bool = False
144:
145:class TokenError(Exception): pass
146:
147:class StopTokenizing(Exception): pass
148:
149:def printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line): # for testing
150:    (srow, scol) = xxx_todo_changeme
151:    (erow, ecol) = xxx_todo_changeme1
152:    print("%d,%d-%d,%d:\t%s\t%s" % \
153:        (srow, scol, erow, ecol, tok_name[type], repr(token)))
154:
155:def tokenize(readline, tokeneater=printtoken):
156:    """
157:    The tokenize() function accepts two parameters: one representing the
158:    input stream, and one providing an output mechanism for tokenize().
159:
160:    The first parameter, readline, must be a callable object which provides
161:    the same interface as the readline() method of built-in file objects.
162:    Each call to the function should return one line of input as a string.
163:
164:    The second parameter, tokeneater, must also be a callable object. It is
165:    called once for each token, with five arguments, corresponding to the
166:    tuples generated by generate_tokens().
167:    """
168:    try:
169:        tokenize_loop(readline, tokeneater)
170:    except StopTokenizing:
171:        pass
172:
173:# backwards compatible interface
174:def tokenize_loop(readline, tokeneater):
175:    for token_info in generate_tokens(readline):
176:        tokeneater(*token_info)
177:
178:class Untokenizer:
179:
180:    def __init__(self):
181:        self.tokens = []
182:        self.prev_row = 1
183:        self.prev_col = 0
184:
185:    def add_whitespace(self, start):
186:        row, col = start
187:        assert row <= self.prev_row
188:        col_offset = col - self.prev_col
189:        if col_offset:
190:            self.tokens.append(" " * col_offset)
191:
192:    def untokenize(self, iterable):
193:        for t in iterable:
194:            if len(t) == 2:
195:                self.compat(t, iterable)
196:                break
197:            tok_type, token, start, end, line = t
198:            self.add_whitespace(start)
199:            self.tokens.append(token)
200:            self.prev_row, self.prev_col = end
201:            if tok_type in (NEWLINE, NL):
202:                self.prev_row += 1
203:                self.prev_col = 0
204:        return "".join(self.tokens)
205:
206:    def compat(self, token, iterable):
207:        startline = False
208:        indents = []
209:        toks_append = self.tokens.append
210:        toknum, tokval = token
211:        if toknum in (NAME, NUMBER):
212:            tokval += ' '
213:        if toknum in (NEWLINE, NL):
214:            startline = True
215:        for tok in iterable:
216:            toknum, tokval = tok[:2]
217:
218:            if toknum in (NAME, NUMBER, ASYNC, AWAIT):
219:                tokval += ' '
220:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
220:
221:            if toknum == INDENT:
222:                indents.append(tokval)
223:                continue
224:            elif toknum == DEDENT:
225:                indents.pop()
226:                continue
227:            elif toknum in (NEWLINE, NL):
228:                startline = True
229:            elif startline and indents:
230:                toks_append(indents[-1])
231:                startline = False
232:            toks_append(tokval)
233:
234:cookie_re = re.compile(r'^[ \t\f]*#.*?coding[:=][ \t]*([-\w.]+)', re.ASCII)
235:blank_re = re.compile(br'^[ \t\f]*(?:[#\r\n]|$)', re.ASCII)
236:
237:def _get_normal_name(orig_enc):
238:    """Imitates get_normal_name in tokenizer.c."""
239:    # Only care about the first 12 characters.
240:    enc = orig_enc[:12].lower().replace("_", "-")
241:    if enc == "utf-8" or enc.startswith("utf-8-"):
242:        return "utf-8"
243:    if enc in ("latin-1", "iso-8859-1", "iso-latin-1") or \
244:       enc.startswith(("latin-1-", "iso-8859-1-", "iso-latin-1-")):
245:        return "iso-8859-1"
246:    return orig_enc
247:
248:def detect_encoding(readline):
249:    """
250:    The detect_encoding() function is used to detect the encoding that should
251:    be used to decode a Python source file. It requires one argument, readline,
252:    in the same way as the tokenize() generator.
253:
254:    It will call readline a maximum of twice, and return the encoding used
255:    (as a string) and a list of any lines (left as bytes) it has read
256:    in.
257:
258:    It detects the encoding from the presence of a utf-8 bom or an encoding
259:    cookie as specified in pep-0263. If both a bom and a cookie are present, but
260:    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid
261:    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,
262:    'utf-8-sig' is returned.
263:
264:    If no encoding is specified, then the default of 'utf-8' will be returned.
265:    """
266:    bom_found = False
267:    encoding = None
268:    default = 'utf-8'
269:    def read_or_stop():
270:        try:
271:            return readline()
272:        except StopIteration:
273:            return bytes()
274:
275:    def find_cookie(line):
276:        try:
277:            line_string = line.decode('ascii')
278:        except UnicodeDecodeError:
279:            return None
280:        match = cookie_re.match(line_string)
281:        if not match:
282:            return None
283:        encoding = _get_normal_name(match.group(1))
284:        try:
285:            codec = lookup(encoding)
286:        except LookupError:
287:            # This behaviour mimics the Python interpreter
288:            raise SyntaxError("unknown encoding: " + encoding)
289:
290:        if bom_found:
291:            if codec.name != 'utf-8':
292:                # This behaviour mimics the Python interpreter
293:                raise SyntaxError('encoding problem: utf-8')
294:            encoding += '-sig'
295:        return encoding
296:
297:    first = read_or_stop()
298:    if first.startswith(BOM_UTF8):
299:        bom_found = True
300:        first = first[3:]
301:        default = 'utf-8-sig'
302:    if not first:
303:        return default, []
304:
305:    encoding = find_cookie(first)
306:    if encoding:
307:        return encoding, [first]
308:    if not blank_re.match(first):
309:        return default, [first]
310:
311:    second = read_or_stop()
312:    if not second:
313:        return default, [first]
314:
315:    encoding = find_cookie(second)
316:    if encoding:
317:        return encoding, [first, second]
318:
319:    return default, [first, second]
320:
321:def untokenize(iterable):
322:    """Transform tokens back into Python source code.
323:
324:    Each element returned by the iterable must be a token sequence
325:    with at least two elements, a token number and token value.  If
326:    only two tokens are passed, the resulting output is poor.
327:
328:    Round-trip invariant for full input:
329:        Untokenized source will match input source exactly
330:
331:    Round-trip invariant for limited intput:
332:        # Output text will tokenize the back to the input
333:        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
334:        newcode = untokenize(t1)
335:        readline = iter(newcode.splitlines(1)).next
336:        t2 = [tok[:2] for tokin generate_tokens(readline)]
337:        assert t1 == t2
338:    """
339:    ut = Untokenizer()
340:    return ut.untokenize(iterable)
341:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
343:    """
344:    The generate_tokens() generator requires one argument, readline, which
345:    must be a callable object which provides the same interface as the
346:    readline() method of built-in file objects. Each call to the function
347:    should return one line of input as a string.  Alternately, readline
348:    can be a callable function terminating with StopIteration:
349:        readline = open(myfile).next    # Example of alternate readline
350:
351:    The generator produces 5-tuples with these members: the token type; the
352:    token string; a 2-tuple (srow, scol) of ints specifying the row and
353:    column where the token begins in the source; a 2-tuple (erow, ecol) of
354:    ints specifying the row and column where the token ends in the source;
355:    and the line on which the token was found. The line passed is the
356:    logical line; continuation lines are included.
357:    """
358:    lnum = parenlev = continued = 0
359:    numchars = '0123456789'
360:    contstr, needcont = '', 0
361:    contline = None
362:    indents = [0]
363:
364:    # If we know we're parsing 3.7+, we can unconditionally parse `async` and
365:    # `await` as keywords.
366:    async_is_reserved_keyword = config.async_is_reserved_keyword
367:    # 'stashed' and 'async_*' are used for async/await parsing
368:    stashed = None
369:    async_def = False
370:    async_def_indent = 0
371:    async_def_nl = False
372:
373:    while 1:                                   # loop over lines in stream
374:        try:
375:            line = readline()
376:        except StopIteration:
377:            line = ''
378:        lnum = lnum + 1
379:        pos, max = 0, len(line)
380:
381:        if contstr:                            # continued string
382:            if not line:
383:                raise TokenError("EOF in multi-line string", strstart)
384:            endmatch = endprog.match(line)
385:            if endmatch:
386:                pos = end = endmatch.end(0)
387:                yield (STRING, contstr + line[:end],
388:                       strstart, (lnum, end), contline + line)
389:                contstr, needcont = '', 0
390:                contline = None
391:            elif needcont and line[-2:] != '\\\n' and line[-3:] != '\\\r\n':
392:                yield (ERRORTOKEN, contstr + line,
393:                           strstart, (lnum, len(line)), contline)
394:                contstr = ''
395:                contline = None
396:                continue
397:            else:
398:                contstr = contstr + line
399:                contline = contline + line
400:                continue
401:
402:        elif parenlev == 0 and not continued:  # new statement
403:            if not line: break
404:            column = 0
405:            while pos < max:                   # measure leading whitespace
406:                if line[pos] == ' ': column = column + 1
407:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize
408:                elif line[pos] == '\f': column = 0
409:                else: break
410:                pos = pos + 1
411:            if pos == max: break
412:
413:            if stashed:
414:                yield stashed
415:                stashed = None
416:
417:            if line[pos] in '\r\n':            # skip blank lines
418:                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)
419:                continue
420:
421:            if line[pos] == '#':               # skip comments
422:                comment_token = line[pos:].rstrip('\r\n')
423:                nl_pos = pos + len(comment_token)
424:                yield (COMMENT, comment_token,
425:                        (lnum, pos), (lnum, pos + len(comment_token)), line)
426:                yield (NL, line[nl_pos:],
427:                        (lnum, nl_pos), (lnum, len(line)), line)
428:                continue
429:
430:            if column > indents[-1]:           # count indents
431:                indents.append(column)
432:                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)
433:
434:            while column < indents[-1]:        # count dedents
435:                if column not in indents:
436:                    raise IndentationError(
437:                        "unindent does not match any outer indentation level",
438:                        ("<tokenize>", lnum, pos, line))
439:                indents = indents[:-1]
440:
441:                if async_def and async_def_indent >= indents[-1]:
442:                    async_def = False
443:                    async_def_nl = False
444:                    async_def_indent = 0
445:
446:                yield (DEDENT, '', (lnum, pos), (lnum, pos), line)
447:
448:            if async_def and async_def_nl and async_def_indent >= indents[-1]:
449:                async_def = False
450:                async_def_nl = False
451:                async_def_indent = 0
452:
453:        else:                                  # continued statement
454:            if not line:
455:                raise TokenError("EOF in multi-line statement", (lnum, 0))
456:            continued = 0
457:
458:        while pos < max:
459:            pseudomatch = pseudoprog.match(line, pos)
460:            if pseudomatch:                                # scan for tokens
461:                start, end = pseudomatch.span(1)
462:                spos, epos, pos = (lnum, start), (lnum, end), end
463:                token, initial = line[start:end], line[start]
464:
465:                if initial in numchars or \
466:                   (initial == '.' and token != '.'):      # ordinary number
467:                    yield (NUMBER, token, spos, epos, line)
468:                elif initial in '\r\n':
469:                    newline = NEWLINE
470:                    if parenlev > 0:
471:                        newline = NL
472:                    elif async_def:
473:                        async_def_nl = True
474:                    if stashed:
475:                        yield stashed
476:                        stashed = None
477:                    yield (newline, token, spos, epos, line)
478:
479:                elif initial == '#':
480:                    assert not token.endswith("\n")
481:                    if stashed:
482:                        yield stashed
483:                        stashed = None
484:                    yield (COMMENT, token, spos, epos, line)
485:                elif token in triple_quoted:
486:                    endprog = endprogs[token]
487:                    endmatch = endprog.match(line, pos)
488:                    if endmatch:                           # all on one line
489:                        pos = endmatch.end(0)
490:                        token = line[start:pos]
491:                        if stashed:
492:                            yield stashed
493:                            stashed = None
494:                        yield (STRING, token, spos, (lnum, pos), line)
495:                    else:
496:                        strstart = (lnum, start)           # multiple lines
497:                        contstr = line[start:]
498:                        contline = line
499:                        break
500:                elif initial in single_quoted or \
501:                    token[:2] in single_quoted or \
502:                    token[:3] in single_quoted:
503:                    if token[-1] == '\n':                  # continued string
504:                        strstart = (lnum, start)
505:                        endprog = (endprogs[initial] or endprogs[token[1]] or
506:                                   endprogs[token[2]])
507:                        contstr, needcont = line[start:], 1
508:                        contline = line
509:                        break
510:                    else:                                  # ordinary string
511:                        if stashed:
512:                            yield stashed
513:                            stashed = None
514:                        yield (STRING, token, spos, epos, line)
515:                elif initial.isidentifier():               # ordinary name
516:                    if token in ('async', 'await'):
517:                        if async_is_reserved_keyword or async_def:
518:                            yield (ASYNC if token == 'async' else AWAIT,
519:                                   token, spos, epos, line)
520:                            continue
521:
522:                    tok = (NAME, token, spos, epos, line)
523:                    if token == 'async' and not stashed:
524:                        stashed = tok
525:                        continue
526:
527:                    if token in ('def', 'for'):
528:                        if (stashed
529:                                and stashed[0] == NAME
530:                                and stashed[1] == 'async'):
531:
532:                            if token == 'def':
533:                                async_def = True
534:                                async_def_indent = indents[-1]
535:
536:                            yield (ASYNC, stashed[1],
537:                                   stashed[2], stashed[3],
538:                                   stashed[4])
539:                            stashed = None
540:
541:                    if stashed:
542:                        yield stashed
543:                        stashed = None
544:
545:                    yield tok
546:                elif initial == '\\':                      # continued stmt
547:                    # This yield is new; needed for better idempotency:
548:                    if stashed:
549:                        yield stashed
550:                        stashed = None
551:                    yield (NL, token, spos, (lnum, pos), line)
552:                    continued = 1
553:                else:
554:                    if initial in '([{': parenlev = parenlev + 1
555:                    elif initial in ')]}': parenlev = parenlev - 1
556:                    if stashed:
557:                        yield stashed
558:                        stashed = None
559:                    yield (OP, token, spos, epos, line)

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi
1:# Stubs for lib2to3.pgen2.tokenize (Python 3.6)
2:# NOTE: Only elements from __all__ are present.
3:
4:from typing import Callable, Iterable, Iterator, List, Text, Tuple
5:from attr import dataclass
6:from blib2to3.pgen2.token import *  # noqa
7:
8:
9:_Coord = Tuple[int, int]
10:_TokenEater = Callable[[int, Text, _Coord, _Coord, Text], None]
11:_TokenInfo = Tuple[int, Text, _Coord, _Coord, Text]
12:
13:@dataclass(frozen=True)
14:class TokenizerConfig:
15:    async_is_reserved_keyword: bool = False
16:
17:class TokenError(Exception): ...
18:class StopTokenizing(Exception): ...
19:
20:def tokenize(readline: Callable[[], Text], tokeneater: _TokenEater = ...) -> None: ...
21:
22:class Untokenizer:
23:    tokens: List[Text]
24:    prev_row: int
25:    prev_col: int
26:    def __init__(self) -> None: ...
27:    def add_whitespace(self, start: _Coord) -> None: ...
28:    def untokenize(self, iterable: Iterable[_TokenInfo]) -> Text: ...
29:    def compat(self, token: Tuple[int, Text], iterable: Iterable[_TokenInfo]) -> None: ...
30:
31:def untokenize(iterable: Iterable[_TokenInfo]) -> Text: ...
32:def generate_tokens(
33:    readline: Callable[[], Text]
34:) -> Iterator[_TokenInfo]: ...

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/data/python37.py
1:#!/usr/bin/env python3.7
2:
3:
4:def f():
5:    return (i * 2 async for i in arange(42))
6:
7:
8:def g():
9:    return (
10:        something_long * something_long
11:        async for something_long in async_generator(with_an_argument)
12:    )
13:
14:
15:async def func():
16:    if test:
17:        out_batched = [
18:            i
19:            async for i in aitertools._async_map(
20:                self.async_inc, arange(8), batch_size=3
21:            )
22:        ]
23:
24:
25:def awaited_generator_value(n):
26:    return (await awaitable for awaitable in awaitable_list)
27:
28:
29:def make_arange(n):
30:    return (i * 2 for i in range(n) if await wrap(i))
31:
32:
33:# output
34:
35:
36:#!/usr/bin/env python3.7
37:
38:
39:def f():
40:    return (i * 2 async for i in arange(42))
41:
42:
43:def g():
44:    return (
45:        something_long * something_long
46:        async for something_long in async_generator(with_an_argument)
47:    )
48:
49:
50:async def func():
51:    if test:
52:        out_batched = [
53:            i
54:            async for i in aitertools._async_map(
55:                self.async_inc, arange(8), batch_size=3
56:            )
57:        ]
58:
59:
60:def awaited_generator_value(n):
61:    return (await awaitable for awaitable in awaitable_list)
62:
63:
64:def make_arange(n):
65:    return (i * 2 for i in range(n) if await wrap(i))

Calling tool: globTool
**/blib2to3/pgen2/driver.pyi
Matched files:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.pyi

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.pyi
1:# Stubs for lib2to3.pgen2.driver (Python 3.6)
2:
3:import os
4:import sys
5:from typing import Any, Callable, IO, Iterable, List, Optional, Text, Tuple, Union
6:
7:from logging import Logger
8:from blib2to3.pytree import _Convert, _NL
9:from blib2to3.pgen2 import _Path
10:from blib2to3.pgen2.grammar import Grammar
11:from blib2to3.pgen2.tokenize import TokenizerConfig
12:
13:
14:class Driver:
15:    grammar: Grammar
16:    logger: Logger
17:    convert: _Convert
18:    def __init__(
19:        self,
20:        grammar: Grammar,
21:        convert: Optional[_Convert] = ...,
22:        logger: Optional[Logger] = ...,
23:        tokenizer_config: TokenizerConfig = ...
24:    ) -> None: ...
25:    def parse_tokens(self, tokens: Iterable[Any], debug: bool = ...) -> _NL: ...
26:    def parse_stream_raw(self, stream: IO[Text], debug: bool = ...) -> _NL: ...
27:    def parse_stream(self, stream: IO[Text], debug: bool = ...) -> _NL: ...
28:    def parse_file(self, filename: _Path, encoding: Optional[Text] = ..., debug: bool = ...) -> _NL: ...
29:    def parse_string(self, text: Text, debug: bool = ...) -> _NL: ...
30:
31:def load_grammar(gt: Text = ..., gp: Optional[Text] = ..., save: bool = ..., force: bool = ..., logger: Optional[Logger] = ...) -> Grammar: ...

Calling tool: grepTool
get_grammars\(
No files found. Call grepTool again with a more generic pattern.

Calling tool: grepTool
ASYNC_IS_(VALID_IDENTIFIER|RESERVED_KEYWORD)
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:142:    ASYNC_IS_VALID_IDENTIFIER = 6
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:143:    ASYNC_IS_RESERVED_KEYWORD = 7
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:147:    TargetVersion.PY27: {Feature.ASYNC_IS_VALID_IDENTIFIER},
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:148:    TargetVersion.PY33: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:149:    TargetVersion.PY34: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:153:        Feature.ASYNC_IS_VALID_IDENTIFIER,
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:161:        Feature.ASYNC_IS_VALID_IDENTIFIER,
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:169:        Feature.ASYNC_IS_RESERVED_KEYWORD,
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:177:        Feature.ASYNC_IS_RESERVED_KEYWORD,
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:800:        if not supports_feature(target_versions, Feature.ASYNC_IS_VALID_IDENTIFIER):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:808:        if not supports_feature(target_versions, Feature.ASYNC_IS_RESERVED_KEYWORD):

Calling tool: grepTool
generate_tokens\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:6:generate_tokens(readline) is a generator that breaks a stream of
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:166:    tuples generated by generate_tokens().
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:175:    for token_info in generate_tokens(readline):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:333:        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:336:        t2 = [tok[:2] for tokin generate_tokens(readline)]
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:344:    The generate_tokens() generator requires one argument, readline, which
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py:107:        tokens = tokenize.generate_tokens(stream.readline, config=self.tokenizer_config)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py:121:        tokens = tokenize.generate_tokens(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/pgen.py:19:        self.generator = tokenize.generate_tokens(stream.readline)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi:32:def generate_tokens(

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/pgen.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Pgen imports
5:from . import grammar, token, tokenize
6:
7:class PgenGrammar(grammar.Grammar):
8:    pass
9:
10:class ParserGenerator(object):
11:
12:    def __init__(self, filename, stream=None):
13:        close_stream = None
14:        if stream is None:
15:            stream = open(filename)
16:            close_stream = stream.close
17:        self.filename = filename
18:        self.stream = stream
19:        self.generator = tokenize.generate_tokens(stream.readline)
20:        self.gettoken() # Initialize lookahead
21:        self.dfas, self.startsymbol = self.parse()
22:        if close_stream is not None:
23:            close_stream()
24:        self.first = {} # map from symbol name to set of tokens
25:        self.addfirstsets()
26:
27:    def make_grammar(self):
28:        c = PgenGrammar()
29:        names = list(self.dfas.keys())
30:        names.sort()
31:        names.remove(self.startsymbol)
32:        names.insert(0, self.startsymbol)
33:        for name in names:
34:            i = 256 + len(c.symbol2number)
35:            c.symbol2number[name] = i
36:            c.number2symbol[i] = name
37:        for name in names:
38:            dfa = self.dfas[name]
39:            states = []
40:            for state in dfa:
41:                arcs = []
42:                for label, next in sorted(state.arcs.items()):
43:                    arcs.append((self.make_label(c, label), dfa.index(next)))
44:                if state.isfinal:
45:                    arcs.append((0, dfa.index(state)))
46:                states.append(arcs)
47:            c.states.append(states)
48:            c.dfas[c.symbol2number[name]] = (states, self.make_first(c, name))
49:        c.start = c.symbol2number[self.startsymbol]
50:        return c
51:
52:    def make_first(self, c, name):
53:        rawfirst = self.first[name]
54:        first = {}
55:        for label in sorted(rawfirst):
56:            ilabel = self.make_label(c, label)
57:            ##assert ilabel not in first # XXX failed on <> ... !=
58:            first[ilabel] = 1
59:        return first
60:

Calling tool: createTodo
Todo List Created
Todo List (19 items):
- [ ] Verify Feature enum additions: `ASYNC_IS_VALID_IDENTIFIER` and `ASYNC_IS_RESERVED_KEYWORD` exist and are only used to control async/await tokenization. (ID: todo-1)
- [ ] Check VERSION_TO_FEATURES mapping ensures mutual exclusivity: for each TargetVersion (PY27, PY33, PY34, PY35, PY36) include ASYNC_IS_VALID_IDENTIFIER, and for (PY37, PY38) include ASYNC_IS_RESERVED_KEYWORD; confirm exactly one of the two flags is present per version. (ID: todo-2)
- [ ] Confirm `ParserConfig` dataclass exists in `black.py` with fields `grammar: Grammar` and `tokenizer_config: TokenizerConfig = TokenizerConfig()` and that `TokenizerConfig` is imported from `blib2to3.pgen2.tokenize`. (ID: todo-3)
- [ ] Validate `get_parser_configs()` replaces `get_grammars()` and returns configurations as specified: when no target_versions, order is [Py3.7+ reserved, Py3.0-3.6 non-reserved, Py2.7 no print, Py2.7]; when all Python2 targets, only the two Py2 grammars; when any Python3 targets, only Py3 grammar with: reserved config added if ASYNC_IS_VALID_IDENTIFIER not supported, non-reserved config added if ASYNC_IS_RESERVED_KEYWORD not supported, in that order. (ID: todo-4)
- [ ] Ensure `lib2to3_parse()` iterates over `get_parser_configs()` (not grammars) and constructs `driver.Driver(grammar, pytree.convert, tokenizer_config=config)` accordingly, passing the tokenizer_config through. (ID: todo-5)
- [ ] Confirm `driver.Driver.__init__` accepts a `tokenizer_config` parameter with default `tokenize.TokenizerConfig()` and stores it on `self.tokenizer_config`. (ID: todo-6)
- [ ] Verify `driver.Driver.parse_stream_raw()` and `driver.Driver.parse_string()` call `tokenize.generate_tokens(..., config=self.tokenizer_config)`; ensure `parse_file()` uses `parse_stream()` and thus the config path is used. (ID: todo-7)
- [ ] Check `blib2to3/pgen2/tokenize.py` defines `TokenizerConfig` as a frozen attr dataclass with field `async_is_reserved_keyword: bool = False` and is imported where used. (ID: todo-8)
- [ ] Confirm `generate_tokens` signature in `tokenize.py` is updated to accept `config: TokenizerConfig = TokenizerConfig()` and that it reads `config.async_is_reserved_keyword` into a local flag. (ID: todo-9)
- [ ] Inspect tokenization logic for identifiers in `generate_tokens`: when `token in ('async','await')` it yields `ASYNC`/`AWAIT` tokens if `async_is_reserved_keyword` OR `async_def` is true; otherwise they remain NAME. Ensure existing `async def`/`async for` stashing logic remains intact and resets on dedent/newline as before. (ID: todo-10)
- [ ] Check all call sites of `generate_tokens` across the repo: updated calls pass `config` where needed (driver), and legacy calls (e.g., in `pgen.py` and `tokenize_loop`) still compile using the default parameter. (ID: todo-11)
- [ ] Verify `blib2to3/pgen2/tokenize.pyi` stub reflects the implementation changes: it defines `TokenizerConfig`, but its `generate_tokens` currently lacks the optional `config` parameter; flag this mismatch and ensure the stub's signature is updated to accept `config: TokenizerConfig = ...` to match the implementation. (ID: todo-12)
- [ ] Verify `blib2to3/pgen2/driver.pyi` stub matches implementation by including the `tokenizer_config` parameter in `Driver.__init__` (it should already do so). (ID: todo-13)
- [ ] Confirm `tests/data/python37.py` includes new examples using await in generator contexts: `awaited_generator_value` and `make_arange`, present in both source and output sections. (ID: todo-14)
- [ ] Validate `tests/test_black.py::test_async_as_identifier` expectations align with the new parser configs: default formatting works; with `--target-version py36` black can parse; with `--target-version py37` parsing fails due to async/await being reserved keywords. (ID: todo-15)
- [ ] Validate `tests/test_black.py::test_python37` expectations align with parser configs: with `--target-version py37` black can parse the file; with `--target-version py36` parsing should fail due to use of reserved async/await-only constructs (await inside generator). (ID: todo-16)
- [ ] Check imports added in diffs are present and correct: `black.py` imports `TokenizerConfig`; `tokenize.py` imports `dataclass` from `attr`; verify no missing imports or name conflicts. (ID: todo-17)
- [ ] Confirm that when `target_versions` is empty, `get_parser_configs()` tries reserved-keyword tokenization first to fix parsing of 3.7+ async/await in generators, with fallback to older modes if parsing fails. (ID: todo-18)
- [ ] Ensure error handling in `lib2to3_parse()` remains correct: on parse failure for all configs, it raises `InvalidInput` with a message containing the failing line/column, and `exc` is definitely set before being raised. (ID: todo-19)


=== PHASE 1 COMPLETE ===

=== PHASE 2: VALIDATION ===

Validating todo item 1 of 19
description: 'Verify Feature enum additions: `ASYNC_IS_VALID_IDENTIFIER` and `ASYNC_IS_RESERVED_KEYWORD` exist and are only used to control async/await tokenization.'
Calling tool: grepTool
ASYNC_IS_VALID_IDENTIFIER
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:142:    ASYNC_IS_VALID_IDENTIFIER = 6
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:147:    TargetVersion.PY27: {Feature.ASYNC_IS_VALID_IDENTIFIER},
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:148:    TargetVersion.PY33: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:149:    TargetVersion.PY34: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:153:        Feature.ASYNC_IS_VALID_IDENTIFIER,
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:161:        Feature.ASYNC_IS_VALID_IDENTIFIER,
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:800:        if not supports_feature(target_versions, Feature.ASYNC_IS_VALID_IDENTIFIER):

Calling tool: grepTool
ASYNC_IS_RESERVED_KEYWORD
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:143:    ASYNC_IS_RESERVED_KEYWORD = 7
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:169:        Feature.ASYNC_IS_RESERVED_KEYWORD,
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:177:        Feature.ASYNC_IS_RESERVED_KEYWORD,
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:808:        if not supports_feature(target_versions, Feature.ASYNC_IS_RESERVED_KEYWORD):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
120:    PY34 = 4
121:    PY35 = 5
122:    PY36 = 6
123:    PY37 = 7
124:    PY38 = 8
125:
126:    def is_python2(self) -> bool:
127:        return self is TargetVersion.PY27
128:
129:
130:PY36_VERSIONS = {TargetVersion.PY36, TargetVersion.PY37, TargetVersion.PY38}
131:
132:
133:class Feature(Enum):
134:    # All string literals are unicode
135:    UNICODE_LITERALS = 1
136:    F_STRINGS = 2
137:    NUMERIC_UNDERSCORES = 3
138:    TRAILING_COMMA_IN_CALL = 4
139:    TRAILING_COMMA_IN_DEF = 5
140:    # The following two feature-flags are mutually exclusive, and exactly one should be
141:    # set for every version of python.
142:    ASYNC_IS_VALID_IDENTIFIER = 6
143:    ASYNC_IS_RESERVED_KEYWORD = 7
144:
145:
146:VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {
147:    TargetVersion.PY27: {Feature.ASYNC_IS_VALID_IDENTIFIER},
148:    TargetVersion.PY33: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
149:    TargetVersion.PY34: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
150:    TargetVersion.PY35: {
151:        Feature.UNICODE_LITERALS,
152:        Feature.TRAILING_COMMA_IN_CALL,
153:        Feature.ASYNC_IS_VALID_IDENTIFIER,
154:    },
155:    TargetVersion.PY36: {
156:        Feature.UNICODE_LITERALS,
157:        Feature.F_STRINGS,
158:        Feature.NUMERIC_UNDERSCORES,
159:        Feature.TRAILING_COMMA_IN_CALL,
160:        Feature.TRAILING_COMMA_IN_DEF,
161:        Feature.ASYNC_IS_VALID_IDENTIFIER,
162:    },
163:    TargetVersion.PY37: {
164:        Feature.UNICODE_LITERALS,
165:        Feature.F_STRINGS,
166:        Feature.NUMERIC_UNDERSCORES,
167:        Feature.TRAILING_COMMA_IN_CALL,
168:        Feature.TRAILING_COMMA_IN_DEF,
169:        Feature.ASYNC_IS_RESERVED_KEYWORD,
170:    },
171:    TargetVersion.PY38: {
172:        Feature.UNICODE_LITERALS,
173:        Feature.F_STRINGS,
174:        Feature.NUMERIC_UNDERSCORES,
175:        Feature.TRAILING_COMMA_IN_CALL,
176:        Feature.TRAILING_COMMA_IN_DEF,
177:        Feature.ASYNC_IS_RESERVED_KEYWORD,
178:    },
179:}
180:
181:
182:@dataclass
183:class FileMode:
184:    target_versions: Set[TargetVersion] = Factory(set)
185:    line_length: int = DEFAULT_LINE_LENGTH
186:    string_normalization: bool = True
187:    is_pyi: bool = False
188:
189:    def get_cache_key(self) -> str:
190:        if self.target_versions:
191:            version_str = ",".join(
192:                str(version.value)
193:                for version in sorted(self.target_versions, key=lambda v: v.value)
194:            )
195:        else:
196:            version_str = "-"
197:        parts = [
198:            version_str,
199:            str(self.line_length),

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
732:    }
733:    for current_line in lines.visit(src_node):
734:        for _ in range(after):
735:            dst_contents += str(empty_line)
736:        before, after = elt.maybe_empty_lines(current_line)
737:        for _ in range(before):
738:            dst_contents += str(empty_line)
739:        for line in split_line(
740:            current_line, line_length=mode.line_length, features=split_line_features
741:        ):
742:            dst_contents += str(line)
743:    return dst_contents
744:
745:
746:def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:
747:    """Return a tuple of (decoded_contents, encoding, newline).
748:
749:    `newline` is either CRLF or LF but `decoded_contents` is decoded with
750:    universal newlines (i.e. only contains LF).
751:    """
752:    srcbuf = io.BytesIO(src)
753:    encoding, lines = tokenize.detect_encoding(srcbuf.readline)
754:    if not lines:
755:        return "", encoding, "\n"
756:
757:    newline = "\r\n" if b"\r\n" == lines[0][-2:] else "\n"
758:    srcbuf.seek(0)
759:    with io.TextIOWrapper(srcbuf, encoding) as tiow:
760:        return tiow.read(), encoding, newline
761:
762:
763:@dataclass(frozen=True)
764:class ParserConfig:
765:    grammar: Grammar
766:    tokenizer_config: TokenizerConfig = TokenizerConfig()
767:
768:
769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:
770:    if not target_versions:
771:        # No target_version specified, so try all grammars.
772:        return [
773:            # Python 3.7+
774:            ParserConfig(
775:                pygram.python_grammar_no_print_statement_no_exec_statement,
776:                TokenizerConfig(async_is_reserved_keyword=True),
777:            ),
778:            # Python 3.0-3.6
779:            ParserConfig(
780:                pygram.python_grammar_no_print_statement_no_exec_statement,
781:                TokenizerConfig(async_is_reserved_keyword=False),
782:            ),
783:            # Python 2.7 with future print_function import
784:            ParserConfig(pygram.python_grammar_no_print_statement),
785:            # Python 2.7
786:            ParserConfig(pygram.python_grammar),
787:        ]
788:    elif all(version.is_python2() for version in target_versions):
789:        # Python 2-only code, so try Python 2 grammars.
790:        return [
791:            # Python 2.7 with future print_function import
792:            ParserConfig(pygram.python_grammar_no_print_statement),
793:            # Python 2.7
794:            ParserConfig(pygram.python_grammar),
795:        ]
796:    else:
797:        # Python 3-compatible code, so only try Python 3 grammar.
798:        configs = []
799:        # If we have to parse both, try to parse async as a keyword first
800:        if not supports_feature(target_versions, Feature.ASYNC_IS_VALID_IDENTIFIER):
801:            # Python 3.7+
802:            configs.append(
803:                ParserConfig(
804:                    pygram.python_grammar_no_print_statement_no_exec_statement,
805:                    TokenizerConfig(async_is_reserved_keyword=True),
806:                )
807:            )
808:        if not supports_feature(target_versions, Feature.ASYNC_IS_RESERVED_KEYWORD):
809:            # Python 3.0-3.6
810:            configs.append(
811:                ParserConfig(
812:                    pygram.python_grammar_no_print_statement_no_exec_statement,
813:                    TokenizerConfig(async_is_reserved_keyword=False),
814:                )
815:            )
816:        # At least one of the above branches must have been taken, because every Python
817:        # version has exactly one of the two 'ASYNC_IS_*' flags
818:        return configs
819:
820:
821:def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:
822:    """Given a string with source, return the lib2to3 Node."""
823:    if src_txt[-1:] != "\n":
824:        src_txt += "\n"
825:
826:    for parser_config in get_parser_configs(set(target_versions)):
827:        drv = driver.Driver(
828:            parser_config.grammar,
829:            pytree.convert,
830:            tokenizer_config=parser_config.tokenizer_config,
831:        )
832:        try:
833:            result = drv.parse_string(src_txt, True)
834:            break
835:
836:        except ParseError as pe:
837:            lineno, column = pe.context[1]
838:            lines = src_txt.splitlines()
839:            try:
840:                faulty_line = lines[lineno - 1]
841:            except IndexError:
842:                faulty_line = "<line number missing in source>"
843:            exc = InvalidInput(f"Cannot parse: {lineno}:{column}: {faulty_line}")
844:    else:
845:        raise exc from None
846:
847:    if isinstance(result, Leaf):
848:        result = Node(syms.file_input, [result])
849:    return result
850:
851:

Calling tool: grepTool
def supports_feature\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:206:def supports_feature(target_versions: Set[TargetVersion], feature: Feature) -> bool:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
200:            str(int(self.string_normalization)),
201:            str(int(self.is_pyi)),
202:        ]
203:        return ".".join(parts)
204:
205:
206:def supports_feature(target_versions: Set[TargetVersion], feature: Feature) -> bool:
207:    return all(feature in VERSION_TO_FEATURES[version] for version in target_versions)
208:
209:
210:def read_pyproject_toml(
211:    ctx: click.Context, param: click.Parameter, value: Union[str, int, bool, None]
212:) -> Optional[str]:
213:    """Inject Black configuration from "pyproject.toml" into defaults in `ctx`.
214:
215:    Returns the path to a successfully found and read configuration file, None
216:    otherwise.
217:    """
218:    assert not isinstance(value, (int, bool)), "Invalid parameter type passed"
219:    if not value:
220:        root = find_project_root(ctx.params.get("src", ()))
221:        path = root / "pyproject.toml"
222:        if path.is_file():
223:            value = str(path)
224:        else:
225:            return None
226:
227:    try:
228:        pyproject_toml = toml.load(value)
229:        config = pyproject_toml.get("tool", {}).get("black", {})
230:    except (toml.TomlDecodeError, OSError) as e:
231:        raise click.FileError(
232:            filename=value, hint=f"Error reading configuration file: {e}"
233:        )
234:
235:    if not config:
236:        return None
237:
238:    if ctx.default_map is None:
239:        ctx.default_map = {}
240:    ctx.default_map.update(  # type: ignore  # bad types in .pyi
241:        {k.replace("--", "").replace("-", "_"): v for k, v in config.items()}
242:    )
243:    return value
244:
245:
246:@click.command(context_settings=dict(help_option_names=["-h", "--help"]))
247:@click.option("-c", "--code", type=str, help="Format the code passed in as a string.")
248:@click.option(
249:    "-l",
250:    "--line-length",
251:    type=int,
252:    default=DEFAULT_LINE_LENGTH,
253:    help="How many characters per line to allow.",
254:    show_default=True,
255:)
256:@click.option(
257:    "-t",
258:    "--target-version",
259:    type=click.Choice([v.name.lower() for v in TargetVersion]),

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
320:
321:def untokenize(iterable):
322:    """Transform tokens back into Python source code.
323:
324:    Each element returned by the iterable must be a token sequence
325:    with at least two elements, a token number and token value.  If
326:    only two tokens are passed, the resulting output is poor.
327:
328:    Round-trip invariant for full input:
329:        Untokenized source will match input source exactly
330:
331:    Round-trip invariant for limited intput:
332:        # Output text will tokenize the back to the input
333:        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
334:        newcode = untokenize(t1)
335:        readline = iter(newcode.splitlines(1)).next
336:        t2 = [tok[:2] for tokin generate_tokens(readline)]
337:        assert t1 == t2
338:    """
339:    ut = Untokenizer()
340:    return ut.untokenize(iterable)
341:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
343:    """
344:    The generate_tokens() generator requires one argument, readline, which
345:    must be a callable object which provides the same interface as the
346:    readline() method of built-in file objects. Each call to the function
347:    should return one line of input as a string.  Alternately, readline
348:    can be a callable function terminating with StopIteration:
349:        readline = open(myfile).next    # Example of alternate readline
350:
351:    The generator produces 5-tuples with these members: the token type; the
352:    token string; a 2-tuple (srow, scol) of ints specifying the row and
353:    column where the token begins in the source; a 2-tuple (erow, ecol) of
354:    ints specifying the row and column where the token ends in the source;
355:    and the line on which the token was found. The line passed is the
356:    logical line; continuation lines are included.
357:    """
358:    lnum = parenlev = continued = 0
359:    numchars = '0123456789'
360:    contstr, needcont = '', 0
361:    contline = None
362:    indents = [0]
363:
364:    # If we know we're parsing 3.7+, we can unconditionally parse `async` and
365:    # `await` as keywords.
366:    async_is_reserved_keyword = config.async_is_reserved_keyword
367:    # 'stashed' and 'async_*' are used for async/await parsing
368:    stashed = None
369:    async_def = False
370:    async_def_indent = 0
371:    async_def_nl = False
372:
373:    while 1:                                   # loop over lines in stream
374:        try:
375:            line = readline()
376:        except StopIteration:
377:            line = ''
378:        lnum = lnum + 1
379:        pos, max = 0, len(line)
380:
381:        if contstr:                            # continued string
382:            if not line:
383:                raise TokenError("EOF in multi-line string", strstart)
384:            endmatch = endprog.match(line)
385:            if endmatch:
386:                pos = end = endmatch.end(0)
387:                yield (STRING, contstr + line[:end],
388:                       strstart, (lnum, end), contline + line)
389:                contstr, needcont = '', 0
390:                contline = None
391:            elif needcont and line[-2:] != '\\\n' and line[-3:] != '\\\r\n':
392:                yield (ERRORTOKEN, contstr + line,
393:                           strstart, (lnum, len(line)), contline)
394:                contstr = ''
395:                contline = None
396:                continue
397:            else:
398:                contstr = contstr + line
399:                contline = contline + line
400:                continue
401:
402:        elif parenlev == 0 and not continued:  # new statement
403:            if not line: break
404:            column = 0
405:            while pos < max:                   # measure leading whitespace
406:                if line[pos] == ' ': column = column + 1
407:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize
408:                elif line[pos] == '\f': column = 0
409:                else: break
410:                pos = pos + 1
411:            if pos == max: break
412:
413:            if stashed:
414:                yield stashed
415:                stashed = None
416:
417:            if line[pos] in '\r\n':            # skip blank lines
418:                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)
419:                continue
420:
421:            if line[pos] == '#':               # skip comments
422:                comment_token = line[pos:].rstrip('\r\n')
423:                nl_pos = pos + len(comment_token)
424:                yield (COMMENT, comment_token,
425:                        (lnum, pos), (lnum, pos + len(comment_token)), line)
426:                yield (NL, line[nl_pos:],
427:                        (lnum, nl_pos), (lnum, len(line)), line)
428:                continue
429:
430:            if column > indents[-1]:           # count indents
431:                indents.append(column)
432:                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)
433:
434:            while column < indents[-1]:        # count dedents
435:                if column not in indents:
436:                    raise IndentationError(
437:                        "unindent does not match any outer indentation level",
438:                        ("<tokenize>", lnum, pos, line))
439:                indents = indents[:-1]
440:
441:                if async_def and async_def_indent >= indents[-1]:
442:                    async_def = False
443:                    async_def_nl = False
444:                    async_def_indent = 0
445:
446:                yield (DEDENT, '', (lnum, pos), (lnum, pos), line)
447:
448:            if async_def and async_def_nl and async_def_indent >= indents[-1]:
449:                async_def = False
450:                async_def_nl = False
451:                async_def_indent = 0
452:
453:        else:                                  # continued statement
454:            if not line:
455:                raise TokenError("EOF in multi-line statement", (lnum, 0))
456:            continued = 0
457:
458:        while pos < max:
459:            pseudomatch = pseudoprog.match(line, pos)
460:            if pseudomatch:                                # scan for tokens
461:                start, end = pseudomatch.span(1)
462:                spos, epos, pos = (lnum, start), (lnum, end), end
463:                token, initial = line[start:end], line[start]
464:
465:                if initial in numchars or \
466:                   (initial == '.' and token != '.'):      # ordinary number
467:                    yield (NUMBER, token, spos, epos, line)
468:                elif initial in '\r\n':
469:                    newline = NEWLINE
470:                    if parenlev > 0:
471:                        newline = NL
472:                    elif async_def:
473:                        async_def_nl = True
474:                    if stashed:
475:                        yield stashed
476:                        stashed = None
477:                    yield (newline, token, spos, epos, line)
478:
479:                elif initial == '#':
480:                    assert not token.endswith("\n")
481:                    if stashed:
482:                        yield stashed
483:                        stashed = None
484:                    yield (COMMENT, token, spos, epos, line)
485:                elif token in triple_quoted:
486:                    endprog = endprogs[token]
487:                    endmatch = endprog.match(line, pos)
488:                    if endmatch:                           # all on one line
489:                        pos = endmatch.end(0)
490:                        token = line[start:pos]
491:                        if stashed:
492:                            yield stashed
493:                            stashed = None
494:                        yield (STRING, token, spos, (lnum, pos), line)
495:                    else:
496:                        strstart = (lnum, start)           # multiple lines
497:                        contstr = line[start:]
498:                        contline = line
499:                        break
500:                elif initial in single_quoted or \
501:                    token[:2] in single_quoted or \
502:                    token[:3] in single_quoted:
503:                    if token[-1] == '\n':                  # continued string
504:                        strstart = (lnum, start)
505:                        endprog = (endprogs[initial] or endprogs[token[1]] or
506:                                   endprogs[token[2]])
507:                        contstr, needcont = line[start:], 1
508:                        contline = line
509:                        break
510:                    else:                                  # ordinary string
511:                        if stashed:
512:                            yield stashed
513:                            stashed = None
514:                        yield (STRING, token, spos, epos, line)
515:                elif initial.isidentifier():               # ordinary name
516:                    if token in ('async', 'await'):
517:                        if async_is_reserved_keyword or async_def:
518:                            yield (ASYNC if token == 'async' else AWAIT,
519:                                   token, spos, epos, line)
520:                            continue
521:
522:                    tok = (NAME, token, spos, epos, line)
523:                    if token == 'async' and not stashed:
524:                        stashed = tok
525:                        continue
526:
527:                    if token in ('def', 'for'):
528:                        if (stashed
529:                                and stashed[0] == NAME
530:                                and stashed[1] == 'async'):
531:
532:                            if token == 'def':
533:                                async_def = True
534:                                async_def_indent = indents[-1]
535:
536:                            yield (ASYNC, stashed[1],
537:                                   stashed[2], stashed[3],
538:                                   stashed[4])
539:                            stashed = None

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
540:
541:                    if stashed:
542:                        yield stashed
543:                        stashed = None
544:
545:                    yield tok
546:                elif initial == '\\':                      # continued stmt
547:                    # This yield is new; needed for better idempotency:
548:                    if stashed:
549:                        yield stashed
550:                        stashed = None
551:                    yield (NL, token, spos, (lnum, pos), line)
552:                    continued = 1
553:                else:
554:                    if initial in '([{': parenlev = parenlev + 1
555:                    elif initial in ')]}': parenlev = parenlev - 1
556:                    if stashed:
557:                        yield stashed
558:                        stashed = None
559:                    yield (OP, token, spos, epos, line)
560:            else:
561:                yield (ERRORTOKEN, line[pos],
562:                           (lnum, pos), (lnum, pos+1), line)
563:                pos = pos + 1
564:
565:    if stashed:
566:        yield stashed
567:        stashed = None
568:
569:    for indent in indents[1:]:                 # pop remaining indent levels
570:        yield (DEDENT, '', (lnum, 0), (lnum, 0), '')
571:    yield (ENDMARKER, '', (lnum, 0), (lnum, 0), '')
572:
573:if __name__ == '__main__':                     # testing
574:    import sys
575:    if len(sys.argv) > 1: tokenize(open(sys.argv[1]).readline)
576:    else: tokenize(sys.stdin.readline)

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
40:from attr import dataclass, evolve, Factory
41:import click
42:import toml
43:from typed_ast import ast3, ast27
44:
45:# lib2to3 fork
46:from blib2to3.pytree import Node, Leaf, type_repr
47:from blib2to3 import pygram, pytree
48:from blib2to3.pgen2 import driver, token
49:from blib2to3.pgen2.grammar import Grammar
50:from blib2to3.pgen2.parse import ParseError
51:from blib2to3.pgen2.tokenize import TokenizerConfig
52:
53:
54:__version__ = "19.3b0"
55:DEFAULT_LINE_LENGTH = 88
56:DEFAULT_EXCLUDES = (
57:    r"/(\.eggs|\.git|\.hg|\.mypy_cache|\.nox|\.tox|\.venv|_build|buck-out|build|dist)/"
58:)
59:DEFAULT_INCLUDES = r"\.pyi?$"
60:CACHE_DIR = Path(user_cache_dir("black", version=__version__))
61:
62:
63:# types
64:FileContent = str
65:Encoding = str
66:NewLine = str
67:Depth = int
68:NodeType = int
69:LeafID = int
70:Priority = int
71:Index = int
72:LN = Union[Leaf, Node]
73:SplitFunc = Callable[["Line", Collection["Feature"]], Iterator["Line"]]
74:Timestamp = float
75:FileSize = int
76:CacheInfo = Tuple[Timestamp, FileSize]
77:Cache = Dict[Path, CacheInfo]
78:out = partial(click.secho, bold=True, err=True)
79:err = partial(click.secho, fg="red", err=True)

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: The Feature enum includes ASYNC_IS_VALID_IDENTIFIER and ASYNC_IS_RESERVED_KEYWORD, mapped per version in VERSION_TO_FEATURES. They are only referenced in get_parser_configs to choose TokenizerConfig(async_is_reserved_keyword True/False) for tokenization, and not used elsewhere. TokenizerConfig is passed to the tokenizer, affecting only async/await tokenization.


Todo item 1 of 19 is correct
description: 'Verify Feature enum additions: `ASYNC_IS_VALID_IDENTIFIER` and `ASYNC_IS_RESERVED_KEYWORD` exist and are only used to control async/await tokenization.'
Validating todo item 2 of 19
description: 'Check VERSION_TO_FEATURES mapping ensures mutual exclusivity: for each TargetVersion (PY27, PY33, PY34, PY35, PY36) include ASYNC_IS_VALID_IDENTIFIER, and for (PY37, PY38) include ASYNC_IS_RESERVED_KEYWORD; confirm exactly one of the two flags is present per version.'
Calling tool: globTool
**/black.py
Matched files:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py

Calling tool: grepTool
VERSION_TO_FEATURES
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:146:VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:207:    return all(feature in VERSION_TO_FEATURES[version] for version in target_versions)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:3187:        version for version in TargetVersion if features <= VERSION_TO_FEATURES[version]

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
120:    PY34 = 4
121:    PY35 = 5
122:    PY36 = 6
123:    PY37 = 7
124:    PY38 = 8
125:
126:    def is_python2(self) -> bool:
127:        return self is TargetVersion.PY27
128:
129:
130:PY36_VERSIONS = {TargetVersion.PY36, TargetVersion.PY37, TargetVersion.PY38}
131:
132:
133:class Feature(Enum):
134:    # All string literals are unicode
135:    UNICODE_LITERALS = 1
136:    F_STRINGS = 2
137:    NUMERIC_UNDERSCORES = 3
138:    TRAILING_COMMA_IN_CALL = 4
139:    TRAILING_COMMA_IN_DEF = 5
140:    # The following two feature-flags are mutually exclusive, and exactly one should be
141:    # set for every version of python.
142:    ASYNC_IS_VALID_IDENTIFIER = 6
143:    ASYNC_IS_RESERVED_KEYWORD = 7
144:
145:
146:VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {
147:    TargetVersion.PY27: {Feature.ASYNC_IS_VALID_IDENTIFIER},
148:    TargetVersion.PY33: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
149:    TargetVersion.PY34: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
150:    TargetVersion.PY35: {
151:        Feature.UNICODE_LITERALS,
152:        Feature.TRAILING_COMMA_IN_CALL,
153:        Feature.ASYNC_IS_VALID_IDENTIFIER,
154:    },
155:    TargetVersion.PY36: {
156:        Feature.UNICODE_LITERALS,
157:        Feature.F_STRINGS,
158:        Feature.NUMERIC_UNDERSCORES,
159:        Feature.TRAILING_COMMA_IN_CALL,
160:        Feature.TRAILING_COMMA_IN_DEF,
161:        Feature.ASYNC_IS_VALID_IDENTIFIER,
162:    },
163:    TargetVersion.PY37: {
164:        Feature.UNICODE_LITERALS,
165:        Feature.F_STRINGS,
166:        Feature.NUMERIC_UNDERSCORES,
167:        Feature.TRAILING_COMMA_IN_CALL,
168:        Feature.TRAILING_COMMA_IN_DEF,
169:        Feature.ASYNC_IS_RESERVED_KEYWORD,
170:    },
171:    TargetVersion.PY38: {
172:        Feature.UNICODE_LITERALS,
173:        Feature.F_STRINGS,
174:        Feature.NUMERIC_UNDERSCORES,
175:        Feature.TRAILING_COMMA_IN_CALL,
176:        Feature.TRAILING_COMMA_IN_DEF,
177:        Feature.ASYNC_IS_RESERVED_KEYWORD,
178:    },
179:}
180:
181:
182:@dataclass
183:class FileMode:
184:    target_versions: Set[TargetVersion] = Factory(set)
185:    line_length: int = DEFAULT_LINE_LENGTH
186:    string_normalization: bool = True
187:    is_pyi: bool = False
188:
189:    def get_cache_key(self) -> str:
190:        if self.target_versions:
191:            version_str = ",".join(
192:                str(version.value)
193:                for version in sorted(self.target_versions, key=lambda v: v.value)
194:            )
195:        else:
196:            version_str = "-"
197:        parts = [
198:            version_str,
199:            str(self.line_length),
200:            str(int(self.string_normalization)),
201:            str(int(self.is_pyi)),
202:        ]
203:        return ".".join(parts)
204:
205:
206:def supports_feature(target_versions: Set[TargetVersion], feature: Feature) -> bool:
207:    return all(feature in VERSION_TO_FEATURES[version] for version in target_versions)
208:
209:
210:def read_pyproject_toml(
211:    ctx: click.Context, param: click.Parameter, value: Union[str, int, bool, None]
212:) -> Optional[str]:
213:    """Inject Black configuration from "pyproject.toml" into defaults in `ctx`.
214:
215:    Returns the path to a successfully found and read configuration file, None
216:    otherwise.
217:    """
218:    assert not isinstance(value, (int, bool)), "Invalid parameter type passed"
219:    if not value:
220:        root = find_project_root(ctx.params.get("src", ()))
221:        path = root / "pyproject.toml"
222:        if path.is_file():
223:            value = str(path)
224:        else:
225:            return None
226:
227:    try:
228:        pyproject_toml = toml.load(value)
229:        config = pyproject_toml.get("tool", {}).get("black", {})
230:    except (toml.TomlDecodeError, OSError) as e:
231:        raise click.FileError(
232:            filename=value, hint=f"Error reading configuration file: {e}"
233:        )
234:
235:    if not config:
236:        return None
237:
238:    if ctx.default_map is None:
239:        ctx.default_map = {}

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: Verified in black.py lines 146-179 that VERSION_TO_FEATURES assigns ASYNC_IS_VALID_IDENTIFIER for PY27, PY33, PY34, PY35, PY36 and ASYNC_IS_RESERVED_KEYWORD for PY37, PY38. Each version includes exactly one of the two flagsnone include both or neither.


Todo item 2 of 19 is correct
description: 'Check VERSION_TO_FEATURES mapping ensures mutual exclusivity: for each TargetVersion (PY27, PY33, PY34, PY35, PY36) include ASYNC_IS_VALID_IDENTIFIER, and for (PY37, PY38) include ASYNC_IS_RESERVED_KEYWORD; confirm exactly one of the two flags is present per version.'
Validating todo item 3 of 19
description: 'Confirm `ParserConfig` dataclass exists in `black.py` with fields `grammar: Grammar` and `tokenizer_config: TokenizerConfig = TokenizerConfig()` and that `TokenizerConfig` is imported from `blib2to3.pgen2.tokenize`.'
Calling tool: grepTool
ParserConfig
Grep command failed with exit code 2: rg: /Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/{{FOLDER_PATH}}: IO error for operation on /Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/{{FOLDER_PATH}}: No such file or directory (os error 2)


Calling tool: grepTool
ParserConfig
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:764:class ParserConfig:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:774:            ParserConfig(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:779:            ParserConfig(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:784:            ParserConfig(pygram.python_grammar_no_print_statement),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:786:            ParserConfig(pygram.python_grammar),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:792:            ParserConfig(pygram.python_grammar_no_print_statement),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:794:            ParserConfig(pygram.python_grammar),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:803:                ParserConfig(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:811:                ParserConfig(

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
740:            current_line, line_length=mode.line_length, features=split_line_features
741:        ):
742:            dst_contents += str(line)
743:    return dst_contents
744:
745:
746:def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:
747:    """Return a tuple of (decoded_contents, encoding, newline).
748:
749:    `newline` is either CRLF or LF but `decoded_contents` is decoded with
750:    universal newlines (i.e. only contains LF).
751:    """
752:    srcbuf = io.BytesIO(src)
753:    encoding, lines = tokenize.detect_encoding(srcbuf.readline)
754:    if not lines:
755:        return "", encoding, "\n"
756:
757:    newline = "\r\n" if b"\r\n" == lines[0][-2:] else "\n"
758:    srcbuf.seek(0)
759:    with io.TextIOWrapper(srcbuf, encoding) as tiow:
760:        return tiow.read(), encoding, newline
761:
762:
763:@dataclass(frozen=True)
764:class ParserConfig:
765:    grammar: Grammar
766:    tokenizer_config: TokenizerConfig = TokenizerConfig()
767:
768:
769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:
770:    if not target_versions:
771:        # No target_version specified, so try all grammars.
772:        return [
773:            # Python 3.7+
774:            ParserConfig(
775:                pygram.python_grammar_no_print_statement_no_exec_statement,
776:                TokenizerConfig(async_is_reserved_keyword=True),
777:            ),
778:            # Python 3.0-3.6
779:            ParserConfig(
780:                pygram.python_grammar_no_print_statement_no_exec_statement,
781:                TokenizerConfig(async_is_reserved_keyword=False),
782:            ),
783:            # Python 2.7 with future print_function import
784:            ParserConfig(pygram.python_grammar_no_print_statement),
785:            # Python 2.7
786:            ParserConfig(pygram.python_grammar),
787:        ]
788:    elif all(version.is_python2() for version in target_versions):
789:        # Python 2-only code, so try Python 2 grammars.
790:        return [
791:            # Python 2.7 with future print_function import
792:            ParserConfig(pygram.python_grammar_no_print_statement),
793:            # Python 2.7
794:            ParserConfig(pygram.python_grammar),
795:        ]
796:    else:
797:        # Python 3-compatible code, so only try Python 3 grammar.
798:        configs = []
799:        # If we have to parse both, try to parse async as a keyword first
800:        if not supports_feature(target_versions, Feature.ASYNC_IS_VALID_IDENTIFIER):
801:            # Python 3.7+
802:            configs.append(
803:                ParserConfig(
804:                    pygram.python_grammar_no_print_statement_no_exec_statement,
805:                    TokenizerConfig(async_is_reserved_keyword=True),
806:                )
807:            )
808:        if not supports_feature(target_versions, Feature.ASYNC_IS_RESERVED_KEYWORD):
809:            # Python 3.0-3.6
810:            configs.append(
811:                ParserConfig(
812:                    pygram.python_grammar_no_print_statement_no_exec_statement,
813:                    TokenizerConfig(async_is_reserved_keyword=False),
814:                )
815:            )
816:        # At least one of the above branches must have been taken, because every Python
817:        # version has exactly one of the two 'ASYNC_IS_*' flags
818:        return configs
819:
820:
821:def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:
822:    """Given a string with source, return the lib2to3 Node."""
823:    if src_txt[-1:] != "\n":
824:        src_txt += "\n"
825:
826:    for parser_config in get_parser_configs(set(target_versions)):
827:        drv = driver.Driver(
828:            parser_config.grammar,
829:            pytree.convert,
830:            tokenizer_config=parser_config.tokenizer_config,
831:        )
832:        try:
833:            result = drv.parse_string(src_txt, True)
834:            break
835:
836:        except ParseError as pe:
837:            lineno, column = pe.context[1]
838:            lines = src_txt.splitlines()
839:            try:
840:                faulty_line = lines[lineno - 1]
841:            except IndexError:
842:                faulty_line = "<line number missing in source>"
843:            exc = InvalidInput(f"Cannot parse: {lineno}:{column}: {faulty_line}")
844:    else:
845:        raise exc from None
846:
847:    if isinstance(result, Leaf):
848:        result = Node(syms.file_input, [result])
849:    return result
850:
851:
852:def lib2to3_unparse(node: Node) -> str:
853:    """Given a lib2to3 node, return its string representation."""
854:    code = str(node)
855:    return code
856:
857:
858:T = TypeVar("T")
859:

Calling tool: grepTool
from blib2to3\.pgen2\.tokenize import TokenizerConfig
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.pyi:11:from blib2to3.pgen2.tokenize import TokenizerConfig
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:51:from blib2to3.pgen2.tokenize import TokenizerConfig

Calling tool: grepTool
from blib2to3\.pgen2\.grammar import Grammar
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:49:from blib2to3.pgen2.grammar import Grammar
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pygram.pyi:5:from blib2to3.pgen2.grammar import Grammar
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pytree.pyi:6:from blib2to3.pgen2.grammar import Grammar
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/parse.pyi:5:from blib2to3.pgen2.grammar import Grammar, _DFAS
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.pyi:10:from blib2to3.pgen2.grammar import Grammar

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
40:from attr import dataclass, evolve, Factory
41:import click
42:import toml
43:from typed_ast import ast3, ast27
44:
45:# lib2to3 fork
46:from blib2to3.pytree import Node, Leaf, type_repr
47:from blib2to3 import pygram, pytree
48:from blib2to3.pgen2 import driver, token
49:from blib2to3.pgen2.grammar import Grammar
50:from blib2to3.pgen2.parse import ParseError
51:from blib2to3.pgen2.tokenize import TokenizerConfig
52:
53:
54:__version__ = "19.3b0"
55:DEFAULT_LINE_LENGTH = 88
56:DEFAULT_EXCLUDES = (
57:    r"/(\.eggs|\.git|\.hg|\.mypy_cache|\.nox|\.tox|\.venv|_build|buck-out|build|dist)/"
58:)
59:DEFAULT_INCLUDES = r"\.pyi?$"
60:CACHE_DIR = Path(user_cache_dir("black", version=__version__))
61:
62:
63:# types
64:FileContent = str
65:Encoding = str
66:NewLine = str
67:Depth = int
68:NodeType = int
69:LeafID = int

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: `black.py` defines `@dataclass(frozen=True) class ParserConfig` with fields `grammar: Grammar` and `tokenizer_config: TokenizerConfig = TokenizerConfig()` (lines 763-766). Additionally, `TokenizerConfig` is imported from `blib2to3.pgen2.tokenize` (line 51).


Todo item 3 of 19 is correct
description: 'Confirm `ParserConfig` dataclass exists in `black.py` with fields `grammar: Grammar` and `tokenizer_config: TokenizerConfig = TokenizerConfig()` and that `TokenizerConfig` is imported from `blib2to3.pgen2.tokenize`.'
Validating todo item 4 of 19
description: 'Validate `get_parser_configs()` replaces `get_grammars()` and returns configurations as specified: when no target_versions, order is [Py3.7+ reserved, Py3.0-3.6 non-reserved, Py2.7 no print, Py2.7]; when all Python2 targets, only the two Py2 grammars; when any Python3 targets, only Py3 grammar with: reserved config added if ASYNC_IS_VALID_IDENTIFIER not supported, non-reserved config added if ASYNC_IS_RESERVED_KEYWORD not supported, in that order.'
Calling tool: grepTool
def get_parser_configs\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
740:            current_line, line_length=mode.line_length, features=split_line_features
741:        ):
742:            dst_contents += str(line)
743:    return dst_contents
744:
745:
746:def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:
747:    """Return a tuple of (decoded_contents, encoding, newline).
748:
749:    `newline` is either CRLF or LF but `decoded_contents` is decoded with
750:    universal newlines (i.e. only contains LF).
751:    """
752:    srcbuf = io.BytesIO(src)
753:    encoding, lines = tokenize.detect_encoding(srcbuf.readline)
754:    if not lines:
755:        return "", encoding, "\n"
756:
757:    newline = "\r\n" if b"\r\n" == lines[0][-2:] else "\n"
758:    srcbuf.seek(0)
759:    with io.TextIOWrapper(srcbuf, encoding) as tiow:
760:        return tiow.read(), encoding, newline
761:
762:
763:@dataclass(frozen=True)
764:class ParserConfig:
765:    grammar: Grammar
766:    tokenizer_config: TokenizerConfig = TokenizerConfig()
767:
768:
769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:
770:    if not target_versions:
771:        # No target_version specified, so try all grammars.
772:        return [
773:            # Python 3.7+
774:            ParserConfig(
775:                pygram.python_grammar_no_print_statement_no_exec_statement,
776:                TokenizerConfig(async_is_reserved_keyword=True),
777:            ),
778:            # Python 3.0-3.6
779:            ParserConfig(
780:                pygram.python_grammar_no_print_statement_no_exec_statement,
781:                TokenizerConfig(async_is_reserved_keyword=False),
782:            ),
783:            # Python 2.7 with future print_function import
784:            ParserConfig(pygram.python_grammar_no_print_statement),
785:            # Python 2.7
786:            ParserConfig(pygram.python_grammar),
787:        ]
788:    elif all(version.is_python2() for version in target_versions):
789:        # Python 2-only code, so try Python 2 grammars.
790:        return [
791:            # Python 2.7 with future print_function import
792:            ParserConfig(pygram.python_grammar_no_print_statement),
793:            # Python 2.7
794:            ParserConfig(pygram.python_grammar),
795:        ]
796:    else:
797:        # Python 3-compatible code, so only try Python 3 grammar.
798:        configs = []
799:        # If we have to parse both, try to parse async as a keyword first
800:        if not supports_feature(target_versions, Feature.ASYNC_IS_VALID_IDENTIFIER):
801:            # Python 3.7+
802:            configs.append(
803:                ParserConfig(
804:                    pygram.python_grammar_no_print_statement_no_exec_statement,
805:                    TokenizerConfig(async_is_reserved_keyword=True),
806:                )
807:            )
808:        if not supports_feature(target_versions, Feature.ASYNC_IS_RESERVED_KEYWORD):
809:            # Python 3.0-3.6
810:            configs.append(
811:                ParserConfig(
812:                    pygram.python_grammar_no_print_statement_no_exec_statement,
813:                    TokenizerConfig(async_is_reserved_keyword=False),
814:                )
815:            )
816:        # At least one of the above branches must have been taken, because every Python
817:        # version has exactly one of the two 'ASYNC_IS_*' flags
818:        return configs
819:
820:
821:def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:
822:    """Given a string with source, return the lib2to3 Node."""
823:    if src_txt[-1:] != "\n":
824:        src_txt += "\n"
825:
826:    for parser_config in get_parser_configs(set(target_versions)):
827:        drv = driver.Driver(
828:            parser_config.grammar,
829:            pytree.convert,
830:            tokenizer_config=parser_config.tokenizer_config,
831:        )
832:        try:
833:            result = drv.parse_string(src_txt, True)
834:            break
835:
836:        except ParseError as pe:
837:            lineno, column = pe.context[1]
838:            lines = src_txt.splitlines()
839:            try:
840:                faulty_line = lines[lineno - 1]
841:            except IndexError:
842:                faulty_line = "<line number missing in source>"
843:            exc = InvalidInput(f"Cannot parse: {lineno}:{column}: {faulty_line}")
844:    else:
845:        raise exc from None
846:
847:    if isinstance(result, Leaf):
848:        result = Node(syms.file_input, [result])
849:    return result
850:
851:
852:def lib2to3_unparse(node: Node) -> str:
853:    """Given a lib2to3 node, return its string representation."""
854:    code = str(node)
855:    return code
856:
857:
858:T = TypeVar("T")
859:
860:
861:class Visitor(Generic[T]):
862:    """Basic lib2to3 visitor that yields things of type `T` on `visit()`."""
863:
864:    def visit(self, node: LN) -> Iterator[T]:
865:        """Main method to visit `node` and its children.
866:
867:        It tries to find a `visit_*()` method for the given `node.type`, like
868:        `visit_simple_stmt` for Node objects or `visit_INDENT` for Leaf objects.
869:        If no dedicated `visit_*()` method is found, chooses `visit_default()`
870:        instead.
871:
872:        Then yields objects of type `T` from the selected visitor.
873:        """
874:        if node.type < 256:
875:            name = token.tok_name[node.type]
876:        else:
877:            name = type_repr(node.type)
878:        yield from getattr(self, f"visit_{name}", self.visit_default)(node)
879:

Calling tool: grepTool
from blib2to3\.pgen2\.tokenize import TokenizerConfig
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.pyi:11:from blib2to3.pgen2.tokenize import TokenizerConfig
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:51:from blib2to3.pgen2.tokenize import TokenizerConfig

Calling tool: grepTool
class Driver
Matched content:
30:class Driver(object):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Modifications:
5:# Copyright 2006 Google, Inc. All Rights Reserved.
6:# Licensed to PSF under a Contributor Agreement.
7:
8:"""Parser driver.
9:
10:This provides a high-level interface to parse a file into a syntax tree.
11:
12:"""
13:
14:__author__ = "Guido van Rossum <guido@python.org>"
15:
16:__all__ = ["Driver", "load_grammar"]
17:
18:# Python imports
19:import codecs
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(
33:        self,
34:        grammar,
35:        convert=None,
36:        logger=None,
37:        tokenizer_config=tokenize.TokenizerConfig(),
38:    ):
39:        self.grammar = grammar
40:        if logger is None:
41:            logger = logging.getLogger(__name__)
42:        self.logger = logger
43:        self.convert = convert
44:        self.tokenizer_config = tokenizer_config
45:
46:    def parse_tokens(self, tokens, debug=False):
47:        """Parse a series of tokens and return the syntax tree."""
48:        # XXX Move the prefix computation into a wrapper around tokenize.
49:        p = parse.Parser(self.grammar, self.convert)
50:        p.setup()
51:        lineno = 1
52:        column = 0
53:        indent_columns = []
54:        type = value = start = end = line_text = None
55:        prefix = ""
56:        for quintuple in tokens:
57:            type, value, start, end, line_text = quintuple
58:            if start != (lineno, column):
59:                assert (lineno, column) <= start, ((lineno, column), start)
60:                s_lineno, s_column = start
61:                if lineno < s_lineno:
62:                    prefix += "\n" * (s_lineno - lineno)
63:                    lineno = s_lineno
64:                    column = 0
65:                if column < s_column:
66:                    prefix += line_text[column:s_column]
67:                    column = s_column
68:            if type in (tokenize.COMMENT, tokenize.NL):
69:                prefix += value
70:                lineno, column = end
71:                if value.endswith("\n"):
72:                    lineno += 1
73:                    column = 0
74:                continue
75:            if type == token.OP:
76:                type = grammar.opmap[value]
77:            if debug:
78:                self.logger.debug("%s %r (prefix=%r)",
79:                                  token.tok_name[type], value, prefix)
80:            if type == token.INDENT:
81:                indent_columns.append(len(value))
82:                _prefix = prefix + value
83:                prefix = ""
84:                value = ""
85:            elif type == token.DEDENT:
86:                _indent_col = indent_columns.pop()
87:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
88:            if p.addtoken(type, value, (prefix, start)):
89:                if debug:
90:                    self.logger.debug("Stop.")
91:                break
92:            prefix = ""
93:            if type in {token.INDENT, token.DEDENT}:
94:                prefix = _prefix
95:            lineno, column = end
96:            if value.endswith("\n"):
97:                lineno += 1
98:                column = 0
99:        else:
100:            # We never broke out -- EOF is too soon (how can this happen???)
101:            raise parse.ParseError("incomplete input",
102:                                   type, value, (prefix, start))
103:        return p.rootnode
104:
105:    def parse_stream_raw(self, stream, debug=False):
106:        """Parse a stream and return the syntax tree."""
107:        tokens = tokenize.generate_tokens(stream.readline, config=self.tokenizer_config)
108:        return self.parse_tokens(tokens, debug)
109:
110:    def parse_stream(self, stream, debug=False):
111:        """Parse a stream and return the syntax tree."""
112:        return self.parse_stream_raw(stream, debug)
113:
114:    def parse_file(self, filename, encoding=None, debug=False):
115:        """Parse a file and return the syntax tree."""
116:        with io.open(filename, "r", encoding=encoding) as stream:
117:            return self.parse_stream(stream, debug)
118:
119:    def parse_string(self, text, debug=False):
120:        """Parse a string and return the syntax tree."""
121:        tokens = tokenize.generate_tokens(
122:            io.StringIO(text).readline,
123:            config=self.tokenizer_config,
124:        )
125:        return self.parse_tokens(tokens, debug)
126:
127:    def _partially_consume_prefix(self, prefix, column):
128:        lines = []
129:        current_line = ""
130:        current_column = 0
131:        wait_for_nl = False
132:        for char in prefix:
133:            current_line += char
134:            if wait_for_nl:
135:                if char == '\n':
136:                    if current_line.strip() and current_column < column:
137:                        res = ''.join(lines)
138:                        return res, prefix[len(res):]
139:
140:                    lines.append(current_line)
141:                    current_line = ""
142:                    current_column = 0
143:                    wait_for_nl = False
144:            elif char in ' \t':
145:                current_column += 1
146:            elif char == '\n':
147:                # unexpected empty line
148:                current_column = 0
149:            else:
150:                # indent is finished
151:                wait_for_nl = True
152:        return ''.join(lines), current_line
153:
154:
155:def _generate_pickle_name(gt, cache_dir=None):
156:    head, tail = os.path.splitext(gt)
157:    if tail == ".txt":
158:        tail = ""
159:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
160:    if cache_dir:

Calling tool: grepTool
class TokenizerConfig
Matched content:
142:class TokenizerConfig:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
120:)
121:
122:endprogs = {"'": re.compile(Single), '"': re.compile(Double),
123:            "'''": single3prog, '"""': double3prog,
124:            **{f"{prefix}'''": single3prog for prefix in _strprefixes},
125:            **{f'{prefix}"""': double3prog for prefix in _strprefixes},
126:            **{prefix: None for prefix in _strprefixes}}
127:
128:triple_quoted = (
129:    {"'''", '"""'} |
130:    {f"{prefix}'''" for prefix in _strprefixes} |
131:    {f'{prefix}"""' for prefix in _strprefixes}
132:)
133:single_quoted = (
134:    {"'", '"'} |
135:    {f"{prefix}'" for prefix in _strprefixes} |
136:    {f'{prefix}"' for prefix in _strprefixes}
137:)
138:
139:tabsize = 8
140:
141:@dataclass(frozen=True)
142:class TokenizerConfig:
143:    async_is_reserved_keyword: bool = False
144:
145:class TokenError(Exception): pass
146:
147:class StopTokenizing(Exception): pass
148:
149:def printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line): # for testing
150:    (srow, scol) = xxx_todo_changeme
151:    (erow, ecol) = xxx_todo_changeme1
152:    print("%d,%d-%d,%d:\t%s\t%s" % \
153:        (srow, scol, erow, ecol, tok_name[type], repr(token)))
154:
155:def tokenize(readline, tokeneater=printtoken):
156:    """
157:    The tokenize() function accepts two parameters: one representing the
158:    input stream, and one providing an output mechanism for tokenize().
159:
160:    The first parameter, readline, must be a callable object which provides
161:    the same interface as the readline() method of built-in file objects.
162:    Each call to the function should return one line of input as a string.
163:
164:    The second parameter, tokeneater, must also be a callable object. It is
165:    called once for each token, with five arguments, corresponding to the
166:    tuples generated by generate_tokens().
167:    """
168:    try:
169:        tokenize_loop(readline, tokeneater)
170:    except StopTokenizing:
171:        pass
172:
173:# backwards compatible interface
174:def tokenize_loop(readline, tokeneater):
175:    for token_info in generate_tokens(readline):
176:        tokeneater(*token_info)
177:
178:class Untokenizer:
179:
180:    def __init__(self):
181:        self.tokens = []
182:        self.prev_row = 1
183:        self.prev_col = 0
184:
185:    def add_whitespace(self, start):
186:        row, col = start
187:        assert row <= self.prev_row
188:        col_offset = col - self.prev_col
189:        if col_offset:
190:            self.tokens.append(" " * col_offset)
191:
192:    def untokenize(self, iterable):
193:        for t in iterable:
194:            if len(t) == 2:
195:                self.compat(t, iterable)
196:                break
197:            tok_type, token, start, end, line = t
198:            self.add_whitespace(start)
199:            self.tokens.append(token)

Calling tool: grepTool
def generate_tokens\(
Matched content:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
334:        newcode = untokenize(t1)
335:        readline = iter(newcode.splitlines(1)).next
336:        t2 = [tok[:2] for tokin generate_tokens(readline)]
337:        assert t1 == t2
338:    """
339:    ut = Untokenizer()
340:    return ut.untokenize(iterable)
341:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
343:    """
344:    The generate_tokens() generator requires one argument, readline, which
345:    must be a callable object which provides the same interface as the
346:    readline() method of built-in file objects. Each call to the function
347:    should return one line of input as a string.  Alternately, readline
348:    can be a callable function terminating with StopIteration:
349:        readline = open(myfile).next    # Example of alternate readline
350:
351:    The generator produces 5-tuples with these members: the token type; the
352:    token string; a 2-tuple (srow, scol) of ints specifying the row and
353:    column where the token begins in the source; a 2-tuple (erow, ecol) of
354:    ints specifying the row and column where the token ends in the source;
355:    and the line on which the token was found. The line passed is the
356:    logical line; continuation lines are included.
357:    """
358:    lnum = parenlev = continued = 0
359:    numchars = '0123456789'
360:    contstr, needcont = '', 0
361:    contline = None
362:    indents = [0]
363:
364:    # If we know we're parsing 3.7+, we can unconditionally parse `async` and
365:    # `await` as keywords.
366:    async_is_reserved_keyword = config.async_is_reserved_keyword
367:    # 'stashed' and 'async_*' are used for async/await parsing
368:    stashed = None
369:    async_def = False
370:    async_def_indent = 0
371:    async_def_nl = False
372:
373:    while 1:                                   # loop over lines in stream
374:        try:
375:            line = readline()
376:        except StopIteration:
377:            line = ''
378:        lnum = lnum + 1
379:        pos, max = 0, len(line)
380:
381:        if contstr:                            # continued string
382:            if not line:
383:                raise TokenError("EOF in multi-line string", strstart)
384:            endmatch = endprog.match(line)
385:            if endmatch:
386:                pos = end = endmatch.end(0)
387:                yield (STRING, contstr + line[:end],
388:                       strstart, (lnum, end), contline + line)
389:                contstr, needcont = '', 0
390:                contline = None
391:            elif needcont and line[-2:] != '\\\n' and line[-3:] != '\\\r\n':
392:                yield (ERRORTOKEN, contstr + line,
393:                           strstart, (lnum, len(line)), contline)
394:                contstr = ''
395:                contline = None
396:                continue
397:            else:
398:                contstr = contstr + line
399:                contline = contline + line
400:                continue
401:
402:        elif parenlev == 0 and not continued:  # new statement
403:            if not line: break
404:            column = 0
405:            while pos < max:                   # measure leading whitespace
406:                if line[pos] == ' ': column = column + 1
407:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize
408:                elif line[pos] == '\f': column = 0
409:                else: break
410:                pos = pos + 1
411:            if pos == max: break
412:
413:            if stashed:
414:                yield stashed
415:                stashed = None
416:
417:            if line[pos] in '\r\n':            # skip blank lines
418:                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)
419:                continue
420:
421:            if line[pos] == '#':               # skip comments
422:                comment_token = line[pos:].rstrip('\r\n')
423:                nl_pos = pos + len(comment_token)
424:                yield (COMMENT, comment_token,
425:                        (lnum, pos), (lnum, pos + len(comment_token)), line)
426:                yield (NL, line[nl_pos:],
427:                        (lnum, nl_pos), (lnum, len(line)), line)
428:                continue
429:
430:            if column > indents[-1]:           # count indents
431:                indents.append(column)
432:                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)
433:
434:            while column < indents[-1]:        # count dedents
435:                if column not in indents:
436:                    raise IndentationError(
437:                        "unindent does not match any outer indentation level",
438:                        ("<tokenize>", lnum, pos, line))
439:                indents = indents[:-1]
440:
441:                if async_def and async_def_indent >= indents[-1]:
442:                    async_def = False
443:                    async_def_nl = False
444:                    async_def_indent = 0
445:
446:                yield (DEDENT, '', (lnum, pos), (lnum, pos), line)
447:
448:            if async_def and async_def_nl and async_def_indent >= indents[-1]:
449:                async_def = False
450:                async_def_nl = False
451:                async_def_indent = 0
452:
453:        else:                                  # continued statement
454:            if not line:
455:                raise TokenError("EOF in multi-line statement", (lnum, 0))
456:            continued = 0
457:
458:        while pos < max:
459:            pseudomatch = pseudoprog.match(line, pos)
460:            if pseudomatch:                                # scan for tokens
461:                start, end = pseudomatch.span(1)
462:                spos, epos, pos = (lnum, start), (lnum, end), end
463:                token, initial = line[start:end], line[start]
464:
465:                if initial in numchars or \
466:                   (initial == '.' and token != '.'):      # ordinary number
467:                    yield (NUMBER, token, spos, epos, line)
468:                elif initial in '\r\n':
469:                    newline = NEWLINE
470:                    if parenlev > 0:
471:                        newline = NL
472:                    elif async_def:
473:                        async_def_nl = True
474:                    if stashed:
475:                        yield stashed
476:                        stashed = None
477:                    yield (newline, token, spos, epos, line)
478:
479:                elif initial == '#':
480:                    assert not token.endswith("\n")
481:                    if stashed:
482:                        yield stashed
483:                        stashed = None
484:                    yield (COMMENT, token, spos, epos, line)
485:                elif token in triple_quoted:
486:                    endprog = endprogs[token]
487:                    endmatch = endprog.match(line, pos)
488:                    if endmatch:                           # all on one line
489:                        pos = endmatch.end(0)
490:                        token = line[start:pos]
491:                        if stashed:
492:                            yield stashed
493:                            stashed = None
494:                        yield (STRING, token, spos, (lnum, pos), line)
495:                    else:
496:                        strstart = (lnum, start)           # multiple lines
497:                        contstr = line[start:]
498:                        contline = line
499:                        break
500:                elif initial in single_quoted or \
501:                    token[:2] in single_quoted or \
502:                    token[:3] in single_quoted:
503:                    if token[-1] == '\n':                  # continued string
504:                        strstart = (lnum, start)
505:                        endprog = (endprogs[initial] or endprogs[token[1]] or
506:                                   endprogs[token[2]])
507:                        contstr, needcont = line[start:], 1
508:                        contline = line
509:                        break
510:                    else:                                  # ordinary string
511:                        if stashed:
512:                            yield stashed
513:                            stashed = None
514:                        yield (STRING, token, spos, epos, line)
515:                elif initial.isidentifier():               # ordinary name
516:                    if token in ('async', 'await'):
517:                        if async_is_reserved_keyword or async_def:
518:                            yield (ASYNC if token == 'async' else AWAIT,
519:                                   token, spos, epos, line)
520:                            continue
521:
522:                    tok = (NAME, token, spos, epos, line)
523:                    if token == 'async' and not stashed:
524:                        stashed = tok
525:                        continue
526:
527:                    if token in ('def', 'for'):
528:                        if (stashed
529:                                and stashed[0] == NAME
530:                                and stashed[1] == 'async'):
531:
532:                            if token == 'def':
533:                                async_def = True
534:                                async_def_indent = indents[-1]
535:
536:                            yield (ASYNC, stashed[1],
537:                                   stashed[2], stashed[3],
538:                                   stashed[4])
539:                            stashed = None
540:
541:                    if stashed:
542:                        yield stashed
543:                        stashed = None
544:
545:                    yield tok
546:                elif initial == '\\':                      # continued stmt
547:                    # This yield is new; needed for better idempotency:
548:                    if stashed:
549:                        yield stashed
550:                        stashed = None
551:                    yield (NL, token, spos, (lnum, pos), line)
552:                    continued = 1
553:                else:

Calling tool: grepTool
get_grammars\(
No files found. Call grepTool again with a more generic pattern.

Calling tool: grepTool
def supports_feature\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:206:def supports_feature(target_versions: Set[TargetVersion], feature: Feature) -> bool:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
180:
181:
182:@dataclass
183:class FileMode:
184:    target_versions: Set[TargetVersion] = Factory(set)
185:    line_length: int = DEFAULT_LINE_LENGTH
186:    string_normalization: bool = True
187:    is_pyi: bool = False
188:
189:    def get_cache_key(self) -> str:
190:        if self.target_versions:
191:            version_str = ",".join(
192:                str(version.value)
193:                for version in sorted(self.target_versions, key=lambda v: v.value)
194:            )
195:        else:
196:            version_str = "-"
197:        parts = [
198:            version_str,
199:            str(self.line_length),
200:            str(int(self.string_normalization)),
201:            str(int(self.is_pyi)),
202:        ]
203:        return ".".join(parts)
204:
205:
206:def supports_feature(target_versions: Set[TargetVersion], feature: Feature) -> bool:
207:    return all(feature in VERSION_TO_FEATURES[version] for version in target_versions)
208:
209:
210:def read_pyproject_toml(
211:    ctx: click.Context, param: click.Parameter, value: Union[str, int, bool, None]
212:) -> Optional[str]:
213:    """Inject Black configuration from "pyproject.toml" into defaults in `ctx`.
214:
215:    Returns the path to a successfully found and read configuration file, None
216:    otherwise.
217:    """
218:    assert not isinstance(value, (int, bool)), "Invalid parameter type passed"
219:    if not value:
220:        root = find_project_root(ctx.params.get("src", ()))
221:        path = root / "pyproject.toml"
222:        if path.is_file():
223:            value = str(path)
224:        else:
225:            return None
226:
227:    try:
228:        pyproject_toml = toml.load(value)
229:        config = pyproject_toml.get("tool", {}).get("black", {})
230:    except (toml.TomlDecodeError, OSError) as e:
231:        raise click.FileError(
232:            filename=value, hint=f"Error reading configuration file: {e}"
233:        )
234:
235:    if not config:
236:        return None
237:
238:    if ctx.default_map is None:
239:        ctx.default_map = {}

Calling tool: grepTool
class Feature\(
Matched content:
133:class Feature(Enum):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
130:PY36_VERSIONS = {TargetVersion.PY36, TargetVersion.PY37, TargetVersion.PY38}
131:
132:
133:class Feature(Enum):
134:    # All string literals are unicode
135:    UNICODE_LITERALS = 1
136:    F_STRINGS = 2
137:    NUMERIC_UNDERSCORES = 3
138:    TRAILING_COMMA_IN_CALL = 4
139:    TRAILING_COMMA_IN_DEF = 5
140:    # The following two feature-flags are mutually exclusive, and exactly one should be
141:    # set for every version of python.
142:    ASYNC_IS_VALID_IDENTIFIER = 6
143:    ASYNC_IS_RESERVED_KEYWORD = 7
144:
145:
146:VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {
147:    TargetVersion.PY27: {Feature.ASYNC_IS_VALID_IDENTIFIER},
148:    TargetVersion.PY33: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
149:    TargetVersion.PY34: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
150:    TargetVersion.PY35: {
151:        Feature.UNICODE_LITERALS,
152:        Feature.TRAILING_COMMA_IN_CALL,
153:        Feature.ASYNC_IS_VALID_IDENTIFIER,
154:    },
155:    TargetVersion.PY36: {
156:        Feature.UNICODE_LITERALS,
157:        Feature.F_STRINGS,
158:        Feature.NUMERIC_UNDERSCORES,
159:        Feature.TRAILING_COMMA_IN_CALL,
160:        Feature.TRAILING_COMMA_IN_DEF,
161:        Feature.ASYNC_IS_VALID_IDENTIFIER,
162:    },
163:    TargetVersion.PY37: {
164:        Feature.UNICODE_LITERALS,
165:        Feature.F_STRINGS,
166:        Feature.NUMERIC_UNDERSCORES,
167:        Feature.TRAILING_COMMA_IN_CALL,
168:        Feature.TRAILING_COMMA_IN_DEF,
169:        Feature.ASYNC_IS_RESERVED_KEYWORD,
170:    },
171:    TargetVersion.PY38: {
172:        Feature.UNICODE_LITERALS,
173:        Feature.F_STRINGS,
174:        Feature.NUMERIC_UNDERSCORES,
175:        Feature.TRAILING_COMMA_IN_CALL,
176:        Feature.TRAILING_COMMA_IN_DEF,
177:        Feature.ASYNC_IS_RESERVED_KEYWORD,
178:    },
179:}
180:
181:
182:@dataclass
183:class FileMode:
184:    target_versions: Set[TargetVersion] = Factory(set)
185:    line_length: int = DEFAULT_LINE_LENGTH
186:    string_normalization: bool = True
187:    is_pyi: bool = False
188:
189:    def get_cache_key(self) -> str:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
1:import asyncio
2:from asyncio.base_events import BaseEventLoop
3:from concurrent.futures import Executor, ProcessPoolExecutor
4:from datetime import datetime
5:from enum import Enum
6:from functools import lru_cache, partial, wraps
7:import io
8:import itertools
9:import logging
10:from multiprocessing import Manager, freeze_support
11:import os
12:from pathlib import Path
13:import pickle
14:import re
15:import signal
16:import sys
17:import tempfile
18:import tokenize
19:from typing import (
20:    Any,
21:    Callable,
22:    Collection,
23:    Dict,
24:    Generator,
25:    Generic,
26:    Iterable,
27:    Iterator,
28:    List,
29:    Optional,
30:    Pattern,
31:    Sequence,
32:    Set,
33:    Tuple,
34:    TypeVar,
35:    Union,
36:    cast,
37:)
38:
39:from appdirs import user_cache_dir
40:from attr import dataclass, evolve, Factory
41:import click
42:import toml
43:from typed_ast import ast3, ast27
44:
45:# lib2to3 fork
46:from blib2to3.pytree import Node, Leaf, type_repr
47:from blib2to3 import pygram, pytree
48:from blib2to3.pgen2 import driver, token
49:from blib2to3.pgen2.grammar import Grammar
50:from blib2to3.pgen2.parse import ParseError
51:from blib2to3.pgen2.tokenize import TokenizerConfig
52:
53:
54:__version__ = "19.3b0"
55:DEFAULT_LINE_LENGTH = 88
56:DEFAULT_EXCLUDES = (
57:    r"/(\.eggs|\.git|\.hg|\.mypy_cache|\.nox|\.tox|\.venv|_build|buck-out|build|dist)/"
58:)
59:DEFAULT_INCLUDES = r"\.pyi?$"
60:CACHE_DIR = Path(user_cache_dir("black", version=__version__))
61:
62:
63:# types
64:FileContent = str
65:Encoding = str
66:NewLine = str
67:Depth = int
68:NodeType = int
69:LeafID = int
70:Priority = int
71:Index = int
72:LN = Union[Leaf, Node]
73:SplitFunc = Callable[["Line", Collection["Feature"]], Iterator["Line"]]
74:Timestamp = float
75:FileSize = int
76:CacheInfo = Tuple[Timestamp, FileSize]
77:Cache = Dict[Path, CacheInfo]
78:out = partial(click.secho, bold=True, err=True)
79:err = partial(click.secho, fg="red", err=True)
80:

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: Reviewed black.py get_parser_configs implementation and its usage. It correctly replaces get_grammars and returns configurations in the specified orders and conditions, including reserved first then non-reserved for Python 3 targets, and only Py2 grammars for Python 2 targets. Verified lib2to3_parse uses these configs and no remaining get_grammars usage.


Todo item 4 of 19 is correct
description: 'Validate `get_parser_configs()` replaces `get_grammars()` and returns configurations as specified: when no target_versions, order is [Py3.7+ reserved, Py3.0-3.6 non-reserved, Py2.7 no print, Py2.7]; when all Python2 targets, only the two Py2 grammars; when any Python3 targets, only Py3 grammar with: reserved config added if ASYNC_IS_VALID_IDENTIFIER not supported, non-reserved config added if ASYNC_IS_RESERVED_KEYWORD not supported, in that order.'
Validating todo item 5 of 19
description: 'Ensure `lib2to3_parse()` iterates over `get_parser_configs()` (not grammars) and constructs `driver.Driver(grammar, pytree.convert, tokenizer_config=config)` accordingly, passing the tokenizer_config through.'
Calling tool: grepTool
lib2to3_parse\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/data/debug_visitor.py:32:        list(v.visit(lib2to3_parse(code)))
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:711:    src_node = lib2to3_parse(src_contents.lstrip(), mode.target_versions)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:821:def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:919:            code = lib2to3_parse(code)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:147:                exp_node = black.lib2to3_parse(expected)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:154:                exp_node = black.lib2to3_parse(actual)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:877:    def test_lib2to3_parse(self) -> None:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:879:            black.lib2to3_parse("invalid syntax")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:882:        black.lib2to3_parse(straddling)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:883:        black.lib2to3_parse(straddling, {TargetVersion.PY27})
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:884:        black.lib2to3_parse(straddling, {TargetVersion.PY36})
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:885:        black.lib2to3_parse(straddling, {TargetVersion.PY27, TargetVersion.PY36})
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:888:        black.lib2to3_parse(py2_only)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:889:        black.lib2to3_parse(py2_only, {TargetVersion.PY27})
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:891:            black.lib2to3_parse(py2_only, {TargetVersion.PY36})
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:893:            black.lib2to3_parse(py2_only, {TargetVersion.PY27, TargetVersion.PY36})
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:896:        black.lib2to3_parse(py3_only)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:898:            black.lib2to3_parse(py3_only, {TargetVersion.PY27})
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:899:        black.lib2to3_parse(py3_only, {TargetVersion.PY36})
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:900:        black.lib2to3_parse(py3_only, {TargetVersion.PY27, TargetVersion.PY36})
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:903:        node = black.lib2to3_parse("def f(*, arg): ...\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:905:        node = black.lib2to3_parse("def f(*, arg,): ...\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:907:        node = black.lib2to3_parse("f(*arg,)\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:911:        node = black.lib2to3_parse("def f(*, arg): f'string'\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:913:        node = black.lib2to3_parse("123_456\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:915:        node = black.lib2to3_parse("123456\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:918:        node = black.lib2to3_parse(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:925:        node = black.lib2to3_parse(expected)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:928:        node = black.lib2to3_parse(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:930:        node = black.lib2to3_parse(expected)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:934:        node = black.lib2to3_parse("\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:936:        node = black.lib2to3_parse("from __future__ import black\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:938:        node = black.lib2to3_parse("from __future__ import multiple, imports\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:940:        node = black.lib2to3_parse("from __future__ import (parenthesized, imports)\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:942:        node = black.lib2to3_parse(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:946:        node = black.lib2to3_parse("# comment\nfrom __future__ import black\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:948:        node = black.lib2to3_parse('"""docstring"""\nfrom __future__ import black\n')
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:950:        node = black.lib2to3_parse("some(other, code)\nfrom __future__ import black\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:952:        node = black.lib2to3_parse("from some.module import black\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:954:        node = black.lib2to3_parse(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:958:        node = black.lib2to3_parse(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:1008:        n = black.lib2to3_parse("\n")

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
760:        return tiow.read(), encoding, newline
761:
762:
763:@dataclass(frozen=True)
764:class ParserConfig:
765:    grammar: Grammar
766:    tokenizer_config: TokenizerConfig = TokenizerConfig()
767:
768:
769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:
770:    if not target_versions:
771:        # No target_version specified, so try all grammars.
772:        return [
773:            # Python 3.7+
774:            ParserConfig(
775:                pygram.python_grammar_no_print_statement_no_exec_statement,
776:                TokenizerConfig(async_is_reserved_keyword=True),
777:            ),
778:            # Python 3.0-3.6
779:            ParserConfig(
780:                pygram.python_grammar_no_print_statement_no_exec_statement,
781:                TokenizerConfig(async_is_reserved_keyword=False),
782:            ),
783:            # Python 2.7 with future print_function import
784:            ParserConfig(pygram.python_grammar_no_print_statement),
785:            # Python 2.7
786:            ParserConfig(pygram.python_grammar),
787:        ]
788:    elif all(version.is_python2() for version in target_versions):
789:        # Python 2-only code, so try Python 2 grammars.
790:        return [
791:            # Python 2.7 with future print_function import
792:            ParserConfig(pygram.python_grammar_no_print_statement),
793:            # Python 2.7
794:            ParserConfig(pygram.python_grammar),
795:        ]
796:    else:
797:        # Python 3-compatible code, so only try Python 3 grammar.
798:        configs = []
799:        # If we have to parse both, try to parse async as a keyword first
800:        if not supports_feature(target_versions, Feature.ASYNC_IS_VALID_IDENTIFIER):
801:            # Python 3.7+
802:            configs.append(
803:                ParserConfig(
804:                    pygram.python_grammar_no_print_statement_no_exec_statement,
805:                    TokenizerConfig(async_is_reserved_keyword=True),
806:                )
807:            )
808:        if not supports_feature(target_versions, Feature.ASYNC_IS_RESERVED_KEYWORD):
809:            # Python 3.0-3.6
810:            configs.append(
811:                ParserConfig(
812:                    pygram.python_grammar_no_print_statement_no_exec_statement,
813:                    TokenizerConfig(async_is_reserved_keyword=False),
814:                )
815:            )
816:        # At least one of the above branches must have been taken, because every Python
817:        # version has exactly one of the two 'ASYNC_IS_*' flags
818:        return configs
819:
820:
821:def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:
822:    """Given a string with source, return the lib2to3 Node."""
823:    if src_txt[-1:] != "\n":
824:        src_txt += "\n"
825:
826:    for parser_config in get_parser_configs(set(target_versions)):
827:        drv = driver.Driver(
828:            parser_config.grammar,
829:            pytree.convert,
830:            tokenizer_config=parser_config.tokenizer_config,
831:        )
832:        try:
833:            result = drv.parse_string(src_txt, True)
834:            break
835:
836:        except ParseError as pe:
837:            lineno, column = pe.context[1]
838:            lines = src_txt.splitlines()
839:            try:
840:                faulty_line = lines[lineno - 1]
841:            except IndexError:
842:                faulty_line = "<line number missing in source>"
843:            exc = InvalidInput(f"Cannot parse: {lineno}:{column}: {faulty_line}")
844:    else:
845:        raise exc from None
846:
847:    if isinstance(result, Leaf):
848:        result = Node(syms.file_input, [result])
849:    return result
850:
851:
852:def lib2to3_unparse(node: Node) -> str:
853:    """Given a lib2to3 node, return its string representation."""
854:    code = str(node)
855:    return code
856:
857:
858:T = TypeVar("T")
859:
860:
861:class Visitor(Generic[T]):
862:    """Basic lib2to3 visitor that yields things of type `T` on `visit()`."""
863:
864:    def visit(self, node: LN) -> Iterator[T]:
865:        """Main method to visit `node` and its children.
866:
867:        It tries to find a `visit_*()` method for the given `node.type`, like
868:        `visit_simple_stmt` for Node objects or `visit_INDENT` for Leaf objects.
869:        If no dedicated `visit_*()` method is found, chooses `visit_default()`
870:        instead.
871:
872:        Then yields objects of type `T` from the selected visitor.
873:        """
874:        if node.type < 256:
875:            name = token.tok_name[node.type]
876:        else:
877:            name = type_repr(node.type)
878:        yield from getattr(self, f"visit_{name}", self.visit_default)(node)
879:

Calling tool: grepTool
class Driver\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py:30:class Driver(object):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Modifications:
5:# Copyright 2006 Google, Inc. All Rights Reserved.
6:# Licensed to PSF under a Contributor Agreement.
7:
8:"""Parser driver.
9:
10:This provides a high-level interface to parse a file into a syntax tree.
11:
12:"""
13:
14:__author__ = "Guido van Rossum <guido@python.org>"
15:
16:__all__ = ["Driver", "load_grammar"]
17:
18:# Python imports
19:import codecs
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(
33:        self,
34:        grammar,
35:        convert=None,
36:        logger=None,
37:        tokenizer_config=tokenize.TokenizerConfig(),
38:    ):
39:        self.grammar = grammar
40:        if logger is None:
41:            logger = logging.getLogger(__name__)
42:        self.logger = logger
43:        self.convert = convert
44:        self.tokenizer_config = tokenizer_config
45:
46:    def parse_tokens(self, tokens, debug=False):
47:        """Parse a series of tokens and return the syntax tree."""
48:        # XXX Move the prefix computation into a wrapper around tokenize.
49:        p = parse.Parser(self.grammar, self.convert)
50:        p.setup()
51:        lineno = 1
52:        column = 0
53:        indent_columns = []
54:        type = value = start = end = line_text = None
55:        prefix = ""
56:        for quintuple in tokens:
57:            type, value, start, end, line_text = quintuple
58:            if start != (lineno, column):
59:                assert (lineno, column) <= start, ((lineno, column), start)
60:                s_lineno, s_column = start
61:                if lineno < s_lineno:
62:                    prefix += "\n" * (s_lineno - lineno)
63:                    lineno = s_lineno
64:                    column = 0
65:                if column < s_column:
66:                    prefix += line_text[column:s_column]
67:                    column = s_column
68:            if type in (tokenize.COMMENT, tokenize.NL):
69:                prefix += value
70:                lineno, column = end
71:                if value.endswith("\n"):
72:                    lineno += 1
73:                    column = 0
74:                continue
75:            if type == token.OP:
76:                type = grammar.opmap[value]
77:            if debug:
78:                self.logger.debug("%s %r (prefix=%r)",
79:                                  token.tok_name[type], value, prefix)
80:            if type == token.INDENT:
81:                indent_columns.append(len(value))
82:                _prefix = prefix + value
83:                prefix = ""
84:                value = ""
85:            elif type == token.DEDENT:
86:                _indent_col = indent_columns.pop()
87:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
88:            if p.addtoken(type, value, (prefix, start)):
89:                if debug:
90:                    self.logger.debug("Stop.")
91:                break
92:            prefix = ""
93:            if type in {token.INDENT, token.DEDENT}:
94:                prefix = _prefix
95:            lineno, column = end
96:            if value.endswith("\n"):
97:                lineno += 1
98:                column = 0
99:        else:
100:            # We never broke out -- EOF is too soon (how can this happen???)
101:            raise parse.ParseError("incomplete input",
102:                                   type, value, (prefix, start))
103:        return p.rootnode
104:
105:    def parse_stream_raw(self, stream, debug=False):
106:        """Parse a stream and return the syntax tree."""
107:        tokens = tokenize.generate_tokens(stream.readline, config=self.tokenizer_config)
108:        return self.parse_tokens(tokens, debug)
109:
110:    def parse_stream(self, stream, debug=False):
111:        """Parse a stream and return the syntax tree."""
112:        return self.parse_stream_raw(stream, debug)
113:
114:    def parse_file(self, filename, encoding=None, debug=False):
115:        """Parse a file and return the syntax tree."""
116:        with io.open(filename, "r", encoding=encoding) as stream:
117:            return self.parse_stream(stream, debug)
118:
119:    def parse_string(self, text, debug=False):
120:        """Parse a string and return the syntax tree."""
121:        tokens = tokenize.generate_tokens(
122:            io.StringIO(text).readline,
123:            config=self.tokenizer_config,
124:        )
125:        return self.parse_tokens(tokens, debug)
126:
127:    def _partially_consume_prefix(self, prefix, column):
128:        lines = []
129:        current_line = ""
130:        current_column = 0
131:        wait_for_nl = False
132:        for char in prefix:
133:            current_line += char
134:            if wait_for_nl:
135:                if char == '\n':
136:                    if current_line.strip() and current_column < column:
137:                        res = ''.join(lines)
138:                        return res, prefix[len(res):]
139:
140:                    lines.append(current_line)

Calling tool: grepTool
def generate_tokens\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi:32:def generate_tokens(

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
320:
321:def untokenize(iterable):
322:    """Transform tokens back into Python source code.
323:
324:    Each element returned by the iterable must be a token sequence
325:    with at least two elements, a token number and token value.  If
326:    only two tokens are passed, the resulting output is poor.
327:
328:    Round-trip invariant for full input:
329:        Untokenized source will match input source exactly
330:
331:    Round-trip invariant for limited intput:
332:        # Output text will tokenize the back to the input
333:        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
334:        newcode = untokenize(t1)
335:        readline = iter(newcode.splitlines(1)).next
336:        t2 = [tok[:2] for tokin generate_tokens(readline)]
337:        assert t1 == t2
338:    """
339:    ut = Untokenizer()
340:    return ut.untokenize(iterable)
341:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
343:    """
344:    The generate_tokens() generator requires one argument, readline, which
345:    must be a callable object which provides the same interface as the
346:    readline() method of built-in file objects. Each call to the function
347:    should return one line of input as a string.  Alternately, readline
348:    can be a callable function terminating with StopIteration:
349:        readline = open(myfile).next    # Example of alternate readline
350:
351:    The generator produces 5-tuples with these members: the token type; the
352:    token string; a 2-tuple (srow, scol) of ints specifying the row and
353:    column where the token begins in the source; a 2-tuple (erow, ecol) of
354:    ints specifying the row and column where the token ends in the source;
355:    and the line on which the token was found. The line passed is the
356:    logical line; continuation lines are included.
357:    """
358:    lnum = parenlev = continued = 0
359:    numchars = '0123456789'
360:    contstr, needcont = '', 0
361:    contline = None
362:    indents = [0]
363:
364:    # If we know we're parsing 3.7+, we can unconditionally parse `async` and
365:    # `await` as keywords.
366:    async_is_reserved_keyword = config.async_is_reserved_keyword
367:    # 'stashed' and 'async_*' are used for async/await parsing
368:    stashed = None
369:    async_def = False
370:    async_def_indent = 0
371:    async_def_nl = False
372:
373:    while 1:                                   # loop over lines in stream
374:        try:
375:            line = readline()
376:        except StopIteration:
377:            line = ''
378:        lnum = lnum + 1
379:        pos, max = 0, len(line)
380:
381:        if contstr:                            # continued string
382:            if not line:
383:                raise TokenError("EOF in multi-line string", strstart)
384:            endmatch = endprog.match(line)
385:            if endmatch:
386:                pos = end = endmatch.end(0)
387:                yield (STRING, contstr + line[:end],
388:                       strstart, (lnum, end), contline + line)
389:                contstr, needcont = '', 0
390:                contline = None
391:            elif needcont and line[-2:] != '\\\n' and line[-3:] != '\\\r\n':
392:                yield (ERRORTOKEN, contstr + line,
393:                           strstart, (lnum, len(line)), contline)
394:                contstr = ''
395:                contline = None
396:                continue
397:            else:
398:                contstr = contstr + line
399:                contline = contline + line
400:                continue
401:
402:        elif parenlev == 0 and not continued:  # new statement
403:            if not line: break
404:            column = 0
405:            while pos < max:                   # measure leading whitespace
406:                if line[pos] == ' ': column = column + 1
407:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize
408:                elif line[pos] == '\f': column = 0
409:                else: break
410:                pos = pos + 1
411:            if pos == max: break
412:
413:            if stashed:
414:                yield stashed
415:                stashed = None
416:
417:            if line[pos] in '\r\n':            # skip blank lines
418:                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)
419:                continue
420:
421:            if line[pos] == '#':               # skip comments
422:                comment_token = line[pos:].rstrip('\r\n')
423:                nl_pos = pos + len(comment_token)
424:                yield (COMMENT, comment_token,
425:                        (lnum, pos), (lnum, pos + len(comment_token)), line)
426:                yield (NL, line[nl_pos:],
427:                        (lnum, nl_pos), (lnum, len(line)), line)
428:                continue
429:
430:            if column > indents[-1]:           # count indents
431:                indents.append(column)
432:                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)
433:
434:            while column < indents[-1]:        # count dedents
435:                if column not in indents:
436:                    raise IndentationError(
437:                        "unindent does not match any outer indentation level",
438:                        ("<tokenize>", lnum, pos, line))
439:                indents = indents[:-1]
440:
441:                if async_def and async_def_indent >= indents[-1]:
442:                    async_def = False
443:                    async_def_nl = False
444:                    async_def_indent = 0
445:
446:                yield (DEDENT, '', (lnum, pos), (lnum, pos), line)
447:
448:            if async_def and async_def_nl and async_def_indent >= indents[-1]:
449:                async_def = False
450:                async_def_nl = False
451:                async_def_indent = 0
452:
453:        else:                                  # continued statement
454:            if not line:
455:                raise TokenError("EOF in multi-line statement", (lnum, 0))
456:            continued = 0
457:
458:        while pos < max:
459:            pseudomatch = pseudoprog.match(line, pos)
460:            if pseudomatch:                                # scan for tokens
461:                start, end = pseudomatch.span(1)
462:                spos, epos, pos = (lnum, start), (lnum, end), end
463:                token, initial = line[start:end], line[start]
464:
465:                if initial in numchars or \
466:                   (initial == '.' and token != '.'):      # ordinary number
467:                    yield (NUMBER, token, spos, epos, line)
468:                elif initial in '\r\n':
469:                    newline = NEWLINE
470:                    if parenlev > 0:
471:                        newline = NL
472:                    elif async_def:
473:                        async_def_nl = True
474:                    if stashed:
475:                        yield stashed
476:                        stashed = None
477:                    yield (newline, token, spos, epos, line)
478:
479:                elif initial == '#':
480:                    assert not token.endswith("\n")
481:                    if stashed:
482:                        yield stashed
483:                        stashed = None
484:                    yield (COMMENT, token, spos, epos, line)
485:                elif token in triple_quoted:
486:                    endprog = endprogs[token]
487:                    endmatch = endprog.match(line, pos)
488:                    if endmatch:                           # all on one line
489:                        pos = endmatch.end(0)
490:                        token = line[start:pos]
491:                        if stashed:
492:                            yield stashed
493:                            stashed = None
494:                        yield (STRING, token, spos, (lnum, pos), line)
495:                    else:
496:                        strstart = (lnum, start)           # multiple lines
497:                        contstr = line[start:]
498:                        contline = line
499:                        break
500:                elif initial in single_quoted or \
501:                    token[:2] in single_quoted or \
502:                    token[:3] in single_quoted:
503:                    if token[-1] == '\n':                  # continued string
504:                        strstart = (lnum, start)
505:                        endprog = (endprogs[initial] or endprogs[token[1]] or
506:                                   endprogs[token[2]])
507:                        contstr, needcont = line[start:], 1
508:                        contline = line
509:                        break
510:                    else:                                  # ordinary string
511:                        if stashed:
512:                            yield stashed
513:                            stashed = None
514:                        yield (STRING, token, spos, epos, line)
515:                elif initial.isidentifier():               # ordinary name
516:                    if token in ('async', 'await'):
517:                        if async_is_reserved_keyword or async_def:
518:                            yield (ASYNC if token == 'async' else AWAIT,
519:                                   token, spos, epos, line)
520:                            continue
521:
522:                    tok = (NAME, token, spos, epos, line)
523:                    if token == 'async' and not stashed:
524:                        stashed = tok
525:                        continue
526:
527:                    if token in ('def', 'for'):
528:                        if (stashed
529:                                and stashed[0] == NAME
530:                                and stashed[1] == 'async'):
531:
532:                            if token == 'def':
533:                                async_def = True
534:                                async_def_indent = indents[-1]
535:
536:                            yield (ASYNC, stashed[1],
537:                                   stashed[2], stashed[3],
538:                                   stashed[4])
539:                            stashed = None
540:
541:                    if stashed:
542:                        yield stashed
543:                        stashed = None
544:
545:                    yield tok
546:                elif initial == '\\':                      # continued stmt
547:                    # This yield is new; needed for better idempotency:
548:                    if stashed:
549:                        yield stashed
550:                        stashed = None
551:                    yield (NL, token, spos, (lnum, pos), line)
552:                    continued = 1
553:                else:
554:                    if initial in '([{': parenlev = parenlev + 1
555:                    elif initial in ')]}': parenlev = parenlev - 1
556:                    if stashed:
557:                        yield stashed
558:                        stashed = None
559:                    yield (OP, token, spos, epos, line)
560:            else:
561:                yield (ERRORTOKEN, line[pos],
562:                           (lnum, pos), (lnum, pos+1), line)
563:                pos = pos + 1
564:
565:    if stashed:
566:        yield stashed
567:        stashed = None
568:
569:    for indent in indents[1:]:                 # pop remaining indent levels
570:        yield (DEDENT, '', (lnum, 0), (lnum, 0), '')
571:    yield (ENDMARKER, '', (lnum, 0), (lnum, 0), '')
572:
573:if __name__ == '__main__':                     # testing
574:    import sys
575:    if len(sys.argv) > 1: tokenize(open(sys.argv[1]).readline)
576:    else: tokenize(sys.stdin.readline)

Calling tool: grepTool
from blib2to3\.pgen2\.tokenize import TokenizerConfig
Matched content:
51:from blib2to3.pgen2.tokenize import TokenizerConfig

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
40:from attr import dataclass, evolve, Factory
41:import click
42:import toml
43:from typed_ast import ast3, ast27
44:
45:# lib2to3 fork
46:from blib2to3.pytree import Node, Leaf, type_repr
47:from blib2to3 import pygram, pytree
48:from blib2to3.pgen2 import driver, token
49:from blib2to3.pgen2.grammar import Grammar
50:from blib2to3.pgen2.parse import ParseError
51:from blib2to3.pgen2.tokenize import TokenizerConfig
52:
53:
54:__version__ = "19.3b0"
55:DEFAULT_LINE_LENGTH = 88
56:DEFAULT_EXCLUDES = (
57:    r"/(\.eggs|\.git|\.hg|\.mypy_cache|\.nox|\.tox|\.venv|_build|buck-out|build|dist)/"
58:)
59:DEFAULT_INCLUDES = r"\.pyi?$"
60:CACHE_DIR = Path(user_cache_dir("black", version=__version__))
61:
62:
63:# types
64:FileContent = str
65:Encoding = str
66:NewLine = str
67:Depth = int
68:NodeType = int
69:LeafID = int

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: In black.py, lib2to3_parse() now iterates over get_parser_configs() and constructs driver.Driver(parser_config.grammar, pytree.convert, tokenizer_config=parser_config.tokenizer_config) (lines 826-831). Driver accepts tokenizer_config and passes it to tokenize.generate_tokens in parse_stream_raw and parse_string (driver.py lines 32-45, 105-108, 119-125). Thus the tokenizer_config is correctly passed through.


Todo item 5 of 19 is correct
description: 'Ensure `lib2to3_parse()` iterates over `get_parser_configs()` (not grammars) and constructs `driver.Driver(grammar, pytree.convert, tokenizer_config=config)` accordingly, passing the tokenizer_config through.'
Validating todo item 6 of 19
description: 'Confirm `driver.Driver.__init__` accepts a `tokenizer_config` parameter with default `tokenize.TokenizerConfig()` and stores it on `self.tokenizer_config`.'
Calling tool: globTool
**/blib2to3/pgen2/driver.py
Matched files:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py

Calling tool: grepTool
class Driver
Matched content:
30:class Driver(object):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Modifications:
5:# Copyright 2006 Google, Inc. All Rights Reserved.
6:# Licensed to PSF under a Contributor Agreement.
7:
8:"""Parser driver.
9:
10:This provides a high-level interface to parse a file into a syntax tree.
11:
12:"""
13:
14:__author__ = "Guido van Rossum <guido@python.org>"
15:
16:__all__ = ["Driver", "load_grammar"]
17:
18:# Python imports
19:import codecs
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(
33:        self,
34:        grammar,
35:        convert=None,
36:        logger=None,
37:        tokenizer_config=tokenize.TokenizerConfig(),
38:    ):
39:        self.grammar = grammar
40:        if logger is None:
41:            logger = logging.getLogger(__name__)
42:        self.logger = logger
43:        self.convert = convert
44:        self.tokenizer_config = tokenizer_config
45:
46:    def parse_tokens(self, tokens, debug=False):
47:        """Parse a series of tokens and return the syntax tree."""
48:        # XXX Move the prefix computation into a wrapper around tokenize.
49:        p = parse.Parser(self.grammar, self.convert)
50:        p.setup()
51:        lineno = 1
52:        column = 0
53:        indent_columns = []
54:        type = value = start = end = line_text = None
55:        prefix = ""
56:        for quintuple in tokens:
57:            type, value, start, end, line_text = quintuple
58:            if start != (lineno, column):
59:                assert (lineno, column) <= start, ((lineno, column), start)
60:                s_lineno, s_column = start
61:                if lineno < s_lineno:
62:                    prefix += "\n" * (s_lineno - lineno)
63:                    lineno = s_lineno
64:                    column = 0
65:                if column < s_column:
66:                    prefix += line_text[column:s_column]
67:                    column = s_column
68:            if type in (tokenize.COMMENT, tokenize.NL):
69:                prefix += value
70:                lineno, column = end
71:                if value.endswith("\n"):
72:                    lineno += 1
73:                    column = 0
74:                continue
75:            if type == token.OP:
76:                type = grammar.opmap[value]
77:            if debug:
78:                self.logger.debug("%s %r (prefix=%r)",
79:                                  token.tok_name[type], value, prefix)
80:            if type == token.INDENT:

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: In blib2to3/pgen2/driver.py, Driver.__init__ defines a tokenizer_config parameter with default tokenize.TokenizerConfig() (lines 32-38) and assigns it to self.tokenizer_config (line 44).


Todo item 6 of 19 is correct
description: 'Confirm `driver.Driver.__init__` accepts a `tokenizer_config` parameter with default `tokenize.TokenizerConfig()` and stores it on `self.tokenizer_config`.'
Validating todo item 7 of 19
description: 'Verify `driver.Driver.parse_stream_raw()` and `driver.Driver.parse_string()` call `tokenize.generate_tokens(..., config=self.tokenizer_config)`; ensure `parse_file()` uses `parse_stream()` and thus the config path is used.'
Calling tool: grepTool
class Driver
Grep command failed with exit code 2: rg: /Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/{{FOLDER_PATH}}: IO error for operation on /Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/{{FOLDER_PATH}}: No such file or directory (os error 2)


Calling tool: grepTool
class Driver
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py:30:class Driver(object):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.pyi:14:class Driver:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Modifications:
5:# Copyright 2006 Google, Inc. All Rights Reserved.
6:# Licensed to PSF under a Contributor Agreement.
7:
8:"""Parser driver.
9:
10:This provides a high-level interface to parse a file into a syntax tree.
11:
12:"""
13:
14:__author__ = "Guido van Rossum <guido@python.org>"
15:
16:__all__ = ["Driver", "load_grammar"]
17:
18:# Python imports
19:import codecs
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(
33:        self,
34:        grammar,
35:        convert=None,
36:        logger=None,
37:        tokenizer_config=tokenize.TokenizerConfig(),
38:    ):
39:        self.grammar = grammar
40:        if logger is None:
41:            logger = logging.getLogger(__name__)
42:        self.logger = logger
43:        self.convert = convert
44:        self.tokenizer_config = tokenizer_config
45:
46:    def parse_tokens(self, tokens, debug=False):
47:        """Parse a series of tokens and return the syntax tree."""
48:        # XXX Move the prefix computation into a wrapper around tokenize.
49:        p = parse.Parser(self.grammar, self.convert)
50:        p.setup()
51:        lineno = 1
52:        column = 0
53:        indent_columns = []
54:        type = value = start = end = line_text = None
55:        prefix = ""
56:        for quintuple in tokens:
57:            type, value, start, end, line_text = quintuple
58:            if start != (lineno, column):
59:                assert (lineno, column) <= start, ((lineno, column), start)
60:                s_lineno, s_column = start
61:                if lineno < s_lineno:
62:                    prefix += "\n" * (s_lineno - lineno)
63:                    lineno = s_lineno
64:                    column = 0
65:                if column < s_column:
66:                    prefix += line_text[column:s_column]
67:                    column = s_column
68:            if type in (tokenize.COMMENT, tokenize.NL):
69:                prefix += value
70:                lineno, column = end
71:                if value.endswith("\n"):
72:                    lineno += 1
73:                    column = 0
74:                continue
75:            if type == token.OP:
76:                type = grammar.opmap[value]
77:            if debug:
78:                self.logger.debug("%s %r (prefix=%r)",
79:                                  token.tok_name[type], value, prefix)
80:            if type == token.INDENT:
81:                indent_columns.append(len(value))
82:                _prefix = prefix + value
83:                prefix = ""
84:                value = ""
85:            elif type == token.DEDENT:
86:                _indent_col = indent_columns.pop()
87:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
88:            if p.addtoken(type, value, (prefix, start)):
89:                if debug:
90:                    self.logger.debug("Stop.")
91:                break
92:            prefix = ""
93:            if type in {token.INDENT, token.DEDENT}:
94:                prefix = _prefix
95:            lineno, column = end
96:            if value.endswith("\n"):
97:                lineno += 1
98:                column = 0
99:        else:
100:            # We never broke out -- EOF is too soon (how can this happen???)
101:            raise parse.ParseError("incomplete input",
102:                                   type, value, (prefix, start))
103:        return p.rootnode
104:
105:    def parse_stream_raw(self, stream, debug=False):
106:        """Parse a stream and return the syntax tree."""
107:        tokens = tokenize.generate_tokens(stream.readline, config=self.tokenizer_config)
108:        return self.parse_tokens(tokens, debug)
109:
110:    def parse_stream(self, stream, debug=False):
111:        """Parse a stream and return the syntax tree."""
112:        return self.parse_stream_raw(stream, debug)
113:
114:    def parse_file(self, filename, encoding=None, debug=False):
115:        """Parse a file and return the syntax tree."""
116:        with io.open(filename, "r", encoding=encoding) as stream:
117:            return self.parse_stream(stream, debug)
118:
119:    def parse_string(self, text, debug=False):
120:        """Parse a string and return the syntax tree."""
121:        tokens = tokenize.generate_tokens(
122:            io.StringIO(text).readline,
123:            config=self.tokenizer_config,
124:        )
125:        return self.parse_tokens(tokens, debug)
126:
127:    def _partially_consume_prefix(self, prefix, column):
128:        lines = []
129:        current_line = ""
130:        current_column = 0
131:        wait_for_nl = False
132:        for char in prefix:
133:            current_line += char
134:            if wait_for_nl:
135:                if char == '\n':
136:                    if current_line.strip() and current_column < column:
137:                        res = ''.join(lines)
138:                        return res, prefix[len(res):]
139:
140:                    lines.append(current_line)
141:                    current_line = ""
142:                    current_column = 0
143:                    wait_for_nl = False
144:            elif char in ' \t':
145:                current_column += 1
146:            elif char == '\n':
147:                # unexpected empty line
148:                current_column = 0
149:            else:
150:                # indent is finished
151:                wait_for_nl = True
152:        return ''.join(lines), current_line
153:
154:
155:def _generate_pickle_name(gt, cache_dir=None):
156:    head, tail = os.path.splitext(gt)
157:    if tail == ".txt":
158:        tail = ""
159:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
160:    if cache_dir:
161:        return os.path.join(cache_dir, os.path.basename(name))
162:    else:
163:        return name
164:
165:
166:def load_grammar(gt="Grammar.txt", gp=None,
167:                 save=True, force=False, logger=None):
168:    """Load the grammar (maybe from a pickle)."""
169:    if logger is None:
170:        logger = logging.getLogger(__name__)
171:    gp = _generate_pickle_name(gt) if gp is None else gp
172:    if force or not _newer(gp, gt):
173:        logger.info("Generating grammar tables from %s", gt)
174:        g = pgen.generate_grammar(gt)
175:        if save:
176:            logger.info("Writing grammar tables to %s", gp)
177:            try:
178:                g.dump(gp)
179:            except OSError as e:
180:                logger.info("Writing failed: %s", e)

Calling tool: grepTool
class TokenizerConfig
Matched content:
142:class TokenizerConfig:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
120:)
121:
122:endprogs = {"'": re.compile(Single), '"': re.compile(Double),
123:            "'''": single3prog, '"""': double3prog,
124:            **{f"{prefix}'''": single3prog for prefix in _strprefixes},
125:            **{f'{prefix}"""': double3prog for prefix in _strprefixes},
126:            **{prefix: None for prefix in _strprefixes}}
127:
128:triple_quoted = (
129:    {"'''", '"""'} |
130:    {f"{prefix}'''" for prefix in _strprefixes} |
131:    {f'{prefix}"""' for prefix in _strprefixes}
132:)
133:single_quoted = (
134:    {"'", '"'} |
135:    {f"{prefix}'" for prefix in _strprefixes} |
136:    {f'{prefix}"' for prefix in _strprefixes}
137:)
138:
139:tabsize = 8
140:
141:@dataclass(frozen=True)
142:class TokenizerConfig:
143:    async_is_reserved_keyword: bool = False
144:
145:class TokenError(Exception): pass
146:
147:class StopTokenizing(Exception): pass
148:
149:def printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line): # for testing
150:    (srow, scol) = xxx_todo_changeme
151:    (erow, ecol) = xxx_todo_changeme1
152:    print("%d,%d-%d,%d:\t%s\t%s" % \
153:        (srow, scol, erow, ecol, tok_name[type], repr(token)))
154:
155:def tokenize(readline, tokeneater=printtoken):
156:    """
157:    The tokenize() function accepts two parameters: one representing the
158:    input stream, and one providing an output mechanism for tokenize().
159:
160:    The first parameter, readline, must be a callable object which provides
161:    the same interface as the readline() method of built-in file objects.
162:    Each call to the function should return one line of input as a string.
163:
164:    The second parameter, tokeneater, must also be a callable object. It is
165:    called once for each token, with five arguments, corresponding to the
166:    tuples generated by generate_tokens().
167:    """
168:    try:
169:        tokenize_loop(readline, tokeneater)
170:    except StopTokenizing:
171:        pass
172:
173:# backwards compatible interface
174:def tokenize_loop(readline, tokeneater):
175:    for token_info in generate_tokens(readline):
176:        tokeneater(*token_info)
177:
178:class Untokenizer:
179:
180:    def __init__(self):
181:        self.tokens = []
182:        self.prev_row = 1
183:        self.prev_col = 0
184:
185:    def add_whitespace(self, start):
186:        row, col = start
187:        assert row <= self.prev_row
188:        col_offset = col - self.prev_col
189:        if col_offset:

Calling tool: grepTool
def generate_tokens\(
Matched content:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
334:        newcode = untokenize(t1)
335:        readline = iter(newcode.splitlines(1)).next
336:        t2 = [tok[:2] for tokin generate_tokens(readline)]
337:        assert t1 == t2
338:    """
339:    ut = Untokenizer()
340:    return ut.untokenize(iterable)
341:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
343:    """
344:    The generate_tokens() generator requires one argument, readline, which
345:    must be a callable object which provides the same interface as the
346:    readline() method of built-in file objects. Each call to the function
347:    should return one line of input as a string.  Alternately, readline
348:    can be a callable function terminating with StopIteration:
349:        readline = open(myfile).next    # Example of alternate readline
350:
351:    The generator produces 5-tuples with these members: the token type; the
352:    token string; a 2-tuple (srow, scol) of ints specifying the row and
353:    column where the token begins in the source; a 2-tuple (erow, ecol) of
354:    ints specifying the row and column where the token ends in the source;
355:    and the line on which the token was found. The line passed is the
356:    logical line; continuation lines are included.
357:    """
358:    lnum = parenlev = continued = 0
359:    numchars = '0123456789'
360:    contstr, needcont = '', 0
361:    contline = None
362:    indents = [0]
363:
364:    # If we know we're parsing 3.7+, we can unconditionally parse `async` and
365:    # `await` as keywords.
366:    async_is_reserved_keyword = config.async_is_reserved_keyword
367:    # 'stashed' and 'async_*' are used for async/await parsing
368:    stashed = None
369:    async_def = False
370:    async_def_indent = 0
371:    async_def_nl = False
372:
373:    while 1:                                   # loop over lines in stream
374:        try:
375:            line = readline()
376:        except StopIteration:
377:            line = ''
378:        lnum = lnum + 1
379:        pos, max = 0, len(line)
380:
381:        if contstr:                            # continued string
382:            if not line:
383:                raise TokenError("EOF in multi-line string", strstart)
384:            endmatch = endprog.match(line)
385:            if endmatch:
386:                pos = end = endmatch.end(0)
387:                yield (STRING, contstr + line[:end],
388:                       strstart, (lnum, end), contline + line)
389:                contstr, needcont = '', 0
390:                contline = None
391:            elif needcont and line[-2:] != '\\\n' and line[-3:] != '\\\r\n':
392:                yield (ERRORTOKEN, contstr + line,
393:                           strstart, (lnum, len(line)), contline)
394:                contstr = ''
395:                contline = None
396:                continue
397:            else:
398:                contstr = contstr + line
399:                contline = contline + line
400:                continue
401:
402:        elif parenlev == 0 and not continued:  # new statement
403:            if not line: break
404:            column = 0
405:            while pos < max:                   # measure leading whitespace
406:                if line[pos] == ' ': column = column + 1
407:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize
408:                elif line[pos] == '\f': column = 0
409:                else: break
410:                pos = pos + 1
411:            if pos == max: break
412:
413:            if stashed:
414:                yield stashed
415:                stashed = None
416:
417:            if line[pos] in '\r\n':            # skip blank lines
418:                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)
419:                continue
420:
421:            if line[pos] == '#':               # skip comments
422:                comment_token = line[pos:].rstrip('\r\n')
423:                nl_pos = pos + len(comment_token)
424:                yield (COMMENT, comment_token,
425:                        (lnum, pos), (lnum, pos + len(comment_token)), line)
426:                yield (NL, line[nl_pos:],
427:                        (lnum, nl_pos), (lnum, len(line)), line)
428:                continue
429:
430:            if column > indents[-1]:           # count indents
431:                indents.append(column)
432:                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)
433:
434:            while column < indents[-1]:        # count dedents
435:                if column not in indents:
436:                    raise IndentationError(
437:                        "unindent does not match any outer indentation level",
438:                        ("<tokenize>", lnum, pos, line))
439:                indents = indents[:-1]
440:
441:                if async_def and async_def_indent >= indents[-1]:
442:                    async_def = False
443:                    async_def_nl = False
444:                    async_def_indent = 0
445:
446:                yield (DEDENT, '', (lnum, pos), (lnum, pos), line)
447:
448:            if async_def and async_def_nl and async_def_indent >= indents[-1]:
449:                async_def = False
450:                async_def_nl = False
451:                async_def_indent = 0
452:
453:        else:                                  # continued statement
454:            if not line:
455:                raise TokenError("EOF in multi-line statement", (lnum, 0))
456:            continued = 0
457:
458:        while pos < max:
459:            pseudomatch = pseudoprog.match(line, pos)
460:            if pseudomatch:                                # scan for tokens
461:                start, end = pseudomatch.span(1)
462:                spos, epos, pos = (lnum, start), (lnum, end), end
463:                token, initial = line[start:end], line[start]
464:
465:                if initial in numchars or \
466:                   (initial == '.' and token != '.'):      # ordinary number
467:                    yield (NUMBER, token, spos, epos, line)
468:                elif initial in '\r\n':
469:                    newline = NEWLINE
470:                    if parenlev > 0:
471:                        newline = NL
472:                    elif async_def:
473:                        async_def_nl = True
474:                    if stashed:
475:                        yield stashed
476:                        stashed = None
477:                    yield (newline, token, spos, epos, line)
478:
479:                elif initial == '#':
480:                    assert not token.endswith("\n")
481:                    if stashed:
482:                        yield stashed
483:                        stashed = None
484:                    yield (COMMENT, token, spos, epos, line)
485:                elif token in triple_quoted:
486:                    endprog = endprogs[token]
487:                    endmatch = endprog.match(line, pos)
488:                    if endmatch:                           # all on one line
489:                        pos = endmatch.end(0)
490:                        token = line[start:pos]
491:                        if stashed:
492:                            yield stashed
493:                            stashed = None
494:                        yield (STRING, token, spos, (lnum, pos), line)
495:                    else:
496:                        strstart = (lnum, start)           # multiple lines
497:                        contstr = line[start:]
498:                        contline = line
499:                        break
500:                elif initial in single_quoted or \
501:                    token[:2] in single_quoted or \
502:                    token[:3] in single_quoted:
503:                    if token[-1] == '\n':                  # continued string
504:                        strstart = (lnum, start)
505:                        endprog = (endprogs[initial] or endprogs[token[1]] or
506:                                   endprogs[token[2]])
507:                        contstr, needcont = line[start:], 1
508:                        contline = line
509:                        break
510:                    else:                                  # ordinary string
511:                        if stashed:
512:                            yield stashed
513:                            stashed = None
514:                        yield (STRING, token, spos, epos, line)
515:                elif initial.isidentifier():               # ordinary name
516:                    if token in ('async', 'await'):
517:                        if async_is_reserved_keyword or async_def:
518:                            yield (ASYNC if token == 'async' else AWAIT,
519:                                   token, spos, epos, line)
520:                            continue
521:
522:                    tok = (NAME, token, spos, epos, line)
523:                    if token == 'async' and not stashed:
524:                        stashed = tok
525:                        continue
526:
527:                    if token in ('def', 'for'):
528:                        if (stashed
529:                                and stashed[0] == NAME
530:                                and stashed[1] == 'async'):
531:
532:                            if token == 'def':
533:                                async_def = True
534:                                async_def_indent = indents[-1]
535:
536:                            yield (ASYNC, stashed[1],
537:                                   stashed[2], stashed[3],
538:                                   stashed[4])
539:                            stashed = None
540:
541:                    if stashed:
542:                        yield stashed
543:                        stashed = None
544:
545:                    yield tok
546:                elif initial == '\\':                      # continued stmt
547:                    # This yield is new; needed for better idempotency:
548:                    if stashed:
549:                        yield stashed
550:                        stashed = None
551:                    yield (NL, token, spos, (lnum, pos), line)
552:                    continued = 1
553:                else:
554:                    if initial in '([{': parenlev = parenlev + 1
555:                    elif initial in ')]}': parenlev = parenlev - 1
556:                    if stashed:
557:                        yield stashed
558:                        stashed = None
559:                    yield (OP, token, spos, epos, line)
560:            else:
561:                yield (ERRORTOKEN, line[pos],
562:                           (lnum, pos), (lnum, pos+1), line)
563:                pos = pos + 1
564:
565:    if stashed:
566:        yield stashed
567:        stashed = None
568:
569:    for indent in indents[1:]:                 # pop remaining indent levels
570:        yield (DEDENT, '', (lnum, 0), (lnum, 0), '')
571:    yield (ENDMARKER, '', (lnum, 0), (lnum, 0), '')
572:
573:if __name__ == '__main__':                     # testing

Calling tool: grepTool
def get_parser_configs\(|def lib2to3_parse\(
Matched content:
769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:
821:def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
730:        for feature in {Feature.TRAILING_COMMA_IN_CALL, Feature.TRAILING_COMMA_IN_DEF}
731:        if supports_feature(versions, feature)
732:    }
733:    for current_line in lines.visit(src_node):
734:        for _ in range(after):
735:            dst_contents += str(empty_line)
736:        before, after = elt.maybe_empty_lines(current_line)
737:        for _ in range(before):
738:            dst_contents += str(empty_line)
739:        for line in split_line(
740:            current_line, line_length=mode.line_length, features=split_line_features
741:        ):
742:            dst_contents += str(line)
743:    return dst_contents
744:
745:
746:def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:
747:    """Return a tuple of (decoded_contents, encoding, newline).
748:
749:    `newline` is either CRLF or LF but `decoded_contents` is decoded with
750:    universal newlines (i.e. only contains LF).
751:    """
752:    srcbuf = io.BytesIO(src)
753:    encoding, lines = tokenize.detect_encoding(srcbuf.readline)
754:    if not lines:
755:        return "", encoding, "\n"
756:
757:    newline = "\r\n" if b"\r\n" == lines[0][-2:] else "\n"
758:    srcbuf.seek(0)
759:    with io.TextIOWrapper(srcbuf, encoding) as tiow:
760:        return tiow.read(), encoding, newline
761:
762:
763:@dataclass(frozen=True)
764:class ParserConfig:
765:    grammar: Grammar
766:    tokenizer_config: TokenizerConfig = TokenizerConfig()
767:
768:
769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:
770:    if not target_versions:
771:        # No target_version specified, so try all grammars.
772:        return [
773:            # Python 3.7+
774:            ParserConfig(
775:                pygram.python_grammar_no_print_statement_no_exec_statement,
776:                TokenizerConfig(async_is_reserved_keyword=True),
777:            ),
778:            # Python 3.0-3.6
779:            ParserConfig(
780:                pygram.python_grammar_no_print_statement_no_exec_statement,
781:                TokenizerConfig(async_is_reserved_keyword=False),
782:            ),
783:            # Python 2.7 with future print_function import
784:            ParserConfig(pygram.python_grammar_no_print_statement),
785:            # Python 2.7
786:            ParserConfig(pygram.python_grammar),
787:        ]
788:    elif all(version.is_python2() for version in target_versions):
789:        # Python 2-only code, so try Python 2 grammars.
790:        return [
791:            # Python 2.7 with future print_function import
792:            ParserConfig(pygram.python_grammar_no_print_statement),
793:            # Python 2.7
794:            ParserConfig(pygram.python_grammar),
795:        ]
796:    else:
797:        # Python 3-compatible code, so only try Python 3 grammar.
798:        configs = []
799:        # If we have to parse both, try to parse async as a keyword first
800:        if not supports_feature(target_versions, Feature.ASYNC_IS_VALID_IDENTIFIER):
801:            # Python 3.7+
802:            configs.append(
803:                ParserConfig(
804:                    pygram.python_grammar_no_print_statement_no_exec_statement,
805:                    TokenizerConfig(async_is_reserved_keyword=True),
806:                )
807:            )
808:        if not supports_feature(target_versions, Feature.ASYNC_IS_RESERVED_KEYWORD):
809:            # Python 3.0-3.6
810:            configs.append(
811:                ParserConfig(
812:                    pygram.python_grammar_no_print_statement_no_exec_statement,
813:                    TokenizerConfig(async_is_reserved_keyword=False),
814:                )
815:            )
816:        # At least one of the above branches must have been taken, because every Python
817:        # version has exactly one of the two 'ASYNC_IS_*' flags
818:        return configs
819:
820:
821:def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:
822:    """Given a string with source, return the lib2to3 Node."""
823:    if src_txt[-1:] != "\n":
824:        src_txt += "\n"
825:
826:    for parser_config in get_parser_configs(set(target_versions)):
827:        drv = driver.Driver(
828:            parser_config.grammar,
829:            pytree.convert,
830:            tokenizer_config=parser_config.tokenizer_config,
831:        )
832:        try:
833:            result = drv.parse_string(src_txt, True)
834:            break
835:
836:        except ParseError as pe:
837:            lineno, column = pe.context[1]
838:            lines = src_txt.splitlines()
839:            try:
840:                faulty_line = lines[lineno - 1]
841:            except IndexError:
842:                faulty_line = "<line number missing in source>"
843:            exc = InvalidInput(f"Cannot parse: {lineno}:{column}: {faulty_line}")
844:    else:
845:        raise exc from None
846:
847:    if isinstance(result, Leaf):
848:        result = Node(syms.file_input, [result])
849:    return result

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.pyi
1:# Stubs for lib2to3.pgen2.driver (Python 3.6)
2:
3:import os
4:import sys
5:from typing import Any, Callable, IO, Iterable, List, Optional, Text, Tuple, Union
6:
7:from logging import Logger
8:from blib2to3.pytree import _Convert, _NL
9:from blib2to3.pgen2 import _Path
10:from blib2to3.pgen2.grammar import Grammar
11:from blib2to3.pgen2.tokenize import TokenizerConfig
12:
13:
14:class Driver:
15:    grammar: Grammar
16:    logger: Logger
17:    convert: _Convert
18:    def __init__(
19:        self,
20:        grammar: Grammar,
21:        convert: Optional[_Convert] = ...,
22:        logger: Optional[Logger] = ...,
23:        tokenizer_config: TokenizerConfig = ...
24:    ) -> None: ...
25:    def parse_tokens(self, tokens: Iterable[Any], debug: bool = ...) -> _NL: ...
26:    def parse_stream_raw(self, stream: IO[Text], debug: bool = ...) -> _NL: ...
27:    def parse_stream(self, stream: IO[Text], debug: bool = ...) -> _NL: ...
28:    def parse_file(self, filename: _Path, encoding: Optional[Text] = ..., debug: bool = ...) -> _NL: ...
29:    def parse_string(self, text: Text, debug: bool = ...) -> _NL: ...
30:
31:def load_grammar(gt: Text = ..., gp: Optional[Text] = ..., save: bool = ..., force: bool = ..., logger: Optional[Logger] = ...) -> Grammar: ...

Calling tool: grepTool
from blib2to3\.pgen2\.tokenize import TokenizerConfig
Matched content:
51:from blib2to3.pgen2.tokenize import TokenizerConfig

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: In blib2to3/pgen2/driver.py, parse_stream_raw() and parse_string() both call tokenize.generate_tokens(..., config=self.tokenizer_config) (lines 105108, 119125). parse_stream() simply delegates to parse_stream_raw(), and parse_file() calls parse_stream(), ensuring the tokenizer config is applied across file parsing.


Todo item 7 of 19 is correct
description: 'Verify `driver.Driver.parse_stream_raw()` and `driver.Driver.parse_string()` call `tokenize.generate_tokens(..., config=self.tokenizer_config)`; ensure `parse_file()` uses `parse_stream()` and thus the config path is used.'
Validating todo item 8 of 19
description: 'Check `blib2to3/pgen2/tokenize.py` defines `TokenizerConfig` as a frozen attr dataclass with field `async_is_reserved_keyword: bool = False` and is imported where used.'
Calling tool: grepTool
TokenizerConfig
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:51:from blib2to3.pgen2.tokenize import TokenizerConfig
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:766:    tokenizer_config: TokenizerConfig = TokenizerConfig()
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:776:                TokenizerConfig(async_is_reserved_keyword=True),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:781:                TokenizerConfig(async_is_reserved_keyword=False),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:805:                    TokenizerConfig(async_is_reserved_keyword=True),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:813:                    TokenizerConfig(async_is_reserved_keyword=False),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:142:class TokenizerConfig:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py:37:        tokenizer_config=tokenize.TokenizerConfig(),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.pyi:11:from blib2to3.pgen2.tokenize import TokenizerConfig
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.pyi:23:        tokenizer_config: TokenizerConfig = ...
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi:14:class TokenizerConfig:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
120:)
121:
122:endprogs = {"'": re.compile(Single), '"': re.compile(Double),
123:            "'''": single3prog, '"""': double3prog,
124:            **{f"{prefix}'''": single3prog for prefix in _strprefixes},
125:            **{f'{prefix}"""': double3prog for prefix in _strprefixes},
126:            **{prefix: None for prefix in _strprefixes}}
127:
128:triple_quoted = (
129:    {"'''", '"""'} |
130:    {f"{prefix}'''" for prefix in _strprefixes} |
131:    {f'{prefix}"""' for prefix in _strprefixes}
132:)
133:single_quoted = (
134:    {"'", '"'} |
135:    {f"{prefix}'" for prefix in _strprefixes} |
136:    {f'{prefix}"' for prefix in _strprefixes}
137:)
138:
139:tabsize = 8
140:
141:@dataclass(frozen=True)
142:class TokenizerConfig:
143:    async_is_reserved_keyword: bool = False
144:
145:class TokenError(Exception): pass
146:
147:class StopTokenizing(Exception): pass
148:
149:def printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line): # for testing
150:    (srow, scol) = xxx_todo_changeme
151:    (erow, ecol) = xxx_todo_changeme1
152:    print("%d,%d-%d,%d:\t%s\t%s" % \
153:        (srow, scol, erow, ecol, tok_name[type], repr(token)))
154:
155:def tokenize(readline, tokeneater=printtoken):
156:    """
157:    The tokenize() function accepts two parameters: one representing the
158:    input stream, and one providing an output mechanism for tokenize().
159:
160:    The first parameter, readline, must be a callable object which provides
161:    the same interface as the readline() method of built-in file objects.
162:    Each call to the function should return one line of input as a string.
163:
164:    The second parameter, tokeneater, must also be a callable object. It is
165:    called once for each token, with five arguments, corresponding to the
166:    tuples generated by generate_tokens().
167:    """
168:    try:
169:        tokenize_loop(readline, tokeneater)
170:    except StopTokenizing:
171:        pass
172:
173:# backwards compatible interface
174:def tokenize_loop(readline, tokeneater):
175:    for token_info in generate_tokens(readline):
176:        tokeneater(*token_info)
177:
178:class Untokenizer:
179:
180:    def __init__(self):
181:        self.tokens = []
182:        self.prev_row = 1
183:        self.prev_col = 0
184:
185:    def add_whitespace(self, start):
186:        row, col = start
187:        assert row <= self.prev_row
188:        col_offset = col - self.prev_col
189:        if col_offset:
190:            self.tokens.append(" " * col_offset)
191:
192:    def untokenize(self, iterable):
193:        for t in iterable:
194:            if len(t) == 2:
195:                self.compat(t, iterable)
196:                break
197:            tok_type, token, start, end, line = t
198:            self.add_whitespace(start)
199:            self.tokens.append(token)

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
330:
331:    Round-trip invariant for limited intput:
332:        # Output text will tokenize the back to the input
333:        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
334:        newcode = untokenize(t1)
335:        readline = iter(newcode.splitlines(1)).next
336:        t2 = [tok[:2] for tokin generate_tokens(readline)]
337:        assert t1 == t2
338:    """
339:    ut = Untokenizer()
340:    return ut.untokenize(iterable)
341:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
343:    """
344:    The generate_tokens() generator requires one argument, readline, which
345:    must be a callable object which provides the same interface as the
346:    readline() method of built-in file objects. Each call to the function
347:    should return one line of input as a string.  Alternately, readline
348:    can be a callable function terminating with StopIteration:
349:        readline = open(myfile).next    # Example of alternate readline
350:
351:    The generator produces 5-tuples with these members: the token type; the
352:    token string; a 2-tuple (srow, scol) of ints specifying the row and
353:    column where the token begins in the source; a 2-tuple (erow, ecol) of
354:    ints specifying the row and column where the token ends in the source;
355:    and the line on which the token was found. The line passed is the
356:    logical line; continuation lines are included.
357:    """
358:    lnum = parenlev = continued = 0
359:    numchars = '0123456789'
360:    contstr, needcont = '', 0
361:    contline = None
362:    indents = [0]
363:
364:    # If we know we're parsing 3.7+, we can unconditionally parse `async` and
365:    # `await` as keywords.
366:    async_is_reserved_keyword = config.async_is_reserved_keyword
367:    # 'stashed' and 'async_*' are used for async/await parsing
368:    stashed = None
369:    async_def = False
370:    async_def_indent = 0
371:    async_def_nl = False
372:
373:    while 1:                                   # loop over lines in stream
374:        try:
375:            line = readline()
376:        except StopIteration:
377:            line = ''
378:        lnum = lnum + 1
379:        pos, max = 0, len(line)
380:
381:        if contstr:                            # continued string
382:            if not line:
383:                raise TokenError("EOF in multi-line string", strstart)
384:            endmatch = endprog.match(line)
385:            if endmatch:
386:                pos = end = endmatch.end(0)
387:                yield (STRING, contstr + line[:end],
388:                       strstart, (lnum, end), contline + line)
389:                contstr, needcont = '', 0
390:                contline = None
391:            elif needcont and line[-2:] != '\\\n' and line[-3:] != '\\\r\n':
392:                yield (ERRORTOKEN, contstr + line,
393:                           strstart, (lnum, len(line)), contline)
394:                contstr = ''
395:                contline = None
396:                continue
397:            else:
398:                contstr = contstr + line
399:                contline = contline + line
400:                continue
401:
402:        elif parenlev == 0 and not continued:  # new statement
403:            if not line: break
404:            column = 0
405:            while pos < max:                   # measure leading whitespace
406:                if line[pos] == ' ': column = column + 1
407:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize
408:                elif line[pos] == '\f': column = 0
409:                else: break

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
1:# Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006 Python Software Foundation.
2:# All rights reserved.
3:
4:"""Tokenization help for Python programs.
5:
6:generate_tokens(readline) is a generator that breaks a stream of
7:text into Python tokens.  It accepts a readline-like method which is called
8:repeatedly to get the next line of input (or "" for EOF).  It generates
9:5-tuples with these members:
10:
11:    the token type (see token.py)
12:    the token (a string)
13:    the starting (row, column) indices of the token (a 2-tuple of ints)
14:    the ending (row, column) indices of the token (a 2-tuple of ints)
15:    the original line (string)
16:
17:It is designed to match the working of the Python tokenizer exactly, except
18:that it produces COMMENT tokens for comments and gives type OP for all
19:operators
20:
21:Older entry points
22:    tokenize_loop(readline, tokeneater)
23:    tokenize(readline, tokeneater=printtoken)
24:are the same, except instead of generating tokens, tokeneater is a callback
25:function to which the 5 fields described above are passed as 5 arguments,
26:each time a new token is found."""
27:
28:__author__ = 'Ka-Ping Yee <ping@lfw.org>'
29:__credits__ = \
30:    'GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro'
31:
32:import re
33:from codecs import BOM_UTF8, lookup
34:from attr import dataclass
35:from blib2to3.pgen2.token import *
36:
37:from . import token
38:__all__ = [x for x in dir(token) if x[0] != '_'] + ["tokenize",
39:           "generate_tokens", "untokenize"]
40:del token
41:
42:try:
43:    bytes
44:except NameError:
45:    # Support bytes type in Python <= 2.5, so 2to3 turns itself into
46:    # valid Python 3 code.
47:    bytes = str
48:
49:def group(*choices): return '(' + '|'.join(choices) + ')'
50:def any(*choices): return group(*choices) + '*'
51:def maybe(*choices): return group(*choices) + '?'
52:def _combinations(*l):
53:    return set(
54:        x + y for x in l for y in l + ("",) if x.casefold() != y.casefold()
55:    )
56:
57:Whitespace = r'[ \f\t]*'
58:Comment = r'#[^\r\n]*'
59:Ignore = Whitespace + any(r'\\\r?\n' + Whitespace) + maybe(Comment)
60:Name = r'\w+'  # this is invalid but it's fine because Name comes after Number in all groups

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
40:from attr import dataclass, evolve, Factory
41:import click
42:import toml
43:from typed_ast import ast3, ast27
44:
45:# lib2to3 fork
46:from blib2to3.pytree import Node, Leaf, type_repr
47:from blib2to3 import pygram, pytree
48:from blib2to3.pgen2 import driver, token
49:from blib2to3.pgen2.grammar import Grammar
50:from blib2to3.pgen2.parse import ParseError
51:from blib2to3.pgen2.tokenize import TokenizerConfig
52:
53:
54:__version__ = "19.3b0"
55:DEFAULT_LINE_LENGTH = 88
56:DEFAULT_EXCLUDES = (
57:    r"/(\.eggs|\.git|\.hg|\.mypy_cache|\.nox|\.tox|\.venv|_build|buck-out|build|dist)/"
58:)
59:DEFAULT_INCLUDES = r"\.pyi?$"
60:CACHE_DIR = Path(user_cache_dir("black", version=__version__))
61:
62:
63:# types
64:FileContent = str
65:Encoding = str
66:NewLine = str
67:Depth = int
68:NodeType = int
69:LeafID = int

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
740:            current_line, line_length=mode.line_length, features=split_line_features
741:        ):
742:            dst_contents += str(line)
743:    return dst_contents
744:
745:
746:def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:
747:    """Return a tuple of (decoded_contents, encoding, newline).
748:
749:    `newline` is either CRLF or LF but `decoded_contents` is decoded with
750:    universal newlines (i.e. only contains LF).
751:    """
752:    srcbuf = io.BytesIO(src)
753:    encoding, lines = tokenize.detect_encoding(srcbuf.readline)
754:    if not lines:
755:        return "", encoding, "\n"
756:
757:    newline = "\r\n" if b"\r\n" == lines[0][-2:] else "\n"
758:    srcbuf.seek(0)
759:    with io.TextIOWrapper(srcbuf, encoding) as tiow:
760:        return tiow.read(), encoding, newline
761:
762:
763:@dataclass(frozen=True)
764:class ParserConfig:
765:    grammar: Grammar
766:    tokenizer_config: TokenizerConfig = TokenizerConfig()
767:
768:
769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:
770:    if not target_versions:
771:        # No target_version specified, so try all grammars.
772:        return [
773:            # Python 3.7+
774:            ParserConfig(
775:                pygram.python_grammar_no_print_statement_no_exec_statement,
776:                TokenizerConfig(async_is_reserved_keyword=True),
777:            ),
778:            # Python 3.0-3.6
779:            ParserConfig(
780:                pygram.python_grammar_no_print_statement_no_exec_statement,
781:                TokenizerConfig(async_is_reserved_keyword=False),
782:            ),
783:            # Python 2.7 with future print_function import
784:            ParserConfig(pygram.python_grammar_no_print_statement),
785:            # Python 2.7
786:            ParserConfig(pygram.python_grammar),
787:        ]
788:    elif all(version.is_python2() for version in target_versions):
789:        # Python 2-only code, so try Python 2 grammars.
790:        return [
791:            # Python 2.7 with future print_function import
792:            ParserConfig(pygram.python_grammar_no_print_statement),
793:            # Python 2.7
794:            ParserConfig(pygram.python_grammar),
795:        ]
796:    else:
797:        # Python 3-compatible code, so only try Python 3 grammar.
798:        configs = []
799:        # If we have to parse both, try to parse async as a keyword first
800:        if not supports_feature(target_versions, Feature.ASYNC_IS_VALID_IDENTIFIER):
801:            # Python 3.7+
802:            configs.append(
803:                ParserConfig(
804:                    pygram.python_grammar_no_print_statement_no_exec_statement,
805:                    TokenizerConfig(async_is_reserved_keyword=True),
806:                )
807:            )
808:        if not supports_feature(target_versions, Feature.ASYNC_IS_RESERVED_KEYWORD):
809:            # Python 3.0-3.6
810:            configs.append(
811:                ParserConfig(
812:                    pygram.python_grammar_no_print_statement_no_exec_statement,
813:                    TokenizerConfig(async_is_reserved_keyword=False),
814:                )
815:            )
816:        # At least one of the above branches must have been taken, because every Python
817:        # version has exactly one of the two 'ASYNC_IS_*' flags
818:        return configs
819:
820:
821:def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:
822:    """Given a string with source, return the lib2to3 Node."""
823:    if src_txt[-1:] != "\n":
824:        src_txt += "\n"
825:
826:    for parser_config in get_parser_configs(set(target_versions)):
827:        drv = driver.Driver(
828:            parser_config.grammar,
829:            pytree.convert,
830:            tokenizer_config=parser_config.tokenizer_config,
831:        )
832:        try:
833:            result = drv.parse_string(src_txt, True)
834:            break
835:
836:        except ParseError as pe:
837:            lineno, column = pe.context[1]
838:            lines = src_txt.splitlines()
839:            try:
840:                faulty_line = lines[lineno - 1]
841:            except IndexError:
842:                faulty_line = "<line number missing in source>"
843:            exc = InvalidInput(f"Cannot parse: {lineno}:{column}: {faulty_line}")
844:    else:
845:        raise exc from None
846:
847:    if isinstance(result, Leaf):
848:        result = Node(syms.file_input, [result])
849:    return result
850:
851:
852:def lib2to3_unparse(node: Node) -> str:
853:    """Given a lib2to3 node, return its string representation."""
854:    code = str(node)
855:    return code
856:
857:
858:T = TypeVar("T")
859:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(
33:        self,
34:        grammar,
35:        convert=None,
36:        logger=None,
37:        tokenizer_config=tokenize.TokenizerConfig(),
38:    ):
39:        self.grammar = grammar
40:        if logger is None:
41:            logger = logging.getLogger(__name__)
42:        self.logger = logger
43:        self.convert = convert
44:        self.tokenizer_config = tokenizer_config
45:
46:    def parse_tokens(self, tokens, debug=False):
47:        """Parse a series of tokens and return the syntax tree."""
48:        # XXX Move the prefix computation into a wrapper around tokenize.
49:        p = parse.Parser(self.grammar, self.convert)
50:        p.setup()
51:        lineno = 1
52:        column = 0
53:        indent_columns = []
54:        type = value = start = end = line_text = None
55:        prefix = ""
56:        for quintuple in tokens:
57:            type, value, start, end, line_text = quintuple
58:            if start != (lineno, column):
59:                assert (lineno, column) <= start, ((lineno, column), start)

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: TokenizerConfig is defined in blib2to3/pgen2/tokenize.py as an attrs dataclass with frozen=True and a single field async_is_reserved_keyword: bool = False, and black.py imports it via `from blib2to3.pgen2.tokenize import TokenizerConfig`. The driver uses it via tokenize.TokenizerConfig(), so it is correctly imported/accessible where used.


Todo item 8 of 19 is correct
description: 'Check `blib2to3/pgen2/tokenize.py` defines `TokenizerConfig` as a frozen attr dataclass with field `async_is_reserved_keyword: bool = False` and is imported where used.'
Validating todo item 9 of 19
description: 'Confirm `generate_tokens` signature in `tokenize.py` is updated to accept `config: TokenizerConfig = TokenizerConfig()` and that it reads `config.async_is_reserved_keyword` into a local flag.'
Calling tool: grepTool
def generate_tokens\(
Matched content:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
330:
331:    Round-trip invariant for limited intput:
332:        # Output text will tokenize the back to the input
333:        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
334:        newcode = untokenize(t1)
335:        readline = iter(newcode.splitlines(1)).next
336:        t2 = [tok[:2] for tokin generate_tokens(readline)]
337:        assert t1 == t2
338:    """
339:    ut = Untokenizer()
340:    return ut.untokenize(iterable)
341:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
343:    """
344:    The generate_tokens() generator requires one argument, readline, which
345:    must be a callable object which provides the same interface as the
346:    readline() method of built-in file objects. Each call to the function
347:    should return one line of input as a string.  Alternately, readline
348:    can be a callable function terminating with StopIteration:
349:        readline = open(myfile).next    # Example of alternate readline
350:
351:    The generator produces 5-tuples with these members: the token type; the
352:    token string; a 2-tuple (srow, scol) of ints specifying the row and
353:    column where the token begins in the source; a 2-tuple (erow, ecol) of
354:    ints specifying the row and column where the token ends in the source;
355:    and the line on which the token was found. The line passed is the
356:    logical line; continuation lines are included.
357:    """
358:    lnum = parenlev = continued = 0
359:    numchars = '0123456789'
360:    contstr, needcont = '', 0
361:    contline = None
362:    indents = [0]
363:
364:    # If we know we're parsing 3.7+, we can unconditionally parse `async` and
365:    # `await` as keywords.
366:    async_is_reserved_keyword = config.async_is_reserved_keyword
367:    # 'stashed' and 'async_*' are used for async/await parsing
368:    stashed = None
369:    async_def = False
370:    async_def_indent = 0
371:    async_def_nl = False
372:
373:    while 1:                                   # loop over lines in stream
374:        try:
375:            line = readline()
376:        except StopIteration:
377:            line = ''
378:        lnum = lnum + 1
379:        pos, max = 0, len(line)
380:
381:        if contstr:                            # continued string
382:            if not line:
383:                raise TokenError("EOF in multi-line string", strstart)
384:            endmatch = endprog.match(line)
385:            if endmatch:
386:                pos = end = endmatch.end(0)
387:                yield (STRING, contstr + line[:end],
388:                       strstart, (lnum, end), contline + line)
389:                contstr, needcont = '', 0
390:                contline = None
391:            elif needcont and line[-2:] != '\\\n' and line[-3:] != '\\\r\n':
392:                yield (ERRORTOKEN, contstr + line,
393:                           strstart, (lnum, len(line)), contline)
394:                contstr = ''
395:                contline = None
396:                continue
397:            else:
398:                contstr = contstr + line
399:                contline = contline + line
400:                continue
401:
402:        elif parenlev == 0 and not continued:  # new statement
403:            if not line: break
404:            column = 0
405:            while pos < max:                   # measure leading whitespace
406:                if line[pos] == ' ': column = column + 1
407:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize
408:                elif line[pos] == '\f': column = 0
409:                else: break

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: In blib2to3/pgen2/tokenize.py, generate_tokens is defined as def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()): (line 342) and it assigns async_is_reserved_keyword = config.async_is_reserved_keyword (lines 364-367), satisfying the requirement.


Todo item 9 of 19 is correct
description: 'Confirm `generate_tokens` signature in `tokenize.py` is updated to accept `config: TokenizerConfig = TokenizerConfig()` and that it reads `config.async_is_reserved_keyword` into a local flag.'
Validating todo item 10 of 19
description: 'Inspect tokenization logic for identifiers in `generate_tokens`: when `token in ('async','await')` it yields `ASYNC`/`AWAIT` tokens if `async_is_reserved_keyword` OR `async_def` is true; otherwise they remain NAME. Ensure existing `async def`/`async for` stashing logic remains intact and resets on dedent/newline as before.'
Calling tool: grepTool
def generate_tokens\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi:32:def generate_tokens(

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
330:
331:    Round-trip invariant for limited intput:
332:        # Output text will tokenize the back to the input
333:        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
334:        newcode = untokenize(t1)
335:        readline = iter(newcode.splitlines(1)).next
336:        t2 = [tok[:2] for tokin generate_tokens(readline)]
337:        assert t1 == t2
338:    """
339:    ut = Untokenizer()
340:    return ut.untokenize(iterable)
341:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
343:    """
344:    The generate_tokens() generator requires one argument, readline, which
345:    must be a callable object which provides the same interface as the
346:    readline() method of built-in file objects. Each call to the function
347:    should return one line of input as a string.  Alternately, readline
348:    can be a callable function terminating with StopIteration:
349:        readline = open(myfile).next    # Example of alternate readline
350:
351:    The generator produces 5-tuples with these members: the token type; the
352:    token string; a 2-tuple (srow, scol) of ints specifying the row and
353:    column where the token begins in the source; a 2-tuple (erow, ecol) of
354:    ints specifying the row and column where the token ends in the source;
355:    and the line on which the token was found. The line passed is the
356:    logical line; continuation lines are included.
357:    """
358:    lnum = parenlev = continued = 0
359:    numchars = '0123456789'
360:    contstr, needcont = '', 0
361:    contline = None
362:    indents = [0]
363:
364:    # If we know we're parsing 3.7+, we can unconditionally parse `async` and
365:    # `await` as keywords.
366:    async_is_reserved_keyword = config.async_is_reserved_keyword
367:    # 'stashed' and 'async_*' are used for async/await parsing
368:    stashed = None
369:    async_def = False
370:    async_def_indent = 0
371:    async_def_nl = False
372:
373:    while 1:                                   # loop over lines in stream
374:        try:
375:            line = readline()
376:        except StopIteration:
377:            line = ''
378:        lnum = lnum + 1
379:        pos, max = 0, len(line)
380:
381:        if contstr:                            # continued string
382:            if not line:
383:                raise TokenError("EOF in multi-line string", strstart)
384:            endmatch = endprog.match(line)
385:            if endmatch:
386:                pos = end = endmatch.end(0)
387:                yield (STRING, contstr + line[:end],
388:                       strstart, (lnum, end), contline + line)
389:                contstr, needcont = '', 0
390:                contline = None
391:            elif needcont and line[-2:] != '\\\n' and line[-3:] != '\\\r\n':
392:                yield (ERRORTOKEN, contstr + line,
393:                           strstart, (lnum, len(line)), contline)
394:                contstr = ''
395:                contline = None
396:                continue
397:            else:
398:                contstr = contstr + line
399:                contline = contline + line
400:                continue
401:
402:        elif parenlev == 0 and not continued:  # new statement
403:            if not line: break
404:            column = 0
405:            while pos < max:                   # measure leading whitespace
406:                if line[pos] == ' ': column = column + 1
407:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize
408:                elif line[pos] == '\f': column = 0
409:                else: break

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
410:                pos = pos + 1
411:            if pos == max: break
412:
413:            if stashed:
414:                yield stashed
415:                stashed = None
416:
417:            if line[pos] in '\r\n':            # skip blank lines
418:                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)
419:                continue
420:
421:            if line[pos] == '#':               # skip comments
422:                comment_token = line[pos:].rstrip('\r\n')
423:                nl_pos = pos + len(comment_token)
424:                yield (COMMENT, comment_token,
425:                        (lnum, pos), (lnum, pos + len(comment_token)), line)
426:                yield (NL, line[nl_pos:],
427:                        (lnum, nl_pos), (lnum, len(line)), line)
428:                continue
429:
430:            if column > indents[-1]:           # count indents
431:                indents.append(column)
432:                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)
433:
434:            while column < indents[-1]:        # count dedents
435:                if column not in indents:
436:                    raise IndentationError(
437:                        "unindent does not match any outer indentation level",
438:                        ("<tokenize>", lnum, pos, line))
439:                indents = indents[:-1]
440:
441:                if async_def and async_def_indent >= indents[-1]:
442:                    async_def = False
443:                    async_def_nl = False
444:                    async_def_indent = 0
445:
446:                yield (DEDENT, '', (lnum, pos), (lnum, pos), line)
447:
448:            if async_def and async_def_nl and async_def_indent >= indents[-1]:
449:                async_def = False
450:                async_def_nl = False
451:                async_def_indent = 0
452:
453:        else:                                  # continued statement
454:            if not line:
455:                raise TokenError("EOF in multi-line statement", (lnum, 0))
456:            continued = 0
457:
458:        while pos < max:
459:            pseudomatch = pseudoprog.match(line, pos)
460:            if pseudomatch:                                # scan for tokens
461:                start, end = pseudomatch.span(1)
462:                spos, epos, pos = (lnum, start), (lnum, end), end
463:                token, initial = line[start:end], line[start]
464:
465:                if initial in numchars or \
466:                   (initial == '.' and token != '.'):      # ordinary number
467:                    yield (NUMBER, token, spos, epos, line)
468:                elif initial in '\r\n':
469:                    newline = NEWLINE
470:                    if parenlev > 0:
471:                        newline = NL
472:                    elif async_def:
473:                        async_def_nl = True
474:                    if stashed:
475:                        yield stashed
476:                        stashed = None
477:                    yield (newline, token, spos, epos, line)
478:
479:                elif initial == '#':
480:                    assert not token.endswith("\n")
481:                    if stashed:
482:                        yield stashed
483:                        stashed = None
484:                    yield (COMMENT, token, spos, epos, line)
485:                elif token in triple_quoted:
486:                    endprog = endprogs[token]
487:                    endmatch = endprog.match(line, pos)
488:                    if endmatch:                           # all on one line
489:                        pos = endmatch.end(0)

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
490:                        token = line[start:pos]
491:                        if stashed:
492:                            yield stashed
493:                            stashed = None
494:                        yield (STRING, token, spos, (lnum, pos), line)
495:                    else:
496:                        strstart = (lnum, start)           # multiple lines
497:                        contstr = line[start:]
498:                        contline = line
499:                        break
500:                elif initial in single_quoted or \
501:                    token[:2] in single_quoted or \
502:                    token[:3] in single_quoted:
503:                    if token[-1] == '\n':                  # continued string
504:                        strstart = (lnum, start)
505:                        endprog = (endprogs[initial] or endprogs[token[1]] or
506:                                   endprogs[token[2]])
507:                        contstr, needcont = line[start:], 1
508:                        contline = line
509:                        break
510:                    else:                                  # ordinary string
511:                        if stashed:
512:                            yield stashed
513:                            stashed = None
514:                        yield (STRING, token, spos, epos, line)
515:                elif initial.isidentifier():               # ordinary name
516:                    if token in ('async', 'await'):
517:                        if async_is_reserved_keyword or async_def:
518:                            yield (ASYNC if token == 'async' else AWAIT,
519:                                   token, spos, epos, line)
520:                            continue
521:
522:                    tok = (NAME, token, spos, epos, line)
523:                    if token == 'async' and not stashed:
524:                        stashed = tok
525:                        continue
526:
527:                    if token in ('def', 'for'):
528:                        if (stashed
529:                                and stashed[0] == NAME
530:                                and stashed[1] == 'async'):
531:
532:                            if token == 'def':
533:                                async_def = True
534:                                async_def_indent = indents[-1]
535:
536:                            yield (ASYNC, stashed[1],
537:                                   stashed[2], stashed[3],
538:                                   stashed[4])
539:                            stashed = None
540:
541:                    if stashed:
542:                        yield stashed
543:                        stashed = None
544:
545:                    yield tok
546:                elif initial == '\\':                      # continued stmt
547:                    # This yield is new; needed for better idempotency:
548:                    if stashed:
549:                        yield stashed
550:                        stashed = None
551:                    yield (NL, token, spos, (lnum, pos), line)
552:                    continued = 1
553:                else:
554:                    if initial in '([{': parenlev = parenlev + 1
555:                    elif initial in ')]}': parenlev = parenlev - 1
556:                    if stashed:
557:                        yield stashed
558:                        stashed = None
559:                    yield (OP, token, spos, epos, line)
560:            else:
561:                yield (ERRORTOKEN, line[pos],
562:                           (lnum, pos), (lnum, pos+1), line)
563:                pos = pos + 1
564:
565:    if stashed:
566:        yield stashed
567:        stashed = None
568:
569:    for indent in indents[1:]:                 # pop remaining indent levels

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
570:        yield (DEDENT, '', (lnum, 0), (lnum, 0), '')
571:    yield (ENDMARKER, '', (lnum, 0), (lnum, 0), '')
572:
573:if __name__ == '__main__':                     # testing
574:    import sys
575:    if len(sys.argv) > 1: tokenize(open(sys.argv[1]).readline)
576:    else: tokenize(sys.stdin.readline)

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: In generate_tokens, when token is 'async'/'await' it yields ASYNC/AWAIT if config.async_is_reserved_keyword or async_def is true; otherwise it falls through to NAME. The async stashing logic (stashed, async_def, async_def_indent/nl) remains intact and resets on newline/dedent as before.


Todo item 10 of 19 is correct
description: 'Inspect tokenization logic for identifiers in `generate_tokens`: when `token in ('async','await')` it yields `ASYNC`/`AWAIT` tokens if `async_is_reserved_keyword` OR `async_def` is true; otherwise they remain NAME. Ensure existing `async def`/`async for` stashing logic remains intact and resets on dedent/newline as before.'
Validating todo item 11 of 19
description: 'Check all call sites of `generate_tokens` across the repo: updated calls pass `config` where needed (driver), and legacy calls (e.g., in `pgen.py` and `tokenize_loop`) still compile using the default parameter.'
Calling tool: grepTool
generate_tokens\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:6:generate_tokens(readline) is a generator that breaks a stream of
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:166:    tuples generated by generate_tokens().
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:175:    for token_info in generate_tokens(readline):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:333:        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:336:        t2 = [tok[:2] for tokin generate_tokens(readline)]
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:344:    The generate_tokens() generator requires one argument, readline, which
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py:107:        tokens = tokenize.generate_tokens(stream.readline, config=self.tokenizer_config)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py:121:        tokens = tokenize.generate_tokens(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/pgen.py:19:        self.generator = tokenize.generate_tokens(stream.readline)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi:32:def generate_tokens(

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/pgen.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Pgen imports
5:from . import grammar, token, tokenize
6:
7:class PgenGrammar(grammar.Grammar):
8:    pass
9:
10:class ParserGenerator(object):
11:
12:    def __init__(self, filename, stream=None):
13:        close_stream = None
14:        if stream is None:
15:            stream = open(filename)
16:            close_stream = stream.close
17:        self.filename = filename
18:        self.stream = stream
19:        self.generator = tokenize.generate_tokens(stream.readline)
20:        self.gettoken() # Initialize lookahead
21:        self.dfas, self.startsymbol = self.parse()
22:        if close_stream is not None:
23:            close_stream()
24:        self.first = {} # map from symbol name to set of tokens
25:        self.addfirstsets()
26:
27:    def make_grammar(self):
28:        c = PgenGrammar()
29:        names = list(self.dfas.keys())
30:        names.sort()
31:        names.remove(self.startsymbol)
32:        names.insert(0, self.startsymbol)
33:        for name in names:
34:            i = 256 + len(c.symbol2number)
35:            c.symbol2number[name] = i
36:            c.number2symbol[i] = name
37:        for name in names:
38:            dfa = self.dfas[name]
39:            states = []
40:            for state in dfa:
41:                arcs = []
42:                for label, next in sorted(state.arcs.items()):
43:                    arcs.append((self.make_label(c, label), dfa.index(next)))
44:                if state.isfinal:
45:                    arcs.append((0, dfa.index(state)))
46:                states.append(arcs)
47:            c.states.append(states)
48:            c.dfas[c.symbol2number[name]] = (states, self.make_first(c, name))
49:        c.start = c.symbol2number[self.startsymbol]
50:        return c
51:
52:    def make_first(self, c, name):
53:        rawfirst = self.first[name]
54:        first = {}
55:        for label in sorted(rawfirst):
56:            ilabel = self.make_label(c, label)
57:            ##assert ilabel not in first # XXX failed on <> ... !=
58:            first[ilabel] = 1
59:        return first
60:
61:    def make_label(self, c, label):
62:        # XXX Maybe this should be a method on a subclass of converter?
63:        ilabel = len(c.labels)
64:        if label[0].isalpha():
65:            # Either a symbol name or a named token
66:            if label in c.symbol2number:
67:                # A symbol name (a non-terminal)
68:                if label in c.symbol2label:
69:                    return c.symbol2label[label]
70:                else:
71:                    c.labels.append((c.symbol2number[label], None))
72:                    c.symbol2label[label] = ilabel
73:                    return ilabel
74:            else:
75:                # A named token (NAME, NUMBER, STRING)
76:                itoken = getattr(token, label, None)
77:                assert isinstance(itoken, int), label
78:                assert itoken in token.tok_name, label
79:                if itoken in c.tokens:
80:                    return c.tokens[itoken]

Calling tool: grepTool
tokenize_loop
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:22:    tokenize_loop(readline, tokeneater)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:169:        tokenize_loop(readline, tokeneater)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:174:def tokenize_loop(readline, tokeneater):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
1:# Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006 Python Software Foundation.
2:# All rights reserved.
3:
4:"""Tokenization help for Python programs.
5:
6:generate_tokens(readline) is a generator that breaks a stream of
7:text into Python tokens.  It accepts a readline-like method which is called
8:repeatedly to get the next line of input (or "" for EOF).  It generates
9:5-tuples with these members:
10:
11:    the token type (see token.py)
12:    the token (a string)
13:    the starting (row, column) indices of the token (a 2-tuple of ints)
14:    the ending (row, column) indices of the token (a 2-tuple of ints)
15:    the original line (string)
16:
17:It is designed to match the working of the Python tokenizer exactly, except
18:that it produces COMMENT tokens for comments and gives type OP for all
19:operators
20:
21:Older entry points
22:    tokenize_loop(readline, tokeneater)
23:    tokenize(readline, tokeneater=printtoken)
24:are the same, except instead of generating tokens, tokeneater is a callback
25:function to which the 5 fields described above are passed as 5 arguments,
26:each time a new token is found."""
27:
28:__author__ = 'Ka-Ping Yee <ping@lfw.org>'
29:__credits__ = \
30:    'GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro'
31:
32:import re
33:from codecs import BOM_UTF8, lookup
34:from attr import dataclass
35:from blib2to3.pgen2.token import *
36:
37:from . import token
38:__all__ = [x for x in dir(token) if x[0] != '_'] + ["tokenize",
39:           "generate_tokens", "untokenize"]
40:del token
41:
42:try:
43:    bytes
44:except NameError:
45:    # Support bytes type in Python <= 2.5, so 2to3 turns itself into
46:    # valid Python 3 code.
47:    bytes = str
48:
49:def group(*choices): return '(' + '|'.join(choices) + ')'
50:def any(*choices): return group(*choices) + '*'
51:def maybe(*choices): return group(*choices) + '?'
52:def _combinations(*l):
53:    return set(
54:        x + y for x in l for y in l + ("",) if x.casefold() != y.casefold()
55:    )
56:
57:Whitespace = r'[ \f\t]*'
58:Comment = r'#[^\r\n]*'
59:Ignore = Whitespace + any(r'\\\r?\n' + Whitespace) + maybe(Comment)
60:Name = r'\w+'  # this is invalid but it's fine because Name comes after Number in all groups
61:
62:Binnumber = r'0[bB]_?[01]+(?:_[01]+)*'
63:Hexnumber = r'0[xX]_?[\da-fA-F]+(?:_[\da-fA-F]+)*[lL]?'
64:Octnumber = r'0[oO]?_?[0-7]+(?:_[0-7]+)*[lL]?'
65:Decnumber = group(r'[1-9]\d*(?:_\d+)*[lL]?', '0[lL]?')
66:Intnumber = group(Binnumber, Hexnumber, Octnumber, Decnumber)
67:Exponent = r'[eE][-+]?\d+(?:_\d+)*'
68:Pointfloat = group(r'\d+(?:_\d+)*\.(?:\d+(?:_\d+)*)?', r'\.\d+(?:_\d+)*') + maybe(Exponent)
69:Expfloat = r'\d+(?:_\d+)*' + Exponent
70:Floatnumber = group(Pointfloat, Expfloat)
71:Imagnumber = group(r'\d+(?:_\d+)*[jJ]', Floatnumber + r'[jJ]')
72:Number = group(Imagnumber, Floatnumber, Intnumber)
73:
74:# Tail end of ' string.
75:Single = r"[^'\\]*(?:\\.[^'\\]*)*'"
76:# Tail end of " string.
77:Double = r'[^"\\]*(?:\\.[^"\\]*)*"'
78:# Tail end of ''' string.
79:Single3 = r"[^'\\]*(?:(?:\\.|'(?!''))[^'\\]*)*'''"
80:# Tail end of """ string.
81:Double3 = r'[^"\\]*(?:(?:\\.|"(?!""))[^"\\]*)*"""'
82:_litprefix = r"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?"
83:Triple = group(_litprefix + "'''", _litprefix + '"""')
84:# Single-line ' or " string.
85:String = group(_litprefix + r"'[^\n'\\]*(?:\\.[^\n'\\]*)*'",
86:               _litprefix + r'"[^\n"\\]*(?:\\.[^\n"\\]*)*"')
87:
88:# Because of leftmost-then-longest match semantics, be sure to put the
89:# longest operators first (e.g., if = came before ==, == would get
90:# recognized as two instances of =).
91:Operator = group(r"\*\*=?", r">>=?", r"<<=?", r"<>", r"!=",
92:                 r"//=?", r"->",
93:                 r"[+\-*/%&@|^=<>]=?",
94:                 r"~")
95:
96:Bracket = '[][(){}]'
97:Special = group(r'\r?\n', r'[:;.,`@]')
98:Funny = group(Operator, Bracket, Special)
99:
100:PlainToken = group(Number, Funny, String, Name)
101:Token = Ignore + PlainToken
102:
103:# First (or only) line of ' or " string.
104:ContStr = group(_litprefix + r"'[^\n'\\]*(?:\\.[^\n'\\]*)*" +
105:                group("'", r'\\\r?\n'),
106:                _litprefix + r'"[^\n"\\]*(?:\\.[^\n"\\]*)*' +
107:                group('"', r'\\\r?\n'))
108:PseudoExtras = group(r'\\\r?\n', Comment, Triple)
109:PseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)
110:
111:tokenprog = re.compile(Token, re.UNICODE)
112:pseudoprog = re.compile(PseudoToken, re.UNICODE)
113:single3prog = re.compile(Single3)
114:double3prog = re.compile(Double3)
115:
116:_strprefixes = (
117:    _combinations('r', 'R', 'f', 'F') |
118:    _combinations('r', 'R', 'b', 'B') |
119:    {'u', 'U', 'ur', 'uR', 'Ur', 'UR'}
120:)
121:
122:endprogs = {"'": re.compile(Single), '"': re.compile(Double),
123:            "'''": single3prog, '"""': double3prog,
124:            **{f"{prefix}'''": single3prog for prefix in _strprefixes},
125:            **{f'{prefix}"""': double3prog for prefix in _strprefixes},
126:            **{prefix: None for prefix in _strprefixes}}
127:
128:triple_quoted = (
129:    {"'''", '"""'} |
130:    {f"{prefix}'''" for prefix in _strprefixes} |
131:    {f'{prefix}"""' for prefix in _strprefixes}
132:)
133:single_quoted = (
134:    {"'", '"'} |
135:    {f"{prefix}'" for prefix in _strprefixes} |
136:    {f'{prefix}"' for prefix in _strprefixes}
137:)
138:
139:tabsize = 8
140:
141:@dataclass(frozen=True)
142:class TokenizerConfig:
143:    async_is_reserved_keyword: bool = False
144:
145:class TokenError(Exception): pass
146:
147:class StopTokenizing(Exception): pass
148:
149:def printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line): # for testing
150:    (srow, scol) = xxx_todo_changeme
151:    (erow, ecol) = xxx_todo_changeme1
152:    print("%d,%d-%d,%d:\t%s\t%s" % \
153:        (srow, scol, erow, ecol, tok_name[type], repr(token)))
154:
155:def tokenize(readline, tokeneater=printtoken):
156:    """
157:    The tokenize() function accepts two parameters: one representing the
158:    input stream, and one providing an output mechanism for tokenize().
159:
160:    The first parameter, readline, must be a callable object which provides
161:    the same interface as the readline() method of built-in file objects.
162:    Each call to the function should return one line of input as a string.
163:
164:    The second parameter, tokeneater, must also be a callable object. It is
165:    called once for each token, with five arguments, corresponding to the
166:    tuples generated by generate_tokens().
167:    """
168:    try:
169:        tokenize_loop(readline, tokeneater)
170:    except StopTokenizing:
171:        pass
172:
173:# backwards compatible interface
174:def tokenize_loop(readline, tokeneater):
175:    for token_info in generate_tokens(readline):
176:        tokeneater(*token_info)
177:
178:class Untokenizer:
179:
180:    def __init__(self):
181:        self.tokens = []
182:        self.prev_row = 1
183:        self.prev_col = 0
184:
185:    def add_whitespace(self, start):
186:        row, col = start
187:        assert row <= self.prev_row
188:        col_offset = col - self.prev_col
189:        if col_offset:
190:            self.tokens.append(" " * col_offset)
191:
192:    def untokenize(self, iterable):
193:        for t in iterable:
194:            if len(t) == 2:
195:                self.compat(t, iterable)
196:                break
197:            tok_type, token, start, end, line = t
198:            self.add_whitespace(start)
199:            self.tokens.append(token)
200:            self.prev_row, self.prev_col = end
201:            if tok_type in (NEWLINE, NL):
202:                self.prev_row += 1
203:                self.prev_col = 0
204:        return "".join(self.tokens)
205:
206:    def compat(self, token, iterable):
207:        startline = False
208:        indents = []
209:        toks_append = self.tokens.append
210:        toknum, tokval = token
211:        if toknum in (NAME, NUMBER):
212:            tokval += ' '
213:        if toknum in (NEWLINE, NL):
214:            startline = True
215:        for tok in iterable:
216:            toknum, tokval = tok[:2]
217:
218:            if toknum in (NAME, NUMBER, ASYNC, AWAIT):
219:                tokval += ' '
220:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
320:
321:def untokenize(iterable):
322:    """Transform tokens back into Python source code.
323:
324:    Each element returned by the iterable must be a token sequence
325:    with at least two elements, a token number and token value.  If
326:    only two tokens are passed, the resulting output is poor.
327:
328:    Round-trip invariant for full input:
329:        Untokenized source will match input source exactly
330:
331:    Round-trip invariant for limited intput:
332:        # Output text will tokenize the back to the input
333:        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
334:        newcode = untokenize(t1)
335:        readline = iter(newcode.splitlines(1)).next
336:        t2 = [tok[:2] for tokin generate_tokens(readline)]
337:        assert t1 == t2
338:    """
339:    ut = Untokenizer()
340:    return ut.untokenize(iterable)
341:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
343:    """
344:    The generate_tokens() generator requires one argument, readline, which
345:    must be a callable object which provides the same interface as the
346:    readline() method of built-in file objects. Each call to the function
347:    should return one line of input as a string.  Alternately, readline
348:    can be a callable function terminating with StopIteration:
349:        readline = open(myfile).next    # Example of alternate readline
350:
351:    The generator produces 5-tuples with these members: the token type; the
352:    token string; a 2-tuple (srow, scol) of ints specifying the row and
353:    column where the token begins in the source; a 2-tuple (erow, ecol) of
354:    ints specifying the row and column where the token ends in the source;
355:    and the line on which the token was found. The line passed is the
356:    logical line; continuation lines are included.
357:    """
358:    lnum = parenlev = continued = 0
359:    numchars = '0123456789'

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi
1:# Stubs for lib2to3.pgen2.tokenize (Python 3.6)
2:# NOTE: Only elements from __all__ are present.
3:
4:from typing import Callable, Iterable, Iterator, List, Text, Tuple
5:from attr import dataclass
6:from blib2to3.pgen2.token import *  # noqa
7:
8:
9:_Coord = Tuple[int, int]
10:_TokenEater = Callable[[int, Text, _Coord, _Coord, Text], None]
11:_TokenInfo = Tuple[int, Text, _Coord, _Coord, Text]
12:
13:@dataclass(frozen=True)
14:class TokenizerConfig:
15:    async_is_reserved_keyword: bool = False
16:
17:class TokenError(Exception): ...
18:class StopTokenizing(Exception): ...
19:
20:def tokenize(readline: Callable[[], Text], tokeneater: _TokenEater = ...) -> None: ...
21:
22:class Untokenizer:
23:    tokens: List[Text]
24:    prev_row: int
25:    prev_col: int
26:    def __init__(self) -> None: ...
27:    def add_whitespace(self, start: _Coord) -> None: ...
28:    def untokenize(self, iterable: Iterable[_TokenInfo]) -> Text: ...
29:    def compat(self, token: Tuple[int, Text], iterable: Iterable[_TokenInfo]) -> None: ...
30:
31:def untokenize(iterable: Iterable[_TokenInfo]) -> Text: ...
32:def generate_tokens(
33:    readline: Callable[[], Text]
34:) -> Iterator[_TokenInfo]: ...

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Modifications:
5:# Copyright 2006 Google, Inc. All Rights Reserved.
6:# Licensed to PSF under a Contributor Agreement.
7:
8:"""Parser driver.
9:
10:This provides a high-level interface to parse a file into a syntax tree.
11:
12:"""
13:
14:__author__ = "Guido van Rossum <guido@python.org>"
15:
16:__all__ = ["Driver", "load_grammar"]
17:
18:# Python imports
19:import codecs
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(
33:        self,
34:        grammar,
35:        convert=None,
36:        logger=None,
37:        tokenizer_config=tokenize.TokenizerConfig(),
38:    ):
39:        self.grammar = grammar
40:        if logger is None:
41:            logger = logging.getLogger(__name__)
42:        self.logger = logger
43:        self.convert = convert
44:        self.tokenizer_config = tokenizer_config
45:
46:    def parse_tokens(self, tokens, debug=False):
47:        """Parse a series of tokens and return the syntax tree."""
48:        # XXX Move the prefix computation into a wrapper around tokenize.
49:        p = parse.Parser(self.grammar, self.convert)
50:        p.setup()
51:        lineno = 1
52:        column = 0
53:        indent_columns = []
54:        type = value = start = end = line_text = None
55:        prefix = ""
56:        for quintuple in tokens:
57:            type, value, start, end, line_text = quintuple
58:            if start != (lineno, column):
59:                assert (lineno, column) <= start, ((lineno, column), start)
60:                s_lineno, s_column = start
61:                if lineno < s_lineno:
62:                    prefix += "\n" * (s_lineno - lineno)
63:                    lineno = s_lineno
64:                    column = 0
65:                if column < s_column:
66:                    prefix += line_text[column:s_column]
67:                    column = s_column
68:            if type in (tokenize.COMMENT, tokenize.NL):
69:                prefix += value
70:                lineno, column = end
71:                if value.endswith("\n"):
72:                    lineno += 1
73:                    column = 0
74:                continue
75:            if type == token.OP:
76:                type = grammar.opmap[value]
77:            if debug:
78:                self.logger.debug("%s %r (prefix=%r)",
79:                                  token.tok_name[type], value, prefix)
80:            if type == token.INDENT:
81:                indent_columns.append(len(value))
82:                _prefix = prefix + value
83:                prefix = ""
84:                value = ""
85:            elif type == token.DEDENT:
86:                _indent_col = indent_columns.pop()
87:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
88:            if p.addtoken(type, value, (prefix, start)):
89:                if debug:
90:                    self.logger.debug("Stop.")
91:                break
92:            prefix = ""
93:            if type in {token.INDENT, token.DEDENT}:
94:                prefix = _prefix
95:            lineno, column = end
96:            if value.endswith("\n"):
97:                lineno += 1
98:                column = 0
99:        else:
100:            # We never broke out -- EOF is too soon (how can this happen???)
101:            raise parse.ParseError("incomplete input",
102:                                   type, value, (prefix, start))
103:        return p.rootnode
104:
105:    def parse_stream_raw(self, stream, debug=False):
106:        """Parse a stream and return the syntax tree."""
107:        tokens = tokenize.generate_tokens(stream.readline, config=self.tokenizer_config)
108:        return self.parse_tokens(tokens, debug)
109:
110:    def parse_stream(self, stream, debug=False):
111:        """Parse a stream and return the syntax tree."""
112:        return self.parse_stream_raw(stream, debug)
113:
114:    def parse_file(self, filename, encoding=None, debug=False):
115:        """Parse a file and return the syntax tree."""
116:        with io.open(filename, "r", encoding=encoding) as stream:
117:            return self.parse_stream(stream, debug)
118:
119:    def parse_string(self, text, debug=False):
120:        """Parse a string and return the syntax tree."""
121:        tokens = tokenize.generate_tokens(
122:            io.StringIO(text).readline,
123:            config=self.tokenizer_config,
124:        )
125:        return self.parse_tokens(tokens, debug)
126:
127:    def _partially_consume_prefix(self, prefix, column):
128:        lines = []
129:        current_line = ""
130:        current_column = 0
131:        wait_for_nl = False
132:        for char in prefix:
133:            current_line += char
134:            if wait_for_nl:
135:                if char == '\n':
136:                    if current_line.strip() and current_column < column:
137:                        res = ''.join(lines)
138:                        return res, prefix[len(res):]
139:
140:                    lines.append(current_line)
141:                    current_line = ""
142:                    current_column = 0
143:                    wait_for_nl = False
144:            elif char in ' \t':
145:                current_column += 1
146:            elif char == '\n':
147:                # unexpected empty line
148:                current_column = 0
149:            else:
150:                # indent is finished
151:                wait_for_nl = True
152:        return ''.join(lines), current_line
153:
154:
155:def _generate_pickle_name(gt, cache_dir=None):
156:    head, tail = os.path.splitext(gt)
157:    if tail == ".txt":
158:        tail = ""
159:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
160:    if cache_dir:

Calling tool: grepTool
get_parser_configs\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:826:    for parser_config in get_parser_configs(set(target_versions)):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
740:            current_line, line_length=mode.line_length, features=split_line_features
741:        ):
742:            dst_contents += str(line)
743:    return dst_contents
744:
745:
746:def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:
747:    """Return a tuple of (decoded_contents, encoding, newline).
748:
749:    `newline` is either CRLF or LF but `decoded_contents` is decoded with
750:    universal newlines (i.e. only contains LF).
751:    """
752:    srcbuf = io.BytesIO(src)
753:    encoding, lines = tokenize.detect_encoding(srcbuf.readline)
754:    if not lines:
755:        return "", encoding, "\n"
756:
757:    newline = "\r\n" if b"\r\n" == lines[0][-2:] else "\n"
758:    srcbuf.seek(0)
759:    with io.TextIOWrapper(srcbuf, encoding) as tiow:
760:        return tiow.read(), encoding, newline
761:
762:
763:@dataclass(frozen=True)
764:class ParserConfig:
765:    grammar: Grammar
766:    tokenizer_config: TokenizerConfig = TokenizerConfig()
767:
768:
769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:
770:    if not target_versions:
771:        # No target_version specified, so try all grammars.
772:        return [
773:            # Python 3.7+
774:            ParserConfig(
775:                pygram.python_grammar_no_print_statement_no_exec_statement,
776:                TokenizerConfig(async_is_reserved_keyword=True),
777:            ),
778:            # Python 3.0-3.6
779:            ParserConfig(
780:                pygram.python_grammar_no_print_statement_no_exec_statement,
781:                TokenizerConfig(async_is_reserved_keyword=False),
782:            ),
783:            # Python 2.7 with future print_function import
784:            ParserConfig(pygram.python_grammar_no_print_statement),
785:            # Python 2.7
786:            ParserConfig(pygram.python_grammar),
787:        ]
788:    elif all(version.is_python2() for version in target_versions):
789:        # Python 2-only code, so try Python 2 grammars.
790:        return [
791:            # Python 2.7 with future print_function import
792:            ParserConfig(pygram.python_grammar_no_print_statement),
793:            # Python 2.7
794:            ParserConfig(pygram.python_grammar),
795:        ]
796:    else:
797:        # Python 3-compatible code, so only try Python 3 grammar.
798:        configs = []
799:        # If we have to parse both, try to parse async as a keyword first
800:        if not supports_feature(target_versions, Feature.ASYNC_IS_VALID_IDENTIFIER):
801:            # Python 3.7+
802:            configs.append(
803:                ParserConfig(
804:                    pygram.python_grammar_no_print_statement_no_exec_statement,
805:                    TokenizerConfig(async_is_reserved_keyword=True),
806:                )
807:            )
808:        if not supports_feature(target_versions, Feature.ASYNC_IS_RESERVED_KEYWORD):
809:            # Python 3.0-3.6
810:            configs.append(
811:                ParserConfig(
812:                    pygram.python_grammar_no_print_statement_no_exec_statement,
813:                    TokenizerConfig(async_is_reserved_keyword=False),
814:                )
815:            )
816:        # At least one of the above branches must have been taken, because every Python
817:        # version has exactly one of the two 'ASYNC_IS_*' flags
818:        return configs
819:
820:
821:def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:
822:    """Given a string with source, return the lib2to3 Node."""
823:    if src_txt[-1:] != "\n":
824:        src_txt += "\n"
825:
826:    for parser_config in get_parser_configs(set(target_versions)):
827:        drv = driver.Driver(
828:            parser_config.grammar,
829:            pytree.convert,
830:            tokenizer_config=parser_config.tokenizer_config,
831:        )
832:        try:
833:            result = drv.parse_string(src_txt, True)
834:            break
835:
836:        except ParseError as pe:
837:            lineno, column = pe.context[1]
838:            lines = src_txt.splitlines()
839:            try:
840:                faulty_line = lines[lineno - 1]
841:            except IndexError:
842:                faulty_line = "<line number missing in source>"
843:            exc = InvalidInput(f"Cannot parse: {lineno}:{column}: {faulty_line}")
844:    else:
845:        raise exc from None
846:
847:    if isinstance(result, Leaf):
848:        result = Node(syms.file_input, [result])
849:    return result
850:
851:
852:def lib2to3_unparse(node: Node) -> str:
853:    """Given a lib2to3 node, return its string representation."""
854:    code = str(node)
855:    return code
856:
857:
858:T = TypeVar("T")
859:

Calling tool: grepTool
Driver\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:827:        drv = driver.Driver(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py:30:class Driver(object):

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: Searched for all generate_tokens call sites. Driver now passes tokenizer_config in parse_stream_raw and parse_string. Legacy calls in pgen.py and tokenize_loop still call generate_tokens(readline) and work due to the new default TokenizerConfig() parameter. No other call sites found.


Todo item 11 of 19 is correct
description: 'Check all call sites of `generate_tokens` across the repo: updated calls pass `config` where needed (driver), and legacy calls (e.g., in `pgen.py` and `tokenize_loop`) still compile using the default parameter.'
Validating todo item 12 of 19
description: 'Verify `blib2to3/pgen2/tokenize.pyi` stub reflects the implementation changes: it defines `TokenizerConfig`, but its `generate_tokens` currently lacks the optional `config` parameter; flag this mismatch and ensure the stub's signature is updated to accept `config: TokenizerConfig = ...` to match the implementation.'
Calling tool: globTool
**/blib2to3/pgen2/tokenize.pyi
Matched files:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi

Calling tool: grepTool
TokenizerConfig|generate_tokens\(
Matched content:
14:class TokenizerConfig:
32:def generate_tokens(

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi
1:# Stubs for lib2to3.pgen2.tokenize (Python 3.6)
2:# NOTE: Only elements from __all__ are present.
3:
4:from typing import Callable, Iterable, Iterator, List, Text, Tuple
5:from attr import dataclass
6:from blib2to3.pgen2.token import *  # noqa
7:
8:
9:_Coord = Tuple[int, int]
10:_TokenEater = Callable[[int, Text, _Coord, _Coord, Text], None]
11:_TokenInfo = Tuple[int, Text, _Coord, _Coord, Text]
12:
13:@dataclass(frozen=True)
14:class TokenizerConfig:
15:    async_is_reserved_keyword: bool = False
16:
17:class TokenError(Exception): ...
18:class StopTokenizing(Exception): ...
19:
20:def tokenize(readline: Callable[[], Text], tokeneater: _TokenEater = ...) -> None: ...
21:
22:class Untokenizer:
23:    tokens: List[Text]
24:    prev_row: int
25:    prev_col: int
26:    def __init__(self) -> None: ...
27:    def add_whitespace(self, start: _Coord) -> None: ...
28:    def untokenize(self, iterable: Iterable[_TokenInfo]) -> Text: ...
29:    def compat(self, token: Tuple[int, Text], iterable: Iterable[_TokenInfo]) -> None: ...
30:
31:def untokenize(iterable: Iterable[_TokenInfo]) -> Text: ...
32:def generate_tokens(
33:    readline: Callable[[], Text]
34:) -> Iterator[_TokenInfo]: ...

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as INCORRECT
Reason: In blib2to3/pgen2/tokenize.pyi, TokenizerConfig is defined, but generate_tokens only accepts (readline: Callable[[], Text]) and lacks the optional config parameter. The implementation defines generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()), so the stub signature must be updated to accept config: TokenizerConfig = ... to match.


Validating todo item 1 of 19
description: 'Verify Feature enum additions: `ASYNC_IS_VALID_IDENTIFIER` and `ASYNC_IS_RESERVED_KEYWORD` exist and are only used to control async/await tokenization.'

Calling tool: grepTool
ASYNC_IS_VALID_IDENTIFIER
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:142:    ASYNC_IS_VALID_IDENTIFIER = 6
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:147:    TargetVersion.PY27: {Feature.ASYNC_IS_VALID_IDENTIFIER},
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:148:    TargetVersion.PY33: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:149:    TargetVersion.PY34: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:153:        Feature.ASYNC_IS_VALID_IDENTIFIER,
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:161:        Feature.ASYNC_IS_VALID_IDENTIFIER,
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:800:        if not supports_feature(target_versions, Feature.ASYNC_IS_VALID_IDENTIFIER):

Calling tool: grepTool
ASYNC_IS_RESERVED_KEYWORD
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:143:    ASYNC_IS_RESERVED_KEYWORD = 7
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:169:        Feature.ASYNC_IS_RESERVED_KEYWORD,
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:177:        Feature.ASYNC_IS_RESERVED_KEYWORD,
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:808:        if not supports_feature(target_versions, Feature.ASYNC_IS_RESERVED_KEYWORD):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
120:    PY34 = 4
121:    PY35 = 5
122:    PY36 = 6
123:    PY37 = 7
124:    PY38 = 8
125:
126:    def is_python2(self) -> bool:
127:        return self is TargetVersion.PY27
128:
129:
130:PY36_VERSIONS = {TargetVersion.PY36, TargetVersion.PY37, TargetVersion.PY38}
131:
132:
133:class Feature(Enum):
134:    # All string literals are unicode
135:    UNICODE_LITERALS = 1
136:    F_STRINGS = 2
137:    NUMERIC_UNDERSCORES = 3
138:    TRAILING_COMMA_IN_CALL = 4
139:    TRAILING_COMMA_IN_DEF = 5
140:    # The following two feature-flags are mutually exclusive, and exactly one should be
141:    # set for every version of python.
142:    ASYNC_IS_VALID_IDENTIFIER = 6
143:    ASYNC_IS_RESERVED_KEYWORD = 7
144:
145:
146:VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {
147:    TargetVersion.PY27: {Feature.ASYNC_IS_VALID_IDENTIFIER},
148:    TargetVersion.PY33: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
149:    TargetVersion.PY34: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
150:    TargetVersion.PY35: {
151:        Feature.UNICODE_LITERALS,
152:        Feature.TRAILING_COMMA_IN_CALL,
153:        Feature.ASYNC_IS_VALID_IDENTIFIER,
154:    },
155:    TargetVersion.PY36: {
156:        Feature.UNICODE_LITERALS,
157:        Feature.F_STRINGS,
158:        Feature.NUMERIC_UNDERSCORES,
159:        Feature.TRAILING_COMMA_IN_CALL,
160:        Feature.TRAILING_COMMA_IN_DEF,
161:        Feature.ASYNC_IS_VALID_IDENTIFIER,
162:    },
163:    TargetVersion.PY37: {
164:        Feature.UNICODE_LITERALS,
165:        Feature.F_STRINGS,
166:        Feature.NUMERIC_UNDERSCORES,
167:        Feature.TRAILING_COMMA_IN_CALL,
168:        Feature.TRAILING_COMMA_IN_DEF,
169:        Feature.ASYNC_IS_RESERVED_KEYWORD,
170:    },
171:    TargetVersion.PY38: {
172:        Feature.UNICODE_LITERALS,
173:        Feature.F_STRINGS,
174:        Feature.NUMERIC_UNDERSCORES,
175:        Feature.TRAILING_COMMA_IN_CALL,
176:        Feature.TRAILING_COMMA_IN_DEF,
177:        Feature.ASYNC_IS_RESERVED_KEYWORD,
178:    },
179:}
180:
181:
182:@dataclass
183:class FileMode:
184:    target_versions: Set[TargetVersion] = Factory(set)
185:    line_length: int = DEFAULT_LINE_LENGTH
186:    string_normalization: bool = True
187:    is_pyi: bool = False
188:
189:    def get_cache_key(self) -> str:
190:        if self.target_versions:
191:            version_str = ",".join(
192:                str(version.value)
193:                for version in sorted(self.target_versions, key=lambda v: v.value)
194:            )
195:        else:
196:            version_str = "-"
197:        parts = [
198:            version_str,
199:            str(self.line_length),

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
732:    }
733:    for current_line in lines.visit(src_node):
734:        for _ in range(after):
735:            dst_contents += str(empty_line)
736:        before, after = elt.maybe_empty_lines(current_line)
737:        for _ in range(before):
738:            dst_contents += str(empty_line)
739:        for line in split_line(
740:            current_line, line_length=mode.line_length, features=split_line_features
741:        ):
742:            dst_contents += str(line)
743:    return dst_contents
744:
745:
746:def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:
747:    """Return a tuple of (decoded_contents, encoding, newline).
748:
749:    `newline` is either CRLF or LF but `decoded_contents` is decoded with
750:    universal newlines (i.e. only contains LF).
751:    """
752:    srcbuf = io.BytesIO(src)
753:    encoding, lines = tokenize.detect_encoding(srcbuf.readline)
754:    if not lines:
755:        return "", encoding, "\n"
756:
757:    newline = "\r\n" if b"\r\n" == lines[0][-2:] else "\n"
758:    srcbuf.seek(0)
759:    with io.TextIOWrapper(srcbuf, encoding) as tiow:
760:        return tiow.read(), encoding, newline
761:
762:
763:@dataclass(frozen=True)
764:class ParserConfig:
765:    grammar: Grammar
766:    tokenizer_config: TokenizerConfig = TokenizerConfig()
767:
768:
769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:
770:    if not target_versions:
771:        # No target_version specified, so try all grammars.
772:        return [
773:            # Python 3.7+
774:            ParserConfig(
775:                pygram.python_grammar_no_print_statement_no_exec_statement,
776:                TokenizerConfig(async_is_reserved_keyword=True),
777:            ),
778:            # Python 3.0-3.6
779:            ParserConfig(
780:                pygram.python_grammar_no_print_statement_no_exec_statement,
781:                TokenizerConfig(async_is_reserved_keyword=False),
782:            ),
783:            # Python 2.7 with future print_function import
784:            ParserConfig(pygram.python_grammar_no_print_statement),
785:            # Python 2.7
786:            ParserConfig(pygram.python_grammar),
787:        ]
788:    elif all(version.is_python2() for version in target_versions):
789:        # Python 2-only code, so try Python 2 grammars.
790:        return [
791:            # Python 2.7 with future print_function import
792:            ParserConfig(pygram.python_grammar_no_print_statement),
793:            # Python 2.7
794:            ParserConfig(pygram.python_grammar),
795:        ]
796:    else:
797:        # Python 3-compatible code, so only try Python 3 grammar.
798:        configs = []
799:        # If we have to parse both, try to parse async as a keyword first
800:        if not supports_feature(target_versions, Feature.ASYNC_IS_VALID_IDENTIFIER):
801:            # Python 3.7+
802:            configs.append(
803:                ParserConfig(
804:                    pygram.python_grammar_no_print_statement_no_exec_statement,
805:                    TokenizerConfig(async_is_reserved_keyword=True),
806:                )
807:            )
808:        if not supports_feature(target_versions, Feature.ASYNC_IS_RESERVED_KEYWORD):
809:            # Python 3.0-3.6
810:            configs.append(
811:                ParserConfig(
812:                    pygram.python_grammar_no_print_statement_no_exec_statement,
813:                    TokenizerConfig(async_is_reserved_keyword=False),
814:                )
815:            )
816:        # At least one of the above branches must have been taken, because every Python
817:        # version has exactly one of the two 'ASYNC_IS_*' flags
818:        return configs
819:
820:
821:def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:
822:    """Given a string with source, return the lib2to3 Node."""
823:    if src_txt[-1:] != "\n":
824:        src_txt += "\n"
825:
826:    for parser_config in get_parser_configs(set(target_versions)):
827:        drv = driver.Driver(
828:            parser_config.grammar,
829:            pytree.convert,
830:            tokenizer_config=parser_config.tokenizer_config,
831:        )
832:        try:
833:            result = drv.parse_string(src_txt, True)
834:            break
835:
836:        except ParseError as pe:
837:            lineno, column = pe.context[1]
838:            lines = src_txt.splitlines()
839:            try:
840:                faulty_line = lines[lineno - 1]
841:            except IndexError:
842:                faulty_line = "<line number missing in source>"
843:            exc = InvalidInput(f"Cannot parse: {lineno}:{column}: {faulty_line}")
844:    else:
845:        raise exc from None
846:
847:    if isinstance(result, Leaf):
848:        result = Node(syms.file_input, [result])
849:    return result
850:
851:

Calling tool: grepTool
def supports_feature\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:206:def supports_feature(target_versions: Set[TargetVersion], feature: Feature) -> bool:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
200:            str(int(self.string_normalization)),
201:            str(int(self.is_pyi)),
202:        ]
203:        return ".".join(parts)
204:
205:
206:def supports_feature(target_versions: Set[TargetVersion], feature: Feature) -> bool:
207:    return all(feature in VERSION_TO_FEATURES[version] for version in target_versions)
208:
209:
210:def read_pyproject_toml(
211:    ctx: click.Context, param: click.Parameter, value: Union[str, int, bool, None]
212:) -> Optional[str]:
213:    """Inject Black configuration from "pyproject.toml" into defaults in `ctx`.
214:
215:    Returns the path to a successfully found and read configuration file, None
216:    otherwise.
217:    """
218:    assert not isinstance(value, (int, bool)), "Invalid parameter type passed"
219:    if not value:
220:        root = find_project_root(ctx.params.get("src", ()))
221:        path = root / "pyproject.toml"
222:        if path.is_file():
223:            value = str(path)
224:        else:
225:            return None
226:
227:    try:
228:        pyproject_toml = toml.load(value)
229:        config = pyproject_toml.get("tool", {}).get("black", {})
230:    except (toml.TomlDecodeError, OSError) as e:
231:        raise click.FileError(
232:            filename=value, hint=f"Error reading configuration file: {e}"
233:        )
234:
235:    if not config:
236:        return None
237:
238:    if ctx.default_map is None:
239:        ctx.default_map = {}
240:    ctx.default_map.update(  # type: ignore  # bad types in .pyi
241:        {k.replace("--", "").replace("-", "_"): v for k, v in config.items()}
242:    )
243:    return value
244:
245:
246:@click.command(context_settings=dict(help_option_names=["-h", "--help"]))
247:@click.option("-c", "--code", type=str, help="Format the code passed in as a string.")
248:@click.option(
249:    "-l",
250:    "--line-length",
251:    type=int,
252:    default=DEFAULT_LINE_LENGTH,
253:    help="How many characters per line to allow.",
254:    show_default=True,
255:)
256:@click.option(
257:    "-t",
258:    "--target-version",
259:    type=click.Choice([v.name.lower() for v in TargetVersion]),

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
320:
321:def untokenize(iterable):
322:    """Transform tokens back into Python source code.
323:
324:    Each element returned by the iterable must be a token sequence
325:    with at least two elements, a token number and token value.  If
326:    only two tokens are passed, the resulting output is poor.
327:
328:    Round-trip invariant for full input:
329:        Untokenized source will match input source exactly
330:
331:    Round-trip invariant for limited intput:
332:        # Output text will tokenize the back to the input
333:        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
334:        newcode = untokenize(t1)
335:        readline = iter(newcode.splitlines(1)).next
336:        t2 = [tok[:2] for tokin generate_tokens(readline)]
337:        assert t1 == t2
338:    """
339:    ut = Untokenizer()
340:    return ut.untokenize(iterable)
341:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
343:    """
344:    The generate_tokens() generator requires one argument, readline, which
345:    must be a callable object which provides the same interface as the
346:    readline() method of built-in file objects. Each call to the function
347:    should return one line of input as a string.  Alternately, readline
348:    can be a callable function terminating with StopIteration:
349:        readline = open(myfile).next    # Example of alternate readline
350:
351:    The generator produces 5-tuples with these members: the token type; the
352:    token string; a 2-tuple (srow, scol) of ints specifying the row and
353:    column where the token begins in the source; a 2-tuple (erow, ecol) of
354:    ints specifying the row and column where the token ends in the source;
355:    and the line on which the token was found. The line passed is the
356:    logical line; continuation lines are included.
357:    """
358:    lnum = parenlev = continued = 0
359:    numchars = '0123456789'
360:    contstr, needcont = '', 0
361:    contline = None
362:    indents = [0]
363:
364:    # If we know we're parsing 3.7+, we can unconditionally parse `async` and
365:    # `await` as keywords.
366:    async_is_reserved_keyword = config.async_is_reserved_keyword
367:    # 'stashed' and 'async_*' are used for async/await parsing
368:    stashed = None
369:    async_def = False
370:    async_def_indent = 0
371:    async_def_nl = False
372:
373:    while 1:                                   # loop over lines in stream
374:        try:
375:            line = readline()
376:        except StopIteration:
377:            line = ''
378:        lnum = lnum + 1
379:        pos, max = 0, len(line)
380:
381:        if contstr:                            # continued string
382:            if not line:
383:                raise TokenError("EOF in multi-line string", strstart)
384:            endmatch = endprog.match(line)
385:            if endmatch:
386:                pos = end = endmatch.end(0)
387:                yield (STRING, contstr + line[:end],
388:                       strstart, (lnum, end), contline + line)
389:                contstr, needcont = '', 0
390:                contline = None
391:            elif needcont and line[-2:] != '\\\n' and line[-3:] != '\\\r\n':
392:                yield (ERRORTOKEN, contstr + line,
393:                           strstart, (lnum, len(line)), contline)
394:                contstr = ''
395:                contline = None
396:                continue
397:            else:
398:                contstr = contstr + line
399:                contline = contline + line
400:                continue
401:
402:        elif parenlev == 0 and not continued:  # new statement
403:            if not line: break
404:            column = 0
405:            while pos < max:                   # measure leading whitespace
406:                if line[pos] == ' ': column = column + 1
407:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize
408:                elif line[pos] == '\f': column = 0
409:                else: break
410:                pos = pos + 1
411:            if pos == max: break
412:
413:            if stashed:
414:                yield stashed
415:                stashed = None
416:
417:            if line[pos] in '\r\n':            # skip blank lines
418:                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)
419:                continue
420:
421:            if line[pos] == '#':               # skip comments
422:                comment_token = line[pos:].rstrip('\r\n')
423:                nl_pos = pos + len(comment_token)
424:                yield (COMMENT, comment_token,
425:                        (lnum, pos), (lnum, pos + len(comment_token)), line)
426:                yield (NL, line[nl_pos:],
427:                        (lnum, nl_pos), (lnum, len(line)), line)
428:                continue
429:
430:            if column > indents[-1]:           # count indents
431:                indents.append(column)
432:                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)
433:
434:            while column < indents[-1]:        # count dedents
435:                if column not in indents:
436:                    raise IndentationError(
437:                        "unindent does not match any outer indentation level",
438:                        ("<tokenize>", lnum, pos, line))
439:                indents = indents[:-1]
440:
441:                if async_def and async_def_indent >= indents[-1]:
442:                    async_def = False
443:                    async_def_nl = False
444:                    async_def_indent = 0
445:
446:                yield (DEDENT, '', (lnum, pos), (lnum, pos), line)
447:
448:            if async_def and async_def_nl and async_def_indent >= indents[-1]:
449:                async_def = False
450:                async_def_nl = False
451:                async_def_indent = 0
452:
453:        else:                                  # continued statement
454:            if not line:
455:                raise TokenError("EOF in multi-line statement", (lnum, 0))
456:            continued = 0
457:
458:        while pos < max:
459:            pseudomatch = pseudoprog.match(line, pos)
460:            if pseudomatch:                                # scan for tokens
461:                start, end = pseudomatch.span(1)
462:                spos, epos, pos = (lnum, start), (lnum, end), end
463:                token, initial = line[start:end], line[start]
464:
465:                if initial in numchars or \
466:                   (initial == '.' and token != '.'):      # ordinary number
467:                    yield (NUMBER, token, spos, epos, line)
468:                elif initial in '\r\n':
469:                    newline = NEWLINE
470:                    if parenlev > 0:
471:                        newline = NL
472:                    elif async_def:
473:                        async_def_nl = True
474:                    if stashed:
475:                        yield stashed
476:                        stashed = None
477:                    yield (newline, token, spos, epos, line)
478:
479:                elif initial == '#':
480:                    assert not token.endswith("\n")
481:                    if stashed:
482:                        yield stashed
483:                        stashed = None
484:                    yield (COMMENT, token, spos, epos, line)
485:                elif token in triple_quoted:
486:                    endprog = endprogs[token]
487:                    endmatch = endprog.match(line, pos)
488:                    if endmatch:                           # all on one line
489:                        pos = endmatch.end(0)
490:                        token = line[start:pos]
491:                        if stashed:
492:                            yield stashed
493:                            stashed = None
494:                        yield (STRING, token, spos, (lnum, pos), line)
495:                    else:
496:                        strstart = (lnum, start)           # multiple lines
497:                        contstr = line[start:]
498:                        contline = line
499:                        break
500:                elif initial in single_quoted or \
501:                    token[:2] in single_quoted or \
502:                    token[:3] in single_quoted:
503:                    if token[-1] == '\n':                  # continued string
504:                        strstart = (lnum, start)
505:                        endprog = (endprogs[initial] or endprogs[token[1]] or
506:                                   endprogs[token[2]])
507:                        contstr, needcont = line[start:], 1
508:                        contline = line
509:                        break
510:                    else:                                  # ordinary string
511:                        if stashed:
512:                            yield stashed
513:                            stashed = None
514:                        yield (STRING, token, spos, epos, line)
515:                elif initial.isidentifier():               # ordinary name
516:                    if token in ('async', 'await'):
517:                        if async_is_reserved_keyword or async_def:
518:                            yield (ASYNC if token == 'async' else AWAIT,
519:                                   token, spos, epos, line)
520:                            continue
521:
522:                    tok = (NAME, token, spos, epos, line)
523:                    if token == 'async' and not stashed:
524:                        stashed = tok
525:                        continue
526:
527:                    if token in ('def', 'for'):
528:                        if (stashed
529:                                and stashed[0] == NAME
530:                                and stashed[1] == 'async'):
531:
532:                            if token == 'def':
533:                                async_def = True
534:                                async_def_indent = indents[-1]
535:
536:                            yield (ASYNC, stashed[1],
537:                                   stashed[2], stashed[3],
538:                                   stashed[4])
539:                            stashed = None

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
540:
541:                    if stashed:
542:                        yield stashed
543:                        stashed = None
544:
545:                    yield tok
546:                elif initial == '\\':                      # continued stmt
547:                    # This yield is new; needed for better idempotency:
548:                    if stashed:
549:                        yield stashed
550:                        stashed = None
551:                    yield (NL, token, spos, (lnum, pos), line)
552:                    continued = 1
553:                else:
554:                    if initial in '([{': parenlev = parenlev + 1
555:                    elif initial in ')]}': parenlev = parenlev - 1
556:                    if stashed:
557:                        yield stashed
558:                        stashed = None
559:                    yield (OP, token, spos, epos, line)
560:            else:
561:                yield (ERRORTOKEN, line[pos],
562:                           (lnum, pos), (lnum, pos+1), line)
563:                pos = pos + 1
564:
565:    if stashed:
566:        yield stashed
567:        stashed = None
568:
569:    for indent in indents[1:]:                 # pop remaining indent levels
570:        yield (DEDENT, '', (lnum, 0), (lnum, 0), '')
571:    yield (ENDMARKER, '', (lnum, 0), (lnum, 0), '')
572:
573:if __name__ == '__main__':                     # testing
574:    import sys
575:    if len(sys.argv) > 1: tokenize(open(sys.argv[1]).readline)
576:    else: tokenize(sys.stdin.readline)

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
40:from attr import dataclass, evolve, Factory
41:import click
42:import toml
43:from typed_ast import ast3, ast27
44:
45:# lib2to3 fork
46:from blib2to3.pytree import Node, Leaf, type_repr
47:from blib2to3 import pygram, pytree
48:from blib2to3.pgen2 import driver, token
49:from blib2to3.pgen2.grammar import Grammar
50:from blib2to3.pgen2.parse import ParseError
51:from blib2to3.pgen2.tokenize import TokenizerConfig
52:
53:
54:__version__ = "19.3b0"
55:DEFAULT_LINE_LENGTH = 88
56:DEFAULT_EXCLUDES = (
57:    r"/(\.eggs|\.git|\.hg|\.mypy_cache|\.nox|\.tox|\.venv|_build|buck-out|build|dist)/"
58:)
59:DEFAULT_INCLUDES = r"\.pyi?$"
60:CACHE_DIR = Path(user_cache_dir("black", version=__version__))
61:
62:
63:# types
64:FileContent = str
65:Encoding = str
66:NewLine = str
67:Depth = int
68:NodeType = int
69:LeafID = int
70:Priority = int
71:Index = int
72:LN = Union[Leaf, Node]
73:SplitFunc = Callable[["Line", Collection["Feature"]], Iterator["Line"]]
74:Timestamp = float
75:FileSize = int
76:CacheInfo = Tuple[Timestamp, FileSize]
77:Cache = Dict[Path, CacheInfo]
78:out = partial(click.secho, bold=True, err=True)
79:err = partial(click.secho, fg="red", err=True)

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: The Feature enum includes ASYNC_IS_VALID_IDENTIFIER and ASYNC_IS_RESERVED_KEYWORD, mapped per version in VERSION_TO_FEATURES. They are only referenced in get_parser_configs to choose TokenizerConfig(async_is_reserved_keyword True/False) for tokenization, and not used elsewhere. TokenizerConfig is passed to the tokenizer, affecting only async/await tokenization.

Todo item 1 of 19 is correct
description: 'Verify Feature enum additions: `ASYNC_IS_VALID_IDENTIFIER` and `ASYNC_IS_RESERVED_KEYWORD` exist and are only used to control async/await tokenization.'

Validating todo item 2 of 19
description: 'Check VERSION_TO_FEATURES mapping ensures mutual exclusivity: for each TargetVersion (PY27, PY33, PY34, PY35, PY36) include ASYNC_IS_VALID_IDENTIFIER, and for (PY37, PY38) include ASYNC_IS_RESERVED_KEYWORD; confirm exactly one of the two flags is present per version.'

Calling tool: globTool
**/black.py
Matched files:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py

Calling tool: grepTool
VERSION_TO_FEATURES
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:146:VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:207:    return all(feature in VERSION_TO_FEATURES[version] for version in target_versions)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:3187:        version for version in TargetVersion if features <= VERSION_TO_FEATURES[version]

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
120:    PY34 = 4
121:    PY35 = 5
122:    PY36 = 6
123:    PY37 = 7
124:    PY38 = 8
125:
126:    def is_python2(self) -> bool:
127:        return self is TargetVersion.PY27
128:
129:
130:PY36_VERSIONS = {TargetVersion.PY36, TargetVersion.PY37, TargetVersion.PY38}
131:
132:
133:class Feature(Enum):
134:    # All string literals are unicode
135:    UNICODE_LITERALS = 1
136:    F_STRINGS = 2
137:    NUMERIC_UNDERSCORES = 3
138:    TRAILING_COMMA_IN_CALL = 4
139:    TRAILING_COMMA_IN_DEF = 5
140:    # The following two feature-flags are mutually exclusive, and exactly one should be
141:    # set for every version of python.
142:    ASYNC_IS_VALID_IDENTIFIER = 6
143:    ASYNC_IS_RESERVED_KEYWORD = 7
144:
145:
146:VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {
147:    TargetVersion.PY27: {Feature.ASYNC_IS_VALID_IDENTIFIER},
148:    TargetVersion.PY33: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
149:    TargetVersion.PY34: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
150:    TargetVersion.PY35: {
151:        Feature.UNICODE_LITERALS,
152:        Feature.TRAILING_COMMA_IN_CALL,
153:        Feature.ASYNC_IS_VALID_IDENTIFIER,
154:    },
155:    TargetVersion.PY36: {
156:        Feature.UNICODE_LITERALS,
157:        Feature.F_STRINGS,
158:        Feature.NUMERIC_UNDERSCORES,
159:        Feature.TRAILING_COMMA_IN_CALL,
160:        Feature.TRAILING_COMMA_IN_DEF,
161:        Feature.ASYNC_IS_VALID_IDENTIFIER,
162:    },
163:    TargetVersion.PY37: {
164:        Feature.UNICODE_LITERALS,
165:        Feature.F_STRINGS,
166:        Feature.NUMERIC_UNDERSCORES,
167:        Feature.TRAILING_COMMA_IN_CALL,
168:        Feature.TRAILING_COMMA_IN_DEF,
169:        Feature.ASYNC_IS_RESERVED_KEYWORD,
170:    },
171:    TargetVersion.PY38: {
172:        Feature.UNICODE_LITERALS,
173:        Feature.F_STRINGS,
174:        Feature.NUMERIC_UNDERSCORES,
175:        Feature.TRAILING_COMMA_IN_CALL,
176:        Feature.TRAILING_COMMA_IN_DEF,
177:        Feature.ASYNC_IS_RESERVED_KEYWORD,
178:    },
179:}
180:
181:
182:@dataclass
183:class FileMode:
184:    target_versions: Set[TargetVersion] = Factory(set)
185:    line_length: int = DEFAULT_LINE_LENGTH
186:    string_normalization: bool = True
187:    is_pyi: bool = False
188:
189:    def get_cache_key(self) -> str:
190:        if self.target_versions:
191:            version_str = ",".join(
192:                str(version.value)
193:                for version in sorted(self.target_versions, key=lambda v: v.value)
194:            )
195:        else:
196:            version_str = "-"
197:        parts = [
198:            version_str,
199:            str(self.line_length),
200:            str(int(self.string_normalization)),
201:            str(int(self.is_pyi)),
202:        ]
203:        return ".".join(parts)
204:
205:
206:def supports_feature(target_versions: Set[TargetVersion], feature: Feature) -> bool:
207:    return all(feature in VERSION_TO_FEATURES[version] for version in target_versions)
208:
209:
210:def read_pyproject_toml(
211:    ctx: click.Context, param: click.Parameter, value: Union[str, int, bool, None]
212:) -> Optional[str]:
213:    """Inject Black configuration from "pyproject.toml" into defaults in `ctx`.
214:
215:    Returns the path to a successfully found and read configuration file, None
216:    otherwise.
217:    """
218:    assert not isinstance(value, (int, bool)), "Invalid parameter type passed"
219:    if not value:
220:        root = find_project_root(ctx.params.get("src", ()))
221:        path = root / "pyproject.toml"
222:        if path.is_file():
223:            value = str(path)
224:        else:
225:            return None
226:
227:    try:
228:        pyproject_toml = toml.load(value)
229:        config = pyproject_toml.get("tool", {}).get("black", {})
230:    except (toml.TomlDecodeError, OSError) as e:
231:        raise click.FileError(
232:            filename=value, hint=f"Error reading configuration file: {e}"
233:        )
234:
235:    if not config:
236:        return None
237:
238:    if ctx.default_map is None:
239:        ctx.default_map = {}

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: Verified in black.py lines 146-179 that VERSION_TO_FEATURES assigns ASYNC_IS_VALID_IDENTIFIER for PY27, PY33, PY34, PY35, PY36 and ASYNC_IS_RESERVED_KEYWORD for PY37, PY38. Each version includes exactly one of the two flagsnone include both or neither.

Todo item 2 of 19 is correct
description: 'Check VERSION_TO_FEATURES mapping ensures mutual exclusivity: for each TargetVersion (PY27, PY33, PY34, PY35, PY36) include ASYNC_IS_VALID_IDENTIFIER, and for (PY37, PY38) include ASYNC_IS_RESERVED_KEYWORD; confirm exactly one of the two flags is present per version.'

Validating todo item 3 of 19
description: 'Confirm `ParserConfig` dataclass exists in `black.py` with fields `grammar: Grammar` and `tokenizer_config: TokenizerConfig = TokenizerConfig()` and that `TokenizerConfig` is imported from `blib2to3.pgen2.tokenize`.'

Calling tool: grepTool
ParserConfig
Grep command failed with exit code 2: rg: /Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/{{FOLDER_PATH}}: IO error for operation on /Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/{{FOLDER_PATH}}: No such file or directory (os error 2)


Calling tool: grepTool
ParserConfig
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:764:class ParserConfig:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:774:            ParserConfig(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:779:            ParserConfig(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:784:            ParserConfig(pygram.python_grammar_no_print_statement),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:786:            ParserConfig(pygram.python_grammar),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:792:            ParserConfig(pygram.python_grammar_no_print_statement),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:794:            ParserConfig(pygram.python_grammar),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:803:                ParserConfig(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:811:                ParserConfig(

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
740:            current_line, line_length=mode.line_length, features=split_line_features
741:        ):
742:            dst_contents += str(line)
743:    return dst_contents
744:
745:
746:def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:
747:    """Return a tuple of (decoded_contents, encoding, newline).
748:
749:    `newline` is either CRLF or LF but `decoded_contents` is decoded with
750:    universal newlines (i.e. only contains LF).
751:    """
752:    srcbuf = io.BytesIO(src)
753:    encoding, lines = tokenize.detect_encoding(srcbuf.readline)
754:    if not lines:
755:        return "", encoding, "\n"
756:
757:    newline = "\r\n" if b"\r\n" == lines[0][-2:] else "\n"
758:    srcbuf.seek(0)
759:    with io.TextIOWrapper(srcbuf, encoding) as tiow:
760:        return tiow.read(), encoding, newline
761:
762:
763:@dataclass(frozen=True)
764:class ParserConfig:
765:    grammar: Grammar
766:    tokenizer_config: TokenizerConfig = TokenizerConfig()
767:
768:
769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:
770:    if not target_versions:
771:        # No target_version specified, so try all grammars.
772:        return [
773:            # Python 3.7+
774:            ParserConfig(
775:                pygram.python_grammar_no_print_statement_no_exec_statement,
776:                TokenizerConfig(async_is_reserved_keyword=True),
777:            ),
778:            # Python 3.0-3.6
779:            ParserConfig(
780:                pygram.python_grammar_no_print_statement_no_exec_statement,
781:                TokenizerConfig(async_is_reserved_keyword=False),
782:            ),
783:            # Python 2.7 with future print_function import
784:            ParserConfig(pygram.python_grammar_no_print_statement),
785:            # Python 2.7
786:            ParserConfig(pygram.python_grammar),
787:        ]
788:    elif all(version.is_python2() for version in target_versions):
789:        # Python 2-only code, so try Python 2 grammars.
790:        return [
791:            # Python 2.7 with future print_function import
792:            ParserConfig(pygram.python_grammar_no_print_statement),
793:            # Python 2.7
794:            ParserConfig(pygram.python_grammar),
795:        ]
796:    else:
797:        # Python 3-compatible code, so only try Python 3 grammar.
798:        configs = []
799:        # If we have to parse both, try to parse async as a keyword first
800:        if not supports_feature(target_versions, Feature.ASYNC_IS_VALID_IDENTIFIER):
801:            # Python 3.7+
802:            configs.append(
803:                ParserConfig(
804:                    pygram.python_grammar_no_print_statement_no_exec_statement,
805:                    TokenizerConfig(async_is_reserved_keyword=True),
806:                )
807:            )
808:        if not supports_feature(target_versions, Feature.ASYNC_IS_RESERVED_KEYWORD):
809:            # Python 3.0-3.6
810:            configs.append(
811:                ParserConfig(
812:                    pygram.python_grammar_no_print_statement_no_exec_statement,
813:                    TokenizerConfig(async_is_reserved_keyword=False),
814:                )
815:            )
816:        # At least one of the above branches must have been taken, because every Python
817:        # version has exactly one of the two 'ASYNC_IS_*' flags
818:        return configs
819:
820:
821:def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:
822:    """Given a string with source, return the lib2to3 Node."""
823:    if src_txt[-1:] != "\n":
824:        src_txt += "\n"
825:
826:    for parser_config in get_parser_configs(set(target_versions)):
827:        drv = driver.Driver(
828:            parser_config.grammar,
829:            pytree.convert,
830:            tokenizer_config=parser_config.tokenizer_config,
831:        )
832:        try:
833:            result = drv.parse_string(src_txt, True)
834:            break
835:
836:        except ParseError as pe:
837:            lineno, column = pe.context[1]
838:            lines = src_txt.splitlines()
839:            try:
840:                faulty_line = lines[lineno - 1]
841:            except IndexError:
842:                faulty_line = "<line number missing in source>"
843:            exc = InvalidInput(f"Cannot parse: {lineno}:{column}: {faulty_line}")
844:    else:
845:        raise exc from None
846:
847:    if isinstance(result, Leaf):
848:        result = Node(syms.file_input, [result])
849:    return result
850:
851:
852:def lib2to3_unparse(node: Node) -> str:
853:    """Given a lib2to3 node, return its string representation."""
854:    code = str(node)
855:    return code
856:
857:
858:T = TypeVar("T")
859:

Calling tool: grepTool
from blib2to3\.pgen2\.tokenize import TokenizerConfig
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.pyi:11:from blib2to3.pgen2.tokenize import TokenizerConfig
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:51:from blib2to3.pgen2.tokenize import TokenizerConfig

Calling tool: grepTool
from blib2to3\.pgen2\.grammar import Grammar
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:49:from blib2to3.pgen2.grammar import Grammar
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pygram.pyi:5:from blib2to3.pgen2.grammar import Grammar
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pytree.pyi:6:from blib2to3.pgen2.grammar import Grammar
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/parse.pyi:5:from blib2to3.pgen2.grammar import Grammar, _DFAS
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.pyi:10:from blib2to3.pgen2.grammar import Grammar

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
40:from attr import dataclass, evolve, Factory
41:import click
42:import toml
43:from typed_ast import ast3, ast27
44:
45:# lib2to3 fork
46:from blib2to3.pytree import Node, Leaf, type_repr
47:from blib2to3 import pygram, pytree
48:from blib2to3.pgen2 import driver, token
49:from blib2to3.pgen2.grammar import Grammar
50:from blib2to3.pgen2.parse import ParseError
51:from blib2to3.pgen2.tokenize import TokenizerConfig
52:
53:
54:__version__ = "19.3b0"
55:DEFAULT_LINE_LENGTH = 88
56:DEFAULT_EXCLUDES = (
57:    r"/(\.eggs|\.git|\.hg|\.mypy_cache|\.nox|\.tox|\.venv|_build|buck-out|build|dist)/"
58:)
59:DEFAULT_INCLUDES = r"\.pyi?$"
60:CACHE_DIR = Path(user_cache_dir("black", version=__version__))
61:
62:
63:# types
64:FileContent = str
65:Encoding = str
66:NewLine = str
67:Depth = int
68:NodeType = int
69:LeafID = int

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: `black.py` defines `@dataclass(frozen=True) class ParserConfig` with fields `grammar: Grammar` and `tokenizer_config: TokenizerConfig = TokenizerConfig()` (lines 763-766). Additionally, `TokenizerConfig` is imported from `blib2to3.pgen2.tokenize` (line 51).

Todo item 3 of 19 is correct
description: 'Confirm `ParserConfig` dataclass exists in `black.py` with fields `grammar: Grammar` and `tokenizer_config: TokenizerConfig = TokenizerConfig()` and that `TokenizerConfig` is imported from `blib2to3.pgen2.tokenize`.'

Validating todo item 4 of 19
description: 'Validate `get_parser_configs()` replaces `get_grammars()` and returns configurations as specified: when no target_versions, order is [Py3.7+ reserved, Py3.0-3.6 non-reserved, Py2.7 no print, Py2.7]; when all Python2 targets, only the two Py2 grammars; when any Python3 targets, only Py3 grammar with: reserved config added if ASYNC_IS_VALID_IDENTIFIER not supported, non-reserved config added if ASYNC_IS_RESERVED_KEYWORD not supported, in that order.'

Calling tool: grepTool
def get_parser_configs\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
740:            current_line, line_length=mode.line_length, features=split_line_features
741:        ):
742:            dst_contents += str(line)
743:    return dst_contents
744:
745:
746:def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:
747:    """Return a tuple of (decoded_contents, encoding, newline).
748:
749:    `newline` is either CRLF or LF but `decoded_contents` is decoded with
750:    universal newlines (i.e. only contains LF).
751:    """
752:    srcbuf = io.BytesIO(src)
753:    encoding, lines = tokenize.detect_encoding(srcbuf.readline)
754:    if not lines:
755:        return "", encoding, "\n"
756:
757:    newline = "\r\n" if b"\r\n" == lines[0][-2:] else "\n"
758:    srcbuf.seek(0)
759:    with io.TextIOWrapper(srcbuf, encoding) as tiow:
760:        return tiow.read(), encoding, newline
761:
762:
763:@dataclass(frozen=True)
764:class ParserConfig:
765:    grammar: Grammar
766:    tokenizer_config: TokenizerConfig = TokenizerConfig()
767:
768:
769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:
770:    if not target_versions:
771:        # No target_version specified, so try all grammars.
772:        return [
773:            # Python 3.7+
774:            ParserConfig(
775:                pygram.python_grammar_no_print_statement_no_exec_statement,
776:                TokenizerConfig(async_is_reserved_keyword=True),
777:            ),
778:            # Python 3.0-3.6
779:            ParserConfig(
780:                pygram.python_grammar_no_print_statement_no_exec_statement,
781:                TokenizerConfig(async_is_reserved_keyword=False),
782:            ),
783:            # Python 2.7 with future print_function import
784:            ParserConfig(pygram.python_grammar_no_print_statement),
785:            # Python 2.7
786:            ParserConfig(pygram.python_grammar),
787:        ]
788:    elif all(version.is_python2() for version in target_versions):
789:        # Python 2-only code, so try Python 2 grammars.
790:        return [
791:            # Python 2.7 with future print_function import
792:            ParserConfig(pygram.python_grammar_no_print_statement),
793:            # Python 2.7
794:            ParserConfig(pygram.python_grammar),
795:        ]
796:    else:
797:        # Python 3-compatible code, so only try Python 3 grammar.
798:        configs = []
799:        # If we have to parse both, try to parse async as a keyword first
800:        if not supports_feature(target_versions, Feature.ASYNC_IS_VALID_IDENTIFIER):
801:            # Python 3.7+
802:            configs.append(
803:                ParserConfig(
804:                    pygram.python_grammar_no_print_statement_no_exec_statement,
805:                    TokenizerConfig(async_is_reserved_keyword=True),
806:                )
807:            )
808:        if not supports_feature(target_versions, Feature.ASYNC_IS_RESERVED_KEYWORD):
809:            # Python 3.0-3.6
810:            configs.append(
811:                ParserConfig(
812:                    pygram.python_grammar_no_print_statement_no_exec_statement,
813:                    TokenizerConfig(async_is_reserved_keyword=False),
814:                )
815:            )
816:        # At least one of the above branches must have been taken, because every Python
817:        # version has exactly one of the two 'ASYNC_IS_*' flags
818:        return configs
819:
820:
821:def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:
822:    """Given a string with source, return the lib2to3 Node."""
823:    if src_txt[-1:] != "\n":
824:        src_txt += "\n"
825:
826:    for parser_config in get_parser_configs(set(target_versions)):
827:        drv = driver.Driver(
828:            parser_config.grammar,
829:            pytree.convert,
830:            tokenizer_config=parser_config.tokenizer_config,
831:        )
832:        try:
833:            result = drv.parse_string(src_txt, True)
834:            break
835:
836:        except ParseError as pe:
837:            lineno, column = pe.context[1]
838:            lines = src_txt.splitlines()
839:            try:
840:                faulty_line = lines[lineno - 1]
841:            except IndexError:
842:                faulty_line = "<line number missing in source>"
843:            exc = InvalidInput(f"Cannot parse: {lineno}:{column}: {faulty_line}")
844:    else:
845:        raise exc from None
846:
847:    if isinstance(result, Leaf):
848:        result = Node(syms.file_input, [result])
849:    return result
850:
851:
852:def lib2to3_unparse(node: Node) -> str:
853:    """Given a lib2to3 node, return its string representation."""
854:    code = str(node)
855:    return code
856:
857:
858:T = TypeVar("T")
859:
860:
861:class Visitor(Generic[T]):
862:    """Basic lib2to3 visitor that yields things of type `T` on `visit()`."""
863:
864:    def visit(self, node: LN) -> Iterator[T]:
865:        """Main method to visit `node` and its children.
866:
867:        It tries to find a `visit_*()` method for the given `node.type`, like
868:        `visit_simple_stmt` for Node objects or `visit_INDENT` for Leaf objects.
869:        If no dedicated `visit_*()` method is found, chooses `visit_default()`
870:        instead.
871:
872:        Then yields objects of type `T` from the selected visitor.
873:        """
874:        if node.type < 256:
875:            name = token.tok_name[node.type]
876:        else:
877:            name = type_repr(node.type)
878:        yield from getattr(self, f"visit_{name}", self.visit_default)(node)
879:

Calling tool: grepTool
from blib2to3\.pgen2\.tokenize import TokenizerConfig
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.pyi:11:from blib2to3.pgen2.tokenize import TokenizerConfig
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:51:from blib2to3.pgen2.tokenize import TokenizerConfig

Calling tool: grepTool
class Driver
Matched content:
30:class Driver(object):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Modifications:
5:# Copyright 2006 Google, Inc. All Rights Reserved.
6:# Licensed to PSF under a Contributor Agreement.
7:
8:"""Parser driver.
9:
10:This provides a high-level interface to parse a file into a syntax tree.
11:
12:"""
13:
14:__author__ = "Guido van Rossum <guido@python.org>"
15:
16:__all__ = ["Driver", "load_grammar"]
17:
18:# Python imports
19:import codecs
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(
33:        self,
34:        grammar,
35:        convert=None,
36:        logger=None,
37:        tokenizer_config=tokenize.TokenizerConfig(),
38:    ):
39:        self.grammar = grammar
40:        if logger is None:
41:            logger = logging.getLogger(__name__)
42:        self.logger = logger
43:        self.convert = convert
44:        self.tokenizer_config = tokenizer_config
45:
46:    def parse_tokens(self, tokens, debug=False):
47:        """Parse a series of tokens and return the syntax tree."""
48:        # XXX Move the prefix computation into a wrapper around tokenize.
49:        p = parse.Parser(self.grammar, self.convert)
50:        p.setup()
51:        lineno = 1
52:        column = 0
53:        indent_columns = []
54:        type = value = start = end = line_text = None
55:        prefix = ""
56:        for quintuple in tokens:
57:            type, value, start, end, line_text = quintuple
58:            if start != (lineno, column):
59:                assert (lineno, column) <= start, ((lineno, column), start)
60:                s_lineno, s_column = start
61:                if lineno < s_lineno:
62:                    prefix += "\n" * (s_lineno - lineno)
63:                    lineno = s_lineno
64:                    column = 0
65:                if column < s_column:
66:                    prefix += line_text[column:s_column]
67:                    column = s_column
68:            if type in (tokenize.COMMENT, tokenize.NL):
69:                prefix += value
70:                lineno, column = end
71:                if value.endswith("\n"):
72:                    lineno += 1
73:                    column = 0
74:                continue
75:            if type == token.OP:
76:                type = grammar.opmap[value]
77:            if debug:
78:                self.logger.debug("%s %r (prefix=%r)",
79:                                  token.tok_name[type], value, prefix)
80:            if type == token.INDENT:
81:                indent_columns.append(len(value))
82:                _prefix = prefix + value
83:                prefix = ""
84:                value = ""
85:            elif type == token.DEDENT:
86:                _indent_col = indent_columns.pop()
87:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
88:            if p.addtoken(type, value, (prefix, start)):
89:                if debug:
90:                    self.logger.debug("Stop.")
91:                break
92:            prefix = ""
93:            if type in {token.INDENT, token.DEDENT}:
94:                prefix = _prefix
95:            lineno, column = end
96:            if value.endswith("\n"):
97:                lineno += 1
98:                column = 0
99:        else:
100:            # We never broke out -- EOF is too soon (how can this happen???)
101:            raise parse.ParseError("incomplete input",
102:                                   type, value, (prefix, start))
103:        return p.rootnode
104:
105:    def parse_stream_raw(self, stream, debug=False):
106:        """Parse a stream and return the syntax tree."""
107:        tokens = tokenize.generate_tokens(stream.readline, config=self.tokenizer_config)
108:        return self.parse_tokens(tokens, debug)
109:
110:    def parse_stream(self, stream, debug=False):
111:        """Parse a stream and return the syntax tree."""
112:        return self.parse_stream_raw(stream, debug)
113:
114:    def parse_file(self, filename, encoding=None, debug=False):
115:        """Parse a file and return the syntax tree."""
116:        with io.open(filename, "r", encoding=encoding) as stream:
117:            return self.parse_stream(stream, debug)
118:
119:    def parse_string(self, text, debug=False):
120:        """Parse a string and return the syntax tree."""
121:        tokens = tokenize.generate_tokens(
122:            io.StringIO(text).readline,
123:            config=self.tokenizer_config,
124:        )
125:        return self.parse_tokens(tokens, debug)
126:
127:    def _partially_consume_prefix(self, prefix, column):
128:        lines = []
129:        current_line = ""
130:        current_column = 0
131:        wait_for_nl = False
132:        for char in prefix:
133:            current_line += char
134:            if wait_for_nl:
135:                if char == '\n':
136:                    if current_line.strip() and current_column < column:
137:                        res = ''.join(lines)
138:                        return res, prefix[len(res):]
139:
140:                    lines.append(current_line)
141:                    current_line = ""
142:                    current_column = 0
143:                    wait_for_nl = False
144:            elif char in ' \t':
145:                current_column += 1
146:            elif char == '\n':
147:                # unexpected empty line
148:                current_column = 0
149:            else:
150:                # indent is finished
151:                wait_for_nl = True
152:        return ''.join(lines), current_line
153:
154:
155:def _generate_pickle_name(gt, cache_dir=None):
156:    head, tail = os.path.splitext(gt)
157:    if tail == ".txt":
158:        tail = ""
159:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
160:    if cache_dir:

Calling tool: grepTool
class TokenizerConfig
Matched content:
142:class TokenizerConfig:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
120:)
121:
122:endprogs = {"'": re.compile(Single), '"': re.compile(Double),
123:            "'''": single3prog, '"""': double3prog,
124:            **{f"{prefix}'''": single3prog for prefix in _strprefixes},
125:            **{f'{prefix}"""': double3prog for prefix in _strprefixes},
126:            **{prefix: None for prefix in _strprefixes}}
127:
128:triple_quoted = (
129:    {"'''", '"""'} |
130:    {f"{prefix}'''" for prefix in _strprefixes} |
131:    {f'{prefix}"""' for prefix in _strprefixes}
132:)
133:single_quoted = (
134:    {"'", '"'} |
135:    {f"{prefix}'" for prefix in _strprefixes} |
136:    {f'{prefix}"' for prefix in _strprefixes}
137:)
138:
139:tabsize = 8
140:
141:@dataclass(frozen=True)
142:class TokenizerConfig:
143:    async_is_reserved_keyword: bool = False
144:
145:class TokenError(Exception): pass
146:
147:class StopTokenizing(Exception): pass
148:
149:def printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line): # for testing
150:    (srow, scol) = xxx_todo_changeme
151:    (erow, ecol) = xxx_todo_changeme1
152:    print("%d,%d-%d,%d:\t%s\t%s" % \
153:        (srow, scol, erow, ecol, tok_name[type], repr(token)))
154:
155:def tokenize(readline, tokeneater=printtoken):
156:    """
157:    The tokenize() function accepts two parameters: one representing the
158:    input stream, and one providing an output mechanism for tokenize().
159:
160:    The first parameter, readline, must be a callable object which provides
161:    the same interface as the readline() method of built-in file objects.
162:    Each call to the function should return one line of input as a string.
163:
164:    The second parameter, tokeneater, must also be a callable object. It is
165:    called once for each token, with five arguments, corresponding to the
166:    tuples generated by generate_tokens().
167:    """
168:    try:
169:        tokenize_loop(readline, tokeneater)
170:    except StopTokenizing:
171:        pass
172:
173:# backwards compatible interface
174:def tokenize_loop(readline, tokeneater):
175:    for token_info in generate_tokens(readline):
176:        tokeneater(*token_info)
177:
178:class Untokenizer:
179:
180:    def __init__(self):
181:        self.tokens = []
182:        self.prev_row = 1
183:        self.prev_col = 0
184:
185:    def add_whitespace(self, start):
186:        row, col = start
187:        assert row <= self.prev_row
188:        col_offset = col - self.prev_col
189:        if col_offset:
190:            self.tokens.append(" " * col_offset)
191:
192:    def untokenize(self, iterable):
193:        for t in iterable:
194:            if len(t) == 2:
195:                self.compat(t, iterable)
196:                break
197:            tok_type, token, start, end, line = t
198:            self.add_whitespace(start)
199:            self.tokens.append(token)

Calling tool: grepTool
def generate_tokens\(
Matched content:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
334:        newcode = untokenize(t1)
335:        readline = iter(newcode.splitlines(1)).next
336:        t2 = [tok[:2] for tokin generate_tokens(readline)]
337:        assert t1 == t2
338:    """
339:    ut = Untokenizer()
340:    return ut.untokenize(iterable)
341:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
343:    """
344:    The generate_tokens() generator requires one argument, readline, which
345:    must be a callable object which provides the same interface as the
346:    readline() method of built-in file objects. Each call to the function
347:    should return one line of input as a string.  Alternately, readline
348:    can be a callable function terminating with StopIteration:
349:        readline = open(myfile).next    # Example of alternate readline
350:
351:    The generator produces 5-tuples with these members: the token type; the
352:    token string; a 2-tuple (srow, scol) of ints specifying the row and
353:    column where the token begins in the source; a 2-tuple (erow, ecol) of
354:    ints specifying the row and column where the token ends in the source;
355:    and the line on which the token was found. The line passed is the
356:    logical line; continuation lines are included.
357:    """
358:    lnum = parenlev = continued = 0
359:    numchars = '0123456789'
360:    contstr, needcont = '', 0
361:    contline = None
362:    indents = [0]
363:
364:    # If we know we're parsing 3.7+, we can unconditionally parse `async` and
365:    # `await` as keywords.
366:    async_is_reserved_keyword = config.async_is_reserved_keyword
367:    # 'stashed' and 'async_*' are used for async/await parsing
368:    stashed = None
369:    async_def = False
370:    async_def_indent = 0
371:    async_def_nl = False
372:
373:    while 1:                                   # loop over lines in stream
374:        try:
375:            line = readline()
376:        except StopIteration:
377:            line = ''
378:        lnum = lnum + 1
379:        pos, max = 0, len(line)
380:
381:        if contstr:                            # continued string
382:            if not line:
383:                raise TokenError("EOF in multi-line string", strstart)
384:            endmatch = endprog.match(line)
385:            if endmatch:
386:                pos = end = endmatch.end(0)
387:                yield (STRING, contstr + line[:end],
388:                       strstart, (lnum, end), contline + line)
389:                contstr, needcont = '', 0
390:                contline = None
391:            elif needcont and line[-2:] != '\\\n' and line[-3:] != '\\\r\n':
392:                yield (ERRORTOKEN, contstr + line,
393:                           strstart, (lnum, len(line)), contline)
394:                contstr = ''
395:                contline = None
396:                continue
397:            else:
398:                contstr = contstr + line
399:                contline = contline + line
400:                continue
401:
402:        elif parenlev == 0 and not continued:  # new statement
403:            if not line: break
404:            column = 0
405:            while pos < max:                   # measure leading whitespace
406:                if line[pos] == ' ': column = column + 1
407:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize
408:                elif line[pos] == '\f': column = 0
409:                else: break
410:                pos = pos + 1
411:            if pos == max: break
412:
413:            if stashed:
414:                yield stashed
415:                stashed = None
416:
417:            if line[pos] in '\r\n':            # skip blank lines
418:                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)
419:                continue
420:
421:            if line[pos] == '#':               # skip comments
422:                comment_token = line[pos:].rstrip('\r\n')
423:                nl_pos = pos + len(comment_token)
424:                yield (COMMENT, comment_token,
425:                        (lnum, pos), (lnum, pos + len(comment_token)), line)
426:                yield (NL, line[nl_pos:],
427:                        (lnum, nl_pos), (lnum, len(line)), line)
428:                continue
429:
430:            if column > indents[-1]:           # count indents
431:                indents.append(column)
432:                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)
433:
434:            while column < indents[-1]:        # count dedents
435:                if column not in indents:
436:                    raise IndentationError(
437:                        "unindent does not match any outer indentation level",
438:                        ("<tokenize>", lnum, pos, line))
439:                indents = indents[:-1]
440:
441:                if async_def and async_def_indent >= indents[-1]:
442:                    async_def = False
443:                    async_def_nl = False
444:                    async_def_indent = 0
445:
446:                yield (DEDENT, '', (lnum, pos), (lnum, pos), line)
447:
448:            if async_def and async_def_nl and async_def_indent >= indents[-1]:
449:                async_def = False
450:                async_def_nl = False
451:                async_def_indent = 0
452:
453:        else:                                  # continued statement
454:            if not line:
455:                raise TokenError("EOF in multi-line statement", (lnum, 0))
456:            continued = 0
457:
458:        while pos < max:
459:            pseudomatch = pseudoprog.match(line, pos)
460:            if pseudomatch:                                # scan for tokens
461:                start, end = pseudomatch.span(1)
462:                spos, epos, pos = (lnum, start), (lnum, end), end
463:                token, initial = line[start:end], line[start]
464:
465:                if initial in numchars or \
466:                   (initial == '.' and token != '.'):      # ordinary number
467:                    yield (NUMBER, token, spos, epos, line)
468:                elif initial in '\r\n':
469:                    newline = NEWLINE
470:                    if parenlev > 0:
471:                        newline = NL
472:                    elif async_def:
473:                        async_def_nl = True
474:                    if stashed:
475:                        yield stashed
476:                        stashed = None
477:                    yield (newline, token, spos, epos, line)
478:
479:                elif initial == '#':
480:                    assert not token.endswith("\n")
481:                    if stashed:
482:                        yield stashed
483:                        stashed = None
484:                    yield (COMMENT, token, spos, epos, line)
485:                elif token in triple_quoted:
486:                    endprog = endprogs[token]
487:                    endmatch = endprog.match(line, pos)
488:                    if endmatch:                           # all on one line
489:                        pos = endmatch.end(0)
490:                        token = line[start:pos]
491:                        if stashed:
492:                            yield stashed
493:                            stashed = None
494:                        yield (STRING, token, spos, (lnum, pos), line)
495:                    else:
496:                        strstart = (lnum, start)           # multiple lines
497:                        contstr = line[start:]
498:                        contline = line
499:                        break
500:                elif initial in single_quoted or \
501:                    token[:2] in single_quoted or \
502:                    token[:3] in single_quoted:
503:                    if token[-1] == '\n':                  # continued string
504:                        strstart = (lnum, start)
505:                        endprog = (endprogs[initial] or endprogs[token[1]] or
506:                                   endprogs[token[2]])
507:                        contstr, needcont = line[start:], 1
508:                        contline = line
509:                        break
510:                    else:                                  # ordinary string
511:                        if stashed:
512:                            yield stashed
513:                            stashed = None
514:                        yield (STRING, token, spos, epos, line)
515:                elif initial.isidentifier():               # ordinary name
516:                    if token in ('async', 'await'):
517:                        if async_is_reserved_keyword or async_def:
518:                            yield (ASYNC if token == 'async' else AWAIT,
519:                                   token, spos, epos, line)
520:                            continue
521:
522:                    tok = (NAME, token, spos, epos, line)
523:                    if token == 'async' and not stashed:
524:                        stashed = tok
525:                        continue
526:
527:                    if token in ('def', 'for'):
528:                        if (stashed
529:                                and stashed[0] == NAME
530:                                and stashed[1] == 'async'):
531:
532:                            if token == 'def':
533:                                async_def = True
534:                                async_def_indent = indents[-1]
535:
536:                            yield (ASYNC, stashed[1],
537:                                   stashed[2], stashed[3],
538:                                   stashed[4])
539:                            stashed = None
540:
541:                    if stashed:
542:                        yield stashed
543:                        stashed = None
544:
545:                    yield tok
546:                elif initial == '\\':                      # continued stmt
547:                    # This yield is new; needed for better idempotency:
548:                    if stashed:
549:                        yield stashed
550:                        stashed = None
551:                    yield (NL, token, spos, (lnum, pos), line)
552:                    continued = 1
553:                else:

Calling tool: grepTool
get_grammars\(
No files found. Call grepTool again with a more generic pattern.

Calling tool: grepTool
def supports_feature\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:206:def supports_feature(target_versions: Set[TargetVersion], feature: Feature) -> bool:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
180:
181:
182:@dataclass
183:class FileMode:
184:    target_versions: Set[TargetVersion] = Factory(set)
185:    line_length: int = DEFAULT_LINE_LENGTH
186:    string_normalization: bool = True
187:    is_pyi: bool = False
188:
189:    def get_cache_key(self) -> str:
190:        if self.target_versions:
191:            version_str = ",".join(
192:                str(version.value)
193:                for version in sorted(self.target_versions, key=lambda v: v.value)
194:            )
195:        else:
196:            version_str = "-"
197:        parts = [
198:            version_str,
199:            str(self.line_length),
200:            str(int(self.string_normalization)),
201:            str(int(self.is_pyi)),
202:        ]
203:        return ".".join(parts)
204:
205:
206:def supports_feature(target_versions: Set[TargetVersion], feature: Feature) -> bool:
207:    return all(feature in VERSION_TO_FEATURES[version] for version in target_versions)
208:
209:
210:def read_pyproject_toml(
211:    ctx: click.Context, param: click.Parameter, value: Union[str, int, bool, None]
212:) -> Optional[str]:
213:    """Inject Black configuration from "pyproject.toml" into defaults in `ctx`.
214:
215:    Returns the path to a successfully found and read configuration file, None
216:    otherwise.
217:    """
218:    assert not isinstance(value, (int, bool)), "Invalid parameter type passed"
219:    if not value:
220:        root = find_project_root(ctx.params.get("src", ()))
221:        path = root / "pyproject.toml"
222:        if path.is_file():
223:            value = str(path)
224:        else:
225:            return None
226:
227:    try:
228:        pyproject_toml = toml.load(value)
229:        config = pyproject_toml.get("tool", {}).get("black", {})
230:    except (toml.TomlDecodeError, OSError) as e:
231:        raise click.FileError(
232:            filename=value, hint=f"Error reading configuration file: {e}"
233:        )
234:
235:    if not config:
236:        return None
237:
238:    if ctx.default_map is None:
239:        ctx.default_map = {}

Calling tool: grepTool
class Feature\(
Matched content:
133:class Feature(Enum):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
130:PY36_VERSIONS = {TargetVersion.PY36, TargetVersion.PY37, TargetVersion.PY38}
131:
132:
133:class Feature(Enum):
134:    # All string literals are unicode
135:    UNICODE_LITERALS = 1
136:    F_STRINGS = 2
137:    NUMERIC_UNDERSCORES = 3
138:    TRAILING_COMMA_IN_CALL = 4
139:    TRAILING_COMMA_IN_DEF = 5
140:    # The following two feature-flags are mutually exclusive, and exactly one should be
141:    # set for every version of python.
142:    ASYNC_IS_VALID_IDENTIFIER = 6
143:    ASYNC_IS_RESERVED_KEYWORD = 7
144:
145:
146:VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {
147:    TargetVersion.PY27: {Feature.ASYNC_IS_VALID_IDENTIFIER},
148:    TargetVersion.PY33: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
149:    TargetVersion.PY34: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
150:    TargetVersion.PY35: {
151:        Feature.UNICODE_LITERALS,
152:        Feature.TRAILING_COMMA_IN_CALL,
153:        Feature.ASYNC_IS_VALID_IDENTIFIER,
154:    },
155:    TargetVersion.PY36: {
156:        Feature.UNICODE_LITERALS,
157:        Feature.F_STRINGS,
158:        Feature.NUMERIC_UNDERSCORES,
159:        Feature.TRAILING_COMMA_IN_CALL,
160:        Feature.TRAILING_COMMA_IN_DEF,
161:        Feature.ASYNC_IS_VALID_IDENTIFIER,
162:    },
163:    TargetVersion.PY37: {
164:        Feature.UNICODE_LITERALS,
165:        Feature.F_STRINGS,
166:        Feature.NUMERIC_UNDERSCORES,
167:        Feature.TRAILING_COMMA_IN_CALL,
168:        Feature.TRAILING_COMMA_IN_DEF,
169:        Feature.ASYNC_IS_RESERVED_KEYWORD,
170:    },
171:    TargetVersion.PY38: {
172:        Feature.UNICODE_LITERALS,
173:        Feature.F_STRINGS,
174:        Feature.NUMERIC_UNDERSCORES,
175:        Feature.TRAILING_COMMA_IN_CALL,
176:        Feature.TRAILING_COMMA_IN_DEF,
177:        Feature.ASYNC_IS_RESERVED_KEYWORD,
178:    },
179:}
180:
181:
182:@dataclass
183:class FileMode:
184:    target_versions: Set[TargetVersion] = Factory(set)
185:    line_length: int = DEFAULT_LINE_LENGTH
186:    string_normalization: bool = True
187:    is_pyi: bool = False
188:
189:    def get_cache_key(self) -> str:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
1:import asyncio
2:from asyncio.base_events import BaseEventLoop
3:from concurrent.futures import Executor, ProcessPoolExecutor
4:from datetime import datetime
5:from enum import Enum
6:from functools import lru_cache, partial, wraps
7:import io
8:import itertools
9:import logging
10:from multiprocessing import Manager, freeze_support
11:import os
12:from pathlib import Path
13:import pickle
14:import re
15:import signal
16:import sys
17:import tempfile
18:import tokenize
19:from typing import (
20:    Any,
21:    Callable,
22:    Collection,
23:    Dict,
24:    Generator,
25:    Generic,
26:    Iterable,
27:    Iterator,
28:    List,
29:    Optional,
30:    Pattern,
31:    Sequence,
32:    Set,
33:    Tuple,
34:    TypeVar,
35:    Union,
36:    cast,
37:)
38:
39:from appdirs import user_cache_dir
40:from attr import dataclass, evolve, Factory
41:import click
42:import toml
43:from typed_ast import ast3, ast27
44:
45:# lib2to3 fork
46:from blib2to3.pytree import Node, Leaf, type_repr
47:from blib2to3 import pygram, pytree
48:from blib2to3.pgen2 import driver, token
49:from blib2to3.pgen2.grammar import Grammar
50:from blib2to3.pgen2.parse import ParseError
51:from blib2to3.pgen2.tokenize import TokenizerConfig
52:
53:
54:__version__ = "19.3b0"
55:DEFAULT_LINE_LENGTH = 88
56:DEFAULT_EXCLUDES = (
57:    r"/(\.eggs|\.git|\.hg|\.mypy_cache|\.nox|\.tox|\.venv|_build|buck-out|build|dist)/"
58:)
59:DEFAULT_INCLUDES = r"\.pyi?$"
60:CACHE_DIR = Path(user_cache_dir("black", version=__version__))
61:
62:
63:# types
64:FileContent = str
65:Encoding = str
66:NewLine = str
67:Depth = int
68:NodeType = int
69:LeafID = int
70:Priority = int
71:Index = int
72:LN = Union[Leaf, Node]
73:SplitFunc = Callable[["Line", Collection["Feature"]], Iterator["Line"]]
74:Timestamp = float
75:FileSize = int
76:CacheInfo = Tuple[Timestamp, FileSize]
77:Cache = Dict[Path, CacheInfo]
78:out = partial(click.secho, bold=True, err=True)
79:err = partial(click.secho, fg="red", err=True)
80:

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: Reviewed black.py get_parser_configs implementation and its usage. It correctly replaces get_grammars and returns configurations in the specified orders and conditions, including reserved first then non-reserved for Python 3 targets, and only Py2 grammars for Python 2 targets. Verified lib2to3_parse uses these configs and no remaining get_grammars usage.

Todo item 4 of 19 is correct
description: 'Validate `get_parser_configs()` replaces `get_grammars()` and returns configurations as specified: when no target_versions, order is [Py3.7+ reserved, Py3.0-3.6 non-reserved, Py2.7 no print, Py2.7]; when all Python2 targets, only the two Py2 grammars; when any Python3 targets, only Py3 grammar with: reserved config added if ASYNC_IS_VALID_IDENTIFIER not supported, non-reserved config added if ASYNC_IS_RESERVED_KEYWORD not supported, in that order.'

Validating todo item 5 of 19
description: 'Ensure `lib2to3_parse()` iterates over `get_parser_configs()` (not grammars) and constructs `driver.Driver(grammar, pytree.convert, tokenizer_config=config)` accordingly, passing the tokenizer_config through.'

Calling tool: grepTool
lib2to3_parse\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/data/debug_visitor.py:32:        list(v.visit(lib2to3_parse(code)))
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:711:    src_node = lib2to3_parse(src_contents.lstrip(), mode.target_versions)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:821:def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:919:            code = lib2to3_parse(code)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:147:                exp_node = black.lib2to3_parse(expected)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:154:                exp_node = black.lib2to3_parse(actual)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:877:    def test_lib2to3_parse(self) -> None:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:879:            black.lib2to3_parse("invalid syntax")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:882:        black.lib2to3_parse(straddling)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:883:        black.lib2to3_parse(straddling, {TargetVersion.PY27})
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:884:        black.lib2to3_parse(straddling, {TargetVersion.PY36})
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:885:        black.lib2to3_parse(straddling, {TargetVersion.PY27, TargetVersion.PY36})
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:888:        black.lib2to3_parse(py2_only)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:889:        black.lib2to3_parse(py2_only, {TargetVersion.PY27})
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:891:            black.lib2to3_parse(py2_only, {TargetVersion.PY36})
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:893:            black.lib2to3_parse(py2_only, {TargetVersion.PY27, TargetVersion.PY36})
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:896:        black.lib2to3_parse(py3_only)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:898:            black.lib2to3_parse(py3_only, {TargetVersion.PY27})
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:899:        black.lib2to3_parse(py3_only, {TargetVersion.PY36})
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:900:        black.lib2to3_parse(py3_only, {TargetVersion.PY27, TargetVersion.PY36})
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:903:        node = black.lib2to3_parse("def f(*, arg): ...\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:905:        node = black.lib2to3_parse("def f(*, arg,): ...\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:907:        node = black.lib2to3_parse("f(*arg,)\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:911:        node = black.lib2to3_parse("def f(*, arg): f'string'\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:913:        node = black.lib2to3_parse("123_456\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:915:        node = black.lib2to3_parse("123456\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:918:        node = black.lib2to3_parse(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:925:        node = black.lib2to3_parse(expected)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:928:        node = black.lib2to3_parse(source)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:930:        node = black.lib2to3_parse(expected)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:934:        node = black.lib2to3_parse("\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:936:        node = black.lib2to3_parse("from __future__ import black\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:938:        node = black.lib2to3_parse("from __future__ import multiple, imports\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:940:        node = black.lib2to3_parse("from __future__ import (parenthesized, imports)\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:942:        node = black.lib2to3_parse(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:946:        node = black.lib2to3_parse("# comment\nfrom __future__ import black\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:948:        node = black.lib2to3_parse('"""docstring"""\nfrom __future__ import black\n')
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:950:        node = black.lib2to3_parse("some(other, code)\nfrom __future__ import black\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:952:        node = black.lib2to3_parse("from some.module import black\n")
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:954:        node = black.lib2to3_parse(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:958:        node = black.lib2to3_parse(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/tests/test_black.py:1008:        n = black.lib2to3_parse("\n")

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
760:        return tiow.read(), encoding, newline
761:
762:
763:@dataclass(frozen=True)
764:class ParserConfig:
765:    grammar: Grammar
766:    tokenizer_config: TokenizerConfig = TokenizerConfig()
767:
768:
769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:
770:    if not target_versions:
771:        # No target_version specified, so try all grammars.
772:        return [
773:            # Python 3.7+
774:            ParserConfig(
775:                pygram.python_grammar_no_print_statement_no_exec_statement,
776:                TokenizerConfig(async_is_reserved_keyword=True),
777:            ),
778:            # Python 3.0-3.6
779:            ParserConfig(
780:                pygram.python_grammar_no_print_statement_no_exec_statement,
781:                TokenizerConfig(async_is_reserved_keyword=False),
782:            ),
783:            # Python 2.7 with future print_function import
784:            ParserConfig(pygram.python_grammar_no_print_statement),
785:            # Python 2.7
786:            ParserConfig(pygram.python_grammar),
787:        ]
788:    elif all(version.is_python2() for version in target_versions):
789:        # Python 2-only code, so try Python 2 grammars.
790:        return [
791:            # Python 2.7 with future print_function import
792:            ParserConfig(pygram.python_grammar_no_print_statement),
793:            # Python 2.7
794:            ParserConfig(pygram.python_grammar),
795:        ]
796:    else:
797:        # Python 3-compatible code, so only try Python 3 grammar.
798:        configs = []
799:        # If we have to parse both, try to parse async as a keyword first
800:        if not supports_feature(target_versions, Feature.ASYNC_IS_VALID_IDENTIFIER):
801:            # Python 3.7+
802:            configs.append(
803:                ParserConfig(
804:                    pygram.python_grammar_no_print_statement_no_exec_statement,
805:                    TokenizerConfig(async_is_reserved_keyword=True),
806:                )
807:            )
808:        if not supports_feature(target_versions, Feature.ASYNC_IS_RESERVED_KEYWORD):
809:            # Python 3.0-3.6
810:            configs.append(
811:                ParserConfig(
812:                    pygram.python_grammar_no_print_statement_no_exec_statement,
813:                    TokenizerConfig(async_is_reserved_keyword=False),
814:                )
815:            )
816:        # At least one of the above branches must have been taken, because every Python
817:        # version has exactly one of the two 'ASYNC_IS_*' flags
818:        return configs
819:
820:
821:def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:
822:    """Given a string with source, return the lib2to3 Node."""
823:    if src_txt[-1:] != "\n":
824:        src_txt += "\n"
825:
826:    for parser_config in get_parser_configs(set(target_versions)):
827:        drv = driver.Driver(
828:            parser_config.grammar,
829:            pytree.convert,
830:            tokenizer_config=parser_config.tokenizer_config,
831:        )
832:        try:
833:            result = drv.parse_string(src_txt, True)
834:            break
835:
836:        except ParseError as pe:
837:            lineno, column = pe.context[1]
838:            lines = src_txt.splitlines()
839:            try:
840:                faulty_line = lines[lineno - 1]
841:            except IndexError:
842:                faulty_line = "<line number missing in source>"
843:            exc = InvalidInput(f"Cannot parse: {lineno}:{column}: {faulty_line}")
844:    else:
845:        raise exc from None
846:
847:    if isinstance(result, Leaf):
848:        result = Node(syms.file_input, [result])
849:    return result
850:
851:
852:def lib2to3_unparse(node: Node) -> str:
853:    """Given a lib2to3 node, return its string representation."""
854:    code = str(node)
855:    return code
856:
857:
858:T = TypeVar("T")
859:
860:
861:class Visitor(Generic[T]):
862:    """Basic lib2to3 visitor that yields things of type `T` on `visit()`."""
863:
864:    def visit(self, node: LN) -> Iterator[T]:
865:        """Main method to visit `node` and its children.
866:
867:        It tries to find a `visit_*()` method for the given `node.type`, like
868:        `visit_simple_stmt` for Node objects or `visit_INDENT` for Leaf objects.
869:        If no dedicated `visit_*()` method is found, chooses `visit_default()`
870:        instead.
871:
872:        Then yields objects of type `T` from the selected visitor.
873:        """
874:        if node.type < 256:
875:            name = token.tok_name[node.type]
876:        else:
877:            name = type_repr(node.type)
878:        yield from getattr(self, f"visit_{name}", self.visit_default)(node)
879:

Calling tool: grepTool
class Driver\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py:30:class Driver(object):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Modifications:
5:# Copyright 2006 Google, Inc. All Rights Reserved.
6:# Licensed to PSF under a Contributor Agreement.
7:
8:"""Parser driver.
9:
10:This provides a high-level interface to parse a file into a syntax tree.
11:
12:"""
13:
14:__author__ = "Guido van Rossum <guido@python.org>"
15:
16:__all__ = ["Driver", "load_grammar"]
17:
18:# Python imports
19:import codecs
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(
33:        self,
34:        grammar,
35:        convert=None,
36:        logger=None,
37:        tokenizer_config=tokenize.TokenizerConfig(),
38:    ):
39:        self.grammar = grammar
40:        if logger is None:
41:            logger = logging.getLogger(__name__)
42:        self.logger = logger
43:        self.convert = convert
44:        self.tokenizer_config = tokenizer_config
45:
46:    def parse_tokens(self, tokens, debug=False):
47:        """Parse a series of tokens and return the syntax tree."""
48:        # XXX Move the prefix computation into a wrapper around tokenize.
49:        p = parse.Parser(self.grammar, self.convert)
50:        p.setup()
51:        lineno = 1
52:        column = 0
53:        indent_columns = []
54:        type = value = start = end = line_text = None
55:        prefix = ""
56:        for quintuple in tokens:
57:            type, value, start, end, line_text = quintuple
58:            if start != (lineno, column):
59:                assert (lineno, column) <= start, ((lineno, column), start)
60:                s_lineno, s_column = start
61:                if lineno < s_lineno:
62:                    prefix += "\n" * (s_lineno - lineno)
63:                    lineno = s_lineno
64:                    column = 0
65:                if column < s_column:
66:                    prefix += line_text[column:s_column]
67:                    column = s_column
68:            if type in (tokenize.COMMENT, tokenize.NL):
69:                prefix += value
70:                lineno, column = end
71:                if value.endswith("\n"):
72:                    lineno += 1
73:                    column = 0
74:                continue
75:            if type == token.OP:
76:                type = grammar.opmap[value]
77:            if debug:
78:                self.logger.debug("%s %r (prefix=%r)",
79:                                  token.tok_name[type], value, prefix)
80:            if type == token.INDENT:
81:                indent_columns.append(len(value))
82:                _prefix = prefix + value
83:                prefix = ""
84:                value = ""
85:            elif type == token.DEDENT:
86:                _indent_col = indent_columns.pop()
87:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
88:            if p.addtoken(type, value, (prefix, start)):
89:                if debug:
90:                    self.logger.debug("Stop.")
91:                break
92:            prefix = ""
93:            if type in {token.INDENT, token.DEDENT}:
94:                prefix = _prefix
95:            lineno, column = end
96:            if value.endswith("\n"):
97:                lineno += 1
98:                column = 0
99:        else:
100:            # We never broke out -- EOF is too soon (how can this happen???)
101:            raise parse.ParseError("incomplete input",
102:                                   type, value, (prefix, start))
103:        return p.rootnode
104:
105:    def parse_stream_raw(self, stream, debug=False):
106:        """Parse a stream and return the syntax tree."""
107:        tokens = tokenize.generate_tokens(stream.readline, config=self.tokenizer_config)
108:        return self.parse_tokens(tokens, debug)
109:
110:    def parse_stream(self, stream, debug=False):
111:        """Parse a stream and return the syntax tree."""
112:        return self.parse_stream_raw(stream, debug)
113:
114:    def parse_file(self, filename, encoding=None, debug=False):
115:        """Parse a file and return the syntax tree."""
116:        with io.open(filename, "r", encoding=encoding) as stream:
117:            return self.parse_stream(stream, debug)
118:
119:    def parse_string(self, text, debug=False):
120:        """Parse a string and return the syntax tree."""
121:        tokens = tokenize.generate_tokens(
122:            io.StringIO(text).readline,
123:            config=self.tokenizer_config,
124:        )
125:        return self.parse_tokens(tokens, debug)
126:
127:    def _partially_consume_prefix(self, prefix, column):
128:        lines = []
129:        current_line = ""
130:        current_column = 0
131:        wait_for_nl = False
132:        for char in prefix:
133:            current_line += char
134:            if wait_for_nl:
135:                if char == '\n':
136:                    if current_line.strip() and current_column < column:
137:                        res = ''.join(lines)
138:                        return res, prefix[len(res):]
139:
140:                    lines.append(current_line)

Calling tool: grepTool
def generate_tokens\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi:32:def generate_tokens(

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
320:
321:def untokenize(iterable):
322:    """Transform tokens back into Python source code.
323:
324:    Each element returned by the iterable must be a token sequence
325:    with at least two elements, a token number and token value.  If
326:    only two tokens are passed, the resulting output is poor.
327:
328:    Round-trip invariant for full input:
329:        Untokenized source will match input source exactly
330:
331:    Round-trip invariant for limited intput:
332:        # Output text will tokenize the back to the input
333:        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
334:        newcode = untokenize(t1)
335:        readline = iter(newcode.splitlines(1)).next
336:        t2 = [tok[:2] for tokin generate_tokens(readline)]
337:        assert t1 == t2
338:    """
339:    ut = Untokenizer()
340:    return ut.untokenize(iterable)
341:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
343:    """
344:    The generate_tokens() generator requires one argument, readline, which
345:    must be a callable object which provides the same interface as the
346:    readline() method of built-in file objects. Each call to the function
347:    should return one line of input as a string.  Alternately, readline
348:    can be a callable function terminating with StopIteration:
349:        readline = open(myfile).next    # Example of alternate readline
350:
351:    The generator produces 5-tuples with these members: the token type; the
352:    token string; a 2-tuple (srow, scol) of ints specifying the row and
353:    column where the token begins in the source; a 2-tuple (erow, ecol) of
354:    ints specifying the row and column where the token ends in the source;
355:    and the line on which the token was found. The line passed is the
356:    logical line; continuation lines are included.
357:    """
358:    lnum = parenlev = continued = 0
359:    numchars = '0123456789'
360:    contstr, needcont = '', 0
361:    contline = None
362:    indents = [0]
363:
364:    # If we know we're parsing 3.7+, we can unconditionally parse `async` and
365:    # `await` as keywords.
366:    async_is_reserved_keyword = config.async_is_reserved_keyword
367:    # 'stashed' and 'async_*' are used for async/await parsing
368:    stashed = None
369:    async_def = False
370:    async_def_indent = 0
371:    async_def_nl = False
372:
373:    while 1:                                   # loop over lines in stream
374:        try:
375:            line = readline()
376:        except StopIteration:
377:            line = ''
378:        lnum = lnum + 1
379:        pos, max = 0, len(line)
380:
381:        if contstr:                            # continued string
382:            if not line:
383:                raise TokenError("EOF in multi-line string", strstart)
384:            endmatch = endprog.match(line)
385:            if endmatch:
386:                pos = end = endmatch.end(0)
387:                yield (STRING, contstr + line[:end],
388:                       strstart, (lnum, end), contline + line)
389:                contstr, needcont = '', 0
390:                contline = None
391:            elif needcont and line[-2:] != '\\\n' and line[-3:] != '\\\r\n':
392:                yield (ERRORTOKEN, contstr + line,
393:                           strstart, (lnum, len(line)), contline)
394:                contstr = ''
395:                contline = None
396:                continue
397:            else:
398:                contstr = contstr + line
399:                contline = contline + line
400:                continue
401:
402:        elif parenlev == 0 and not continued:  # new statement
403:            if not line: break
404:            column = 0
405:            while pos < max:                   # measure leading whitespace
406:                if line[pos] == ' ': column = column + 1
407:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize
408:                elif line[pos] == '\f': column = 0
409:                else: break
410:                pos = pos + 1
411:            if pos == max: break
412:
413:            if stashed:
414:                yield stashed
415:                stashed = None
416:
417:            if line[pos] in '\r\n':            # skip blank lines
418:                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)
419:                continue
420:
421:            if line[pos] == '#':               # skip comments
422:                comment_token = line[pos:].rstrip('\r\n')
423:                nl_pos = pos + len(comment_token)
424:                yield (COMMENT, comment_token,
425:                        (lnum, pos), (lnum, pos + len(comment_token)), line)
426:                yield (NL, line[nl_pos:],
427:                        (lnum, nl_pos), (lnum, len(line)), line)
428:                continue
429:
430:            if column > indents[-1]:           # count indents
431:                indents.append(column)
432:                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)
433:
434:            while column < indents[-1]:        # count dedents
435:                if column not in indents:
436:                    raise IndentationError(
437:                        "unindent does not match any outer indentation level",
438:                        ("<tokenize>", lnum, pos, line))
439:                indents = indents[:-1]
440:
441:                if async_def and async_def_indent >= indents[-1]:
442:                    async_def = False
443:                    async_def_nl = False
444:                    async_def_indent = 0
445:
446:                yield (DEDENT, '', (lnum, pos), (lnum, pos), line)
447:
448:            if async_def and async_def_nl and async_def_indent >= indents[-1]:
449:                async_def = False
450:                async_def_nl = False
451:                async_def_indent = 0
452:
453:        else:                                  # continued statement
454:            if not line:
455:                raise TokenError("EOF in multi-line statement", (lnum, 0))
456:            continued = 0
457:
458:        while pos < max:
459:            pseudomatch = pseudoprog.match(line, pos)
460:            if pseudomatch:                                # scan for tokens
461:                start, end = pseudomatch.span(1)
462:                spos, epos, pos = (lnum, start), (lnum, end), end
463:                token, initial = line[start:end], line[start]
464:
465:                if initial in numchars or \
466:                   (initial == '.' and token != '.'):      # ordinary number
467:                    yield (NUMBER, token, spos, epos, line)
468:                elif initial in '\r\n':
469:                    newline = NEWLINE
470:                    if parenlev > 0:
471:                        newline = NL
472:                    elif async_def:
473:                        async_def_nl = True
474:                    if stashed:
475:                        yield stashed
476:                        stashed = None
477:                    yield (newline, token, spos, epos, line)
478:
479:                elif initial == '#':
480:                    assert not token.endswith("\n")
481:                    if stashed:
482:                        yield stashed
483:                        stashed = None
484:                    yield (COMMENT, token, spos, epos, line)
485:                elif token in triple_quoted:
486:                    endprog = endprogs[token]
487:                    endmatch = endprog.match(line, pos)
488:                    if endmatch:                           # all on one line
489:                        pos = endmatch.end(0)
490:                        token = line[start:pos]
491:                        if stashed:
492:                            yield stashed
493:                            stashed = None
494:                        yield (STRING, token, spos, (lnum, pos), line)
495:                    else:
496:                        strstart = (lnum, start)           # multiple lines
497:                        contstr = line[start:]
498:                        contline = line
499:                        break
500:                elif initial in single_quoted or \
501:                    token[:2] in single_quoted or \
502:                    token[:3] in single_quoted:
503:                    if token[-1] == '\n':                  # continued string
504:                        strstart = (lnum, start)
505:                        endprog = (endprogs[initial] or endprogs[token[1]] or
506:                                   endprogs[token[2]])
507:                        contstr, needcont = line[start:], 1
508:                        contline = line
509:                        break
510:                    else:                                  # ordinary string
511:                        if stashed:
512:                            yield stashed
513:                            stashed = None
514:                        yield (STRING, token, spos, epos, line)
515:                elif initial.isidentifier():               # ordinary name
516:                    if token in ('async', 'await'):
517:                        if async_is_reserved_keyword or async_def:
518:                            yield (ASYNC if token == 'async' else AWAIT,
519:                                   token, spos, epos, line)
520:                            continue
521:
522:                    tok = (NAME, token, spos, epos, line)
523:                    if token == 'async' and not stashed:
524:                        stashed = tok
525:                        continue
526:
527:                    if token in ('def', 'for'):
528:                        if (stashed
529:                                and stashed[0] == NAME
530:                                and stashed[1] == 'async'):
531:
532:                            if token == 'def':
533:                                async_def = True
534:                                async_def_indent = indents[-1]
535:
536:                            yield (ASYNC, stashed[1],
537:                                   stashed[2], stashed[3],
538:                                   stashed[4])
539:                            stashed = None
540:
541:                    if stashed:
542:                        yield stashed
543:                        stashed = None
544:
545:                    yield tok
546:                elif initial == '\\':                      # continued stmt
547:                    # This yield is new; needed for better idempotency:
548:                    if stashed:
549:                        yield stashed
550:                        stashed = None
551:                    yield (NL, token, spos, (lnum, pos), line)
552:                    continued = 1
553:                else:
554:                    if initial in '([{': parenlev = parenlev + 1
555:                    elif initial in ')]}': parenlev = parenlev - 1
556:                    if stashed:
557:                        yield stashed
558:                        stashed = None
559:                    yield (OP, token, spos, epos, line)
560:            else:
561:                yield (ERRORTOKEN, line[pos],
562:                           (lnum, pos), (lnum, pos+1), line)
563:                pos = pos + 1
564:
565:    if stashed:
566:        yield stashed
567:        stashed = None
568:
569:    for indent in indents[1:]:                 # pop remaining indent levels
570:        yield (DEDENT, '', (lnum, 0), (lnum, 0), '')
571:    yield (ENDMARKER, '', (lnum, 0), (lnum, 0), '')
572:
573:if __name__ == '__main__':                     # testing
574:    import sys
575:    if len(sys.argv) > 1: tokenize(open(sys.argv[1]).readline)
576:    else: tokenize(sys.stdin.readline)

Calling tool: grepTool
from blib2to3\.pgen2\.tokenize import TokenizerConfig
Matched content:
51:from blib2to3.pgen2.tokenize import TokenizerConfig

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
40:from attr import dataclass, evolve, Factory
41:import click
42:import toml
43:from typed_ast import ast3, ast27
44:
45:# lib2to3 fork
46:from blib2to3.pytree import Node, Leaf, type_repr
47:from blib2to3 import pygram, pytree
48:from blib2to3.pgen2 import driver, token
49:from blib2to3.pgen2.grammar import Grammar
50:from blib2to3.pgen2.parse import ParseError
51:from blib2to3.pgen2.tokenize import TokenizerConfig
52:
53:
54:__version__ = "19.3b0"
55:DEFAULT_LINE_LENGTH = 88
56:DEFAULT_EXCLUDES = (
57:    r"/(\.eggs|\.git|\.hg|\.mypy_cache|\.nox|\.tox|\.venv|_build|buck-out|build|dist)/"
58:)
59:DEFAULT_INCLUDES = r"\.pyi?$"
60:CACHE_DIR = Path(user_cache_dir("black", version=__version__))
61:
62:
63:# types
64:FileContent = str
65:Encoding = str
66:NewLine = str
67:Depth = int
68:NodeType = int
69:LeafID = int

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: In black.py, lib2to3_parse() now iterates over get_parser_configs() and constructs driver.Driver(parser_config.grammar, pytree.convert, tokenizer_config=parser_config.tokenizer_config) (lines 826-831). Driver accepts tokenizer_config and passes it to tokenize.generate_tokens in parse_stream_raw and parse_string (driver.py lines 32-45, 105-108, 119-125). Thus the tokenizer_config is correctly passed through.

Todo item 5 of 19 is correct
description: 'Ensure `lib2to3_parse()` iterates over `get_parser_configs()` (not grammars) and constructs `driver.Driver(grammar, pytree.convert, tokenizer_config=config)` accordingly, passing the tokenizer_config through.'

Validating todo item 6 of 19
description: 'Confirm `driver.Driver.__init__` accepts a `tokenizer_config` parameter with default `tokenize.TokenizerConfig()` and stores it on `self.tokenizer_config`.'

Calling tool: globTool
**/blib2to3/pgen2/driver.py
Matched files:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py

Calling tool: grepTool
class Driver
Matched content:
30:class Driver(object):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Modifications:
5:# Copyright 2006 Google, Inc. All Rights Reserved.
6:# Licensed to PSF under a Contributor Agreement.
7:
8:"""Parser driver.
9:
10:This provides a high-level interface to parse a file into a syntax tree.
11:
12:"""
13:
14:__author__ = "Guido van Rossum <guido@python.org>"
15:
16:__all__ = ["Driver", "load_grammar"]
17:
18:# Python imports
19:import codecs
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(
33:        self,
34:        grammar,
35:        convert=None,
36:        logger=None,
37:        tokenizer_config=tokenize.TokenizerConfig(),
38:    ):
39:        self.grammar = grammar
40:        if logger is None:
41:            logger = logging.getLogger(__name__)
42:        self.logger = logger
43:        self.convert = convert
44:        self.tokenizer_config = tokenizer_config
45:
46:    def parse_tokens(self, tokens, debug=False):
47:        """Parse a series of tokens and return the syntax tree."""
48:        # XXX Move the prefix computation into a wrapper around tokenize.
49:        p = parse.Parser(self.grammar, self.convert)
50:        p.setup()
51:        lineno = 1
52:        column = 0
53:        indent_columns = []
54:        type = value = start = end = line_text = None
55:        prefix = ""
56:        for quintuple in tokens:
57:            type, value, start, end, line_text = quintuple
58:            if start != (lineno, column):
59:                assert (lineno, column) <= start, ((lineno, column), start)
60:                s_lineno, s_column = start
61:                if lineno < s_lineno:
62:                    prefix += "\n" * (s_lineno - lineno)
63:                    lineno = s_lineno
64:                    column = 0
65:                if column < s_column:
66:                    prefix += line_text[column:s_column]
67:                    column = s_column
68:            if type in (tokenize.COMMENT, tokenize.NL):
69:                prefix += value
70:                lineno, column = end
71:                if value.endswith("\n"):
72:                    lineno += 1
73:                    column = 0
74:                continue
75:            if type == token.OP:
76:                type = grammar.opmap[value]
77:            if debug:
78:                self.logger.debug("%s %r (prefix=%r)",
79:                                  token.tok_name[type], value, prefix)
80:            if type == token.INDENT:

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: In blib2to3/pgen2/driver.py, Driver.__init__ defines a tokenizer_config parameter with default tokenize.TokenizerConfig() (lines 32-38) and assigns it to self.tokenizer_config (line 44).

Todo item 6 of 19 is correct
description: 'Confirm `driver.Driver.__init__` accepts a `tokenizer_config` parameter with default `tokenize.TokenizerConfig()` and stores it on `self.tokenizer_config`.'

Validating todo item 7 of 19
description: 'Verify `driver.Driver.parse_stream_raw()` and `driver.Driver.parse_string()` call `tokenize.generate_tokens(..., config=self.tokenizer_config)`; ensure `parse_file()` uses `parse_stream()` and thus the config path is used.'

Calling tool: grepTool
class Driver
Grep command failed with exit code 2: rg: /Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/{{FOLDER_PATH}}: IO error for operation on /Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/{{FOLDER_PATH}}: No such file or directory (os error 2)


Calling tool: grepTool
class Driver
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py:30:class Driver(object):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.pyi:14:class Driver:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Modifications:
5:# Copyright 2006 Google, Inc. All Rights Reserved.
6:# Licensed to PSF under a Contributor Agreement.
7:
8:"""Parser driver.
9:
10:This provides a high-level interface to parse a file into a syntax tree.
11:
12:"""
13:
14:__author__ = "Guido van Rossum <guido@python.org>"
15:
16:__all__ = ["Driver", "load_grammar"]
17:
18:# Python imports
19:import codecs
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(
33:        self,
34:        grammar,
35:        convert=None,
36:        logger=None,
37:        tokenizer_config=tokenize.TokenizerConfig(),
38:    ):
39:        self.grammar = grammar
40:        if logger is None:
41:            logger = logging.getLogger(__name__)
42:        self.logger = logger
43:        self.convert = convert
44:        self.tokenizer_config = tokenizer_config
45:
46:    def parse_tokens(self, tokens, debug=False):
47:        """Parse a series of tokens and return the syntax tree."""
48:        # XXX Move the prefix computation into a wrapper around tokenize.
49:        p = parse.Parser(self.grammar, self.convert)
50:        p.setup()
51:        lineno = 1
52:        column = 0
53:        indent_columns = []
54:        type = value = start = end = line_text = None
55:        prefix = ""
56:        for quintuple in tokens:
57:            type, value, start, end, line_text = quintuple
58:            if start != (lineno, column):
59:                assert (lineno, column) <= start, ((lineno, column), start)
60:                s_lineno, s_column = start
61:                if lineno < s_lineno:
62:                    prefix += "\n" * (s_lineno - lineno)
63:                    lineno = s_lineno
64:                    column = 0
65:                if column < s_column:
66:                    prefix += line_text[column:s_column]
67:                    column = s_column
68:            if type in (tokenize.COMMENT, tokenize.NL):
69:                prefix += value
70:                lineno, column = end
71:                if value.endswith("\n"):
72:                    lineno += 1
73:                    column = 0
74:                continue
75:            if type == token.OP:
76:                type = grammar.opmap[value]
77:            if debug:
78:                self.logger.debug("%s %r (prefix=%r)",
79:                                  token.tok_name[type], value, prefix)
80:            if type == token.INDENT:
81:                indent_columns.append(len(value))
82:                _prefix = prefix + value
83:                prefix = ""
84:                value = ""
85:            elif type == token.DEDENT:
86:                _indent_col = indent_columns.pop()
87:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
88:            if p.addtoken(type, value, (prefix, start)):
89:                if debug:
90:                    self.logger.debug("Stop.")
91:                break
92:            prefix = ""
93:            if type in {token.INDENT, token.DEDENT}:
94:                prefix = _prefix
95:            lineno, column = end
96:            if value.endswith("\n"):
97:                lineno += 1
98:                column = 0
99:        else:
100:            # We never broke out -- EOF is too soon (how can this happen???)
101:            raise parse.ParseError("incomplete input",
102:                                   type, value, (prefix, start))
103:        return p.rootnode
104:
105:    def parse_stream_raw(self, stream, debug=False):
106:        """Parse a stream and return the syntax tree."""
107:        tokens = tokenize.generate_tokens(stream.readline, config=self.tokenizer_config)
108:        return self.parse_tokens(tokens, debug)
109:
110:    def parse_stream(self, stream, debug=False):
111:        """Parse a stream and return the syntax tree."""
112:        return self.parse_stream_raw(stream, debug)
113:
114:    def parse_file(self, filename, encoding=None, debug=False):
115:        """Parse a file and return the syntax tree."""
116:        with io.open(filename, "r", encoding=encoding) as stream:
117:            return self.parse_stream(stream, debug)
118:
119:    def parse_string(self, text, debug=False):
120:        """Parse a string and return the syntax tree."""
121:        tokens = tokenize.generate_tokens(
122:            io.StringIO(text).readline,
123:            config=self.tokenizer_config,
124:        )
125:        return self.parse_tokens(tokens, debug)
126:
127:    def _partially_consume_prefix(self, prefix, column):
128:        lines = []
129:        current_line = ""
130:        current_column = 0
131:        wait_for_nl = False
132:        for char in prefix:
133:            current_line += char
134:            if wait_for_nl:
135:                if char == '\n':
136:                    if current_line.strip() and current_column < column:
137:                        res = ''.join(lines)
138:                        return res, prefix[len(res):]
139:
140:                    lines.append(current_line)
141:                    current_line = ""
142:                    current_column = 0
143:                    wait_for_nl = False
144:            elif char in ' \t':
145:                current_column += 1
146:            elif char == '\n':
147:                # unexpected empty line
148:                current_column = 0
149:            else:
150:                # indent is finished
151:                wait_for_nl = True
152:        return ''.join(lines), current_line
153:
154:
155:def _generate_pickle_name(gt, cache_dir=None):
156:    head, tail = os.path.splitext(gt)
157:    if tail == ".txt":
158:        tail = ""
159:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
160:    if cache_dir:
161:        return os.path.join(cache_dir, os.path.basename(name))
162:    else:
163:        return name
164:
165:
166:def load_grammar(gt="Grammar.txt", gp=None,
167:                 save=True, force=False, logger=None):
168:    """Load the grammar (maybe from a pickle)."""
169:    if logger is None:
170:        logger = logging.getLogger(__name__)
171:    gp = _generate_pickle_name(gt) if gp is None else gp
172:    if force or not _newer(gp, gt):
173:        logger.info("Generating grammar tables from %s", gt)
174:        g = pgen.generate_grammar(gt)
175:        if save:
176:            logger.info("Writing grammar tables to %s", gp)
177:            try:
178:                g.dump(gp)
179:            except OSError as e:
180:                logger.info("Writing failed: %s", e)

Calling tool: grepTool
class TokenizerConfig
Matched content:
142:class TokenizerConfig:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
120:)
121:
122:endprogs = {"'": re.compile(Single), '"': re.compile(Double),
123:            "'''": single3prog, '"""': double3prog,
124:            **{f"{prefix}'''": single3prog for prefix in _strprefixes},
125:            **{f'{prefix}"""': double3prog for prefix in _strprefixes},
126:            **{prefix: None for prefix in _strprefixes}}
127:
128:triple_quoted = (
129:    {"'''", '"""'} |
130:    {f"{prefix}'''" for prefix in _strprefixes} |
131:    {f'{prefix}"""' for prefix in _strprefixes}
132:)
133:single_quoted = (
134:    {"'", '"'} |
135:    {f"{prefix}'" for prefix in _strprefixes} |
136:    {f'{prefix}"' for prefix in _strprefixes}
137:)
138:
139:tabsize = 8
140:
141:@dataclass(frozen=True)
142:class TokenizerConfig:
143:    async_is_reserved_keyword: bool = False
144:
145:class TokenError(Exception): pass
146:
147:class StopTokenizing(Exception): pass
148:
149:def printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line): # for testing
150:    (srow, scol) = xxx_todo_changeme
151:    (erow, ecol) = xxx_todo_changeme1
152:    print("%d,%d-%d,%d:\t%s\t%s" % \
153:        (srow, scol, erow, ecol, tok_name[type], repr(token)))
154:
155:def tokenize(readline, tokeneater=printtoken):
156:    """
157:    The tokenize() function accepts two parameters: one representing the
158:    input stream, and one providing an output mechanism for tokenize().
159:
160:    The first parameter, readline, must be a callable object which provides
161:    the same interface as the readline() method of built-in file objects.
162:    Each call to the function should return one line of input as a string.
163:
164:    The second parameter, tokeneater, must also be a callable object. It is
165:    called once for each token, with five arguments, corresponding to the
166:    tuples generated by generate_tokens().
167:    """
168:    try:
169:        tokenize_loop(readline, tokeneater)
170:    except StopTokenizing:
171:        pass
172:
173:# backwards compatible interface
174:def tokenize_loop(readline, tokeneater):
175:    for token_info in generate_tokens(readline):
176:        tokeneater(*token_info)
177:
178:class Untokenizer:
179:
180:    def __init__(self):
181:        self.tokens = []
182:        self.prev_row = 1
183:        self.prev_col = 0
184:
185:    def add_whitespace(self, start):
186:        row, col = start
187:        assert row <= self.prev_row
188:        col_offset = col - self.prev_col
189:        if col_offset:

Calling tool: grepTool
def generate_tokens\(
Matched content:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
334:        newcode = untokenize(t1)
335:        readline = iter(newcode.splitlines(1)).next
336:        t2 = [tok[:2] for tokin generate_tokens(readline)]
337:        assert t1 == t2
338:    """
339:    ut = Untokenizer()
340:    return ut.untokenize(iterable)
341:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
343:    """
344:    The generate_tokens() generator requires one argument, readline, which
345:    must be a callable object which provides the same interface as the
346:    readline() method of built-in file objects. Each call to the function
347:    should return one line of input as a string.  Alternately, readline
348:    can be a callable function terminating with StopIteration:
349:        readline = open(myfile).next    # Example of alternate readline
350:
351:    The generator produces 5-tuples with these members: the token type; the
352:    token string; a 2-tuple (srow, scol) of ints specifying the row and
353:    column where the token begins in the source; a 2-tuple (erow, ecol) of
354:    ints specifying the row and column where the token ends in the source;
355:    and the line on which the token was found. The line passed is the
356:    logical line; continuation lines are included.
357:    """
358:    lnum = parenlev = continued = 0
359:    numchars = '0123456789'
360:    contstr, needcont = '', 0
361:    contline = None
362:    indents = [0]
363:
364:    # If we know we're parsing 3.7+, we can unconditionally parse `async` and
365:    # `await` as keywords.
366:    async_is_reserved_keyword = config.async_is_reserved_keyword
367:    # 'stashed' and 'async_*' are used for async/await parsing
368:    stashed = None
369:    async_def = False
370:    async_def_indent = 0
371:    async_def_nl = False
372:
373:    while 1:                                   # loop over lines in stream
374:        try:
375:            line = readline()
376:        except StopIteration:
377:            line = ''
378:        lnum = lnum + 1
379:        pos, max = 0, len(line)
380:
381:        if contstr:                            # continued string
382:            if not line:
383:                raise TokenError("EOF in multi-line string", strstart)
384:            endmatch = endprog.match(line)
385:            if endmatch:
386:                pos = end = endmatch.end(0)
387:                yield (STRING, contstr + line[:end],
388:                       strstart, (lnum, end), contline + line)
389:                contstr, needcont = '', 0
390:                contline = None
391:            elif needcont and line[-2:] != '\\\n' and line[-3:] != '\\\r\n':
392:                yield (ERRORTOKEN, contstr + line,
393:                           strstart, (lnum, len(line)), contline)
394:                contstr = ''
395:                contline = None
396:                continue
397:            else:
398:                contstr = contstr + line
399:                contline = contline + line
400:                continue
401:
402:        elif parenlev == 0 and not continued:  # new statement
403:            if not line: break
404:            column = 0
405:            while pos < max:                   # measure leading whitespace
406:                if line[pos] == ' ': column = column + 1
407:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize
408:                elif line[pos] == '\f': column = 0
409:                else: break
410:                pos = pos + 1
411:            if pos == max: break
412:
413:            if stashed:
414:                yield stashed
415:                stashed = None
416:
417:            if line[pos] in '\r\n':            # skip blank lines
418:                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)
419:                continue
420:
421:            if line[pos] == '#':               # skip comments
422:                comment_token = line[pos:].rstrip('\r\n')
423:                nl_pos = pos + len(comment_token)
424:                yield (COMMENT, comment_token,
425:                        (lnum, pos), (lnum, pos + len(comment_token)), line)
426:                yield (NL, line[nl_pos:],
427:                        (lnum, nl_pos), (lnum, len(line)), line)
428:                continue
429:
430:            if column > indents[-1]:           # count indents
431:                indents.append(column)
432:                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)
433:
434:            while column < indents[-1]:        # count dedents
435:                if column not in indents:
436:                    raise IndentationError(
437:                        "unindent does not match any outer indentation level",
438:                        ("<tokenize>", lnum, pos, line))
439:                indents = indents[:-1]
440:
441:                if async_def and async_def_indent >= indents[-1]:
442:                    async_def = False
443:                    async_def_nl = False
444:                    async_def_indent = 0
445:
446:                yield (DEDENT, '', (lnum, pos), (lnum, pos), line)
447:
448:            if async_def and async_def_nl and async_def_indent >= indents[-1]:
449:                async_def = False
450:                async_def_nl = False
451:                async_def_indent = 0
452:
453:        else:                                  # continued statement
454:            if not line:
455:                raise TokenError("EOF in multi-line statement", (lnum, 0))
456:            continued = 0
457:
458:        while pos < max:
459:            pseudomatch = pseudoprog.match(line, pos)
460:            if pseudomatch:                                # scan for tokens
461:                start, end = pseudomatch.span(1)
462:                spos, epos, pos = (lnum, start), (lnum, end), end
463:                token, initial = line[start:end], line[start]
464:
465:                if initial in numchars or \
466:                   (initial == '.' and token != '.'):      # ordinary number
467:                    yield (NUMBER, token, spos, epos, line)
468:                elif initial in '\r\n':
469:                    newline = NEWLINE
470:                    if parenlev > 0:
471:                        newline = NL
472:                    elif async_def:
473:                        async_def_nl = True
474:                    if stashed:
475:                        yield stashed
476:                        stashed = None
477:                    yield (newline, token, spos, epos, line)
478:
479:                elif initial == '#':
480:                    assert not token.endswith("\n")
481:                    if stashed:
482:                        yield stashed
483:                        stashed = None
484:                    yield (COMMENT, token, spos, epos, line)
485:                elif token in triple_quoted:
486:                    endprog = endprogs[token]
487:                    endmatch = endprog.match(line, pos)
488:                    if endmatch:                           # all on one line
489:                        pos = endmatch.end(0)
490:                        token = line[start:pos]
491:                        if stashed:
492:                            yield stashed
493:                            stashed = None
494:                        yield (STRING, token, spos, (lnum, pos), line)
495:                    else:
496:                        strstart = (lnum, start)           # multiple lines
497:                        contstr = line[start:]
498:                        contline = line
499:                        break
500:                elif initial in single_quoted or \
501:                    token[:2] in single_quoted or \
502:                    token[:3] in single_quoted:
503:                    if token[-1] == '\n':                  # continued string
504:                        strstart = (lnum, start)
505:                        endprog = (endprogs[initial] or endprogs[token[1]] or
506:                                   endprogs[token[2]])
507:                        contstr, needcont = line[start:], 1
508:                        contline = line
509:                        break
510:                    else:                                  # ordinary string
511:                        if stashed:
512:                            yield stashed
513:                            stashed = None
514:                        yield (STRING, token, spos, epos, line)
515:                elif initial.isidentifier():               # ordinary name
516:                    if token in ('async', 'await'):
517:                        if async_is_reserved_keyword or async_def:
518:                            yield (ASYNC if token == 'async' else AWAIT,
519:                                   token, spos, epos, line)
520:                            continue
521:
522:                    tok = (NAME, token, spos, epos, line)
523:                    if token == 'async' and not stashed:
524:                        stashed = tok
525:                        continue
526:
527:                    if token in ('def', 'for'):
528:                        if (stashed
529:                                and stashed[0] == NAME
530:                                and stashed[1] == 'async'):
531:
532:                            if token == 'def':
533:                                async_def = True
534:                                async_def_indent = indents[-1]
535:
536:                            yield (ASYNC, stashed[1],
537:                                   stashed[2], stashed[3],
538:                                   stashed[4])
539:                            stashed = None
540:
541:                    if stashed:
542:                        yield stashed
543:                        stashed = None
544:
545:                    yield tok
546:                elif initial == '\\':                      # continued stmt
547:                    # This yield is new; needed for better idempotency:
548:                    if stashed:
549:                        yield stashed
550:                        stashed = None
551:                    yield (NL, token, spos, (lnum, pos), line)
552:                    continued = 1
553:                else:
554:                    if initial in '([{': parenlev = parenlev + 1
555:                    elif initial in ')]}': parenlev = parenlev - 1
556:                    if stashed:
557:                        yield stashed
558:                        stashed = None
559:                    yield (OP, token, spos, epos, line)
560:            else:
561:                yield (ERRORTOKEN, line[pos],
562:                           (lnum, pos), (lnum, pos+1), line)
563:                pos = pos + 1
564:
565:    if stashed:
566:        yield stashed
567:        stashed = None
568:
569:    for indent in indents[1:]:                 # pop remaining indent levels
570:        yield (DEDENT, '', (lnum, 0), (lnum, 0), '')
571:    yield (ENDMARKER, '', (lnum, 0), (lnum, 0), '')
572:
573:if __name__ == '__main__':                     # testing

Calling tool: grepTool
def get_parser_configs\(|def lib2to3_parse\(
Matched content:
769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:
821:def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
730:        for feature in {Feature.TRAILING_COMMA_IN_CALL, Feature.TRAILING_COMMA_IN_DEF}
731:        if supports_feature(versions, feature)
732:    }
733:    for current_line in lines.visit(src_node):
734:        for _ in range(after):
735:            dst_contents += str(empty_line)
736:        before, after = elt.maybe_empty_lines(current_line)
737:        for _ in range(before):
738:            dst_contents += str(empty_line)
739:        for line in split_line(
740:            current_line, line_length=mode.line_length, features=split_line_features
741:        ):
742:            dst_contents += str(line)
743:    return dst_contents
744:
745:
746:def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:
747:    """Return a tuple of (decoded_contents, encoding, newline).
748:
749:    `newline` is either CRLF or LF but `decoded_contents` is decoded with
750:    universal newlines (i.e. only contains LF).
751:    """
752:    srcbuf = io.BytesIO(src)
753:    encoding, lines = tokenize.detect_encoding(srcbuf.readline)
754:    if not lines:
755:        return "", encoding, "\n"
756:
757:    newline = "\r\n" if b"\r\n" == lines[0][-2:] else "\n"
758:    srcbuf.seek(0)
759:    with io.TextIOWrapper(srcbuf, encoding) as tiow:
760:        return tiow.read(), encoding, newline
761:
762:
763:@dataclass(frozen=True)
764:class ParserConfig:
765:    grammar: Grammar
766:    tokenizer_config: TokenizerConfig = TokenizerConfig()
767:
768:
769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:
770:    if not target_versions:
771:        # No target_version specified, so try all grammars.
772:        return [
773:            # Python 3.7+
774:            ParserConfig(
775:                pygram.python_grammar_no_print_statement_no_exec_statement,
776:                TokenizerConfig(async_is_reserved_keyword=True),
777:            ),
778:            # Python 3.0-3.6
779:            ParserConfig(
780:                pygram.python_grammar_no_print_statement_no_exec_statement,
781:                TokenizerConfig(async_is_reserved_keyword=False),
782:            ),
783:            # Python 2.7 with future print_function import
784:            ParserConfig(pygram.python_grammar_no_print_statement),
785:            # Python 2.7
786:            ParserConfig(pygram.python_grammar),
787:        ]
788:    elif all(version.is_python2() for version in target_versions):
789:        # Python 2-only code, so try Python 2 grammars.
790:        return [
791:            # Python 2.7 with future print_function import
792:            ParserConfig(pygram.python_grammar_no_print_statement),
793:            # Python 2.7
794:            ParserConfig(pygram.python_grammar),
795:        ]
796:    else:
797:        # Python 3-compatible code, so only try Python 3 grammar.
798:        configs = []
799:        # If we have to parse both, try to parse async as a keyword first
800:        if not supports_feature(target_versions, Feature.ASYNC_IS_VALID_IDENTIFIER):
801:            # Python 3.7+
802:            configs.append(
803:                ParserConfig(
804:                    pygram.python_grammar_no_print_statement_no_exec_statement,
805:                    TokenizerConfig(async_is_reserved_keyword=True),
806:                )
807:            )
808:        if not supports_feature(target_versions, Feature.ASYNC_IS_RESERVED_KEYWORD):
809:            # Python 3.0-3.6
810:            configs.append(
811:                ParserConfig(
812:                    pygram.python_grammar_no_print_statement_no_exec_statement,
813:                    TokenizerConfig(async_is_reserved_keyword=False),
814:                )
815:            )
816:        # At least one of the above branches must have been taken, because every Python
817:        # version has exactly one of the two 'ASYNC_IS_*' flags
818:        return configs
819:
820:
821:def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:
822:    """Given a string with source, return the lib2to3 Node."""
823:    if src_txt[-1:] != "\n":
824:        src_txt += "\n"
825:
826:    for parser_config in get_parser_configs(set(target_versions)):
827:        drv = driver.Driver(
828:            parser_config.grammar,
829:            pytree.convert,
830:            tokenizer_config=parser_config.tokenizer_config,
831:        )
832:        try:
833:            result = drv.parse_string(src_txt, True)
834:            break
835:
836:        except ParseError as pe:
837:            lineno, column = pe.context[1]
838:            lines = src_txt.splitlines()
839:            try:
840:                faulty_line = lines[lineno - 1]
841:            except IndexError:
842:                faulty_line = "<line number missing in source>"
843:            exc = InvalidInput(f"Cannot parse: {lineno}:{column}: {faulty_line}")
844:    else:
845:        raise exc from None
846:
847:    if isinstance(result, Leaf):
848:        result = Node(syms.file_input, [result])
849:    return result

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.pyi
1:# Stubs for lib2to3.pgen2.driver (Python 3.6)
2:
3:import os
4:import sys
5:from typing import Any, Callable, IO, Iterable, List, Optional, Text, Tuple, Union
6:
7:from logging import Logger
8:from blib2to3.pytree import _Convert, _NL
9:from blib2to3.pgen2 import _Path
10:from blib2to3.pgen2.grammar import Grammar
11:from blib2to3.pgen2.tokenize import TokenizerConfig
12:
13:
14:class Driver:
15:    grammar: Grammar
16:    logger: Logger
17:    convert: _Convert
18:    def __init__(
19:        self,
20:        grammar: Grammar,
21:        convert: Optional[_Convert] = ...,
22:        logger: Optional[Logger] = ...,
23:        tokenizer_config: TokenizerConfig = ...
24:    ) -> None: ...
25:    def parse_tokens(self, tokens: Iterable[Any], debug: bool = ...) -> _NL: ...
26:    def parse_stream_raw(self, stream: IO[Text], debug: bool = ...) -> _NL: ...
27:    def parse_stream(self, stream: IO[Text], debug: bool = ...) -> _NL: ...
28:    def parse_file(self, filename: _Path, encoding: Optional[Text] = ..., debug: bool = ...) -> _NL: ...
29:    def parse_string(self, text: Text, debug: bool = ...) -> _NL: ...
30:
31:def load_grammar(gt: Text = ..., gp: Optional[Text] = ..., save: bool = ..., force: bool = ..., logger: Optional[Logger] = ...) -> Grammar: ...

Calling tool: grepTool
from blib2to3\.pgen2\.tokenize import TokenizerConfig
Matched content:
51:from blib2to3.pgen2.tokenize import TokenizerConfig

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: In blib2to3/pgen2/driver.py, parse_stream_raw() and parse_string() both call tokenize.generate_tokens(..., config=self.tokenizer_config) (lines 105108, 119125). parse_stream() simply delegates to parse_stream_raw(), and parse_file() calls parse_stream(), ensuring the tokenizer config is applied across file parsing.

Todo item 7 of 19 is correct
description: 'Verify `driver.Driver.parse_stream_raw()` and `driver.Driver.parse_string()` call `tokenize.generate_tokens(..., config=self.tokenizer_config)`; ensure `parse_file()` uses `parse_stream()` and thus the config path is used.'

Validating todo item 8 of 19
description: 'Check `blib2to3/pgen2/tokenize.py` defines `TokenizerConfig` as a frozen attr dataclass with field `async_is_reserved_keyword: bool = False` and is imported where used.'

Calling tool: grepTool
TokenizerConfig
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:51:from blib2to3.pgen2.tokenize import TokenizerConfig
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:766:    tokenizer_config: TokenizerConfig = TokenizerConfig()
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:776:                TokenizerConfig(async_is_reserved_keyword=True),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:781:                TokenizerConfig(async_is_reserved_keyword=False),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:805:                    TokenizerConfig(async_is_reserved_keyword=True),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:813:                    TokenizerConfig(async_is_reserved_keyword=False),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:142:class TokenizerConfig:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py:37:        tokenizer_config=tokenize.TokenizerConfig(),
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.pyi:11:from blib2to3.pgen2.tokenize import TokenizerConfig
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.pyi:23:        tokenizer_config: TokenizerConfig = ...
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi:14:class TokenizerConfig:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
120:)
121:
122:endprogs = {"'": re.compile(Single), '"': re.compile(Double),
123:            "'''": single3prog, '"""': double3prog,
124:            **{f"{prefix}'''": single3prog for prefix in _strprefixes},
125:            **{f'{prefix}"""': double3prog for prefix in _strprefixes},
126:            **{prefix: None for prefix in _strprefixes}}
127:
128:triple_quoted = (
129:    {"'''", '"""'} |
130:    {f"{prefix}'''" for prefix in _strprefixes} |
131:    {f'{prefix}"""' for prefix in _strprefixes}
132:)
133:single_quoted = (
134:    {"'", '"'} |
135:    {f"{prefix}'" for prefix in _strprefixes} |
136:    {f'{prefix}"' for prefix in _strprefixes}
137:)
138:
139:tabsize = 8
140:
141:@dataclass(frozen=True)
142:class TokenizerConfig:
143:    async_is_reserved_keyword: bool = False
144:
145:class TokenError(Exception): pass
146:
147:class StopTokenizing(Exception): pass
148:
149:def printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line): # for testing
150:    (srow, scol) = xxx_todo_changeme
151:    (erow, ecol) = xxx_todo_changeme1
152:    print("%d,%d-%d,%d:\t%s\t%s" % \
153:        (srow, scol, erow, ecol, tok_name[type], repr(token)))
154:
155:def tokenize(readline, tokeneater=printtoken):
156:    """
157:    The tokenize() function accepts two parameters: one representing the
158:    input stream, and one providing an output mechanism for tokenize().
159:
160:    The first parameter, readline, must be a callable object which provides
161:    the same interface as the readline() method of built-in file objects.
162:    Each call to the function should return one line of input as a string.
163:
164:    The second parameter, tokeneater, must also be a callable object. It is
165:    called once for each token, with five arguments, corresponding to the
166:    tuples generated by generate_tokens().
167:    """
168:    try:
169:        tokenize_loop(readline, tokeneater)
170:    except StopTokenizing:
171:        pass
172:
173:# backwards compatible interface
174:def tokenize_loop(readline, tokeneater):
175:    for token_info in generate_tokens(readline):
176:        tokeneater(*token_info)
177:
178:class Untokenizer:
179:
180:    def __init__(self):
181:        self.tokens = []
182:        self.prev_row = 1
183:        self.prev_col = 0
184:
185:    def add_whitespace(self, start):
186:        row, col = start
187:        assert row <= self.prev_row
188:        col_offset = col - self.prev_col
189:        if col_offset:
190:            self.tokens.append(" " * col_offset)
191:
192:    def untokenize(self, iterable):
193:        for t in iterable:
194:            if len(t) == 2:
195:                self.compat(t, iterable)
196:                break
197:            tok_type, token, start, end, line = t
198:            self.add_whitespace(start)
199:            self.tokens.append(token)

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
330:
331:    Round-trip invariant for limited intput:
332:        # Output text will tokenize the back to the input
333:        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
334:        newcode = untokenize(t1)
335:        readline = iter(newcode.splitlines(1)).next
336:        t2 = [tok[:2] for tokin generate_tokens(readline)]
337:        assert t1 == t2
338:    """
339:    ut = Untokenizer()
340:    return ut.untokenize(iterable)
341:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
343:    """
344:    The generate_tokens() generator requires one argument, readline, which
345:    must be a callable object which provides the same interface as the
346:    readline() method of built-in file objects. Each call to the function
347:    should return one line of input as a string.  Alternately, readline
348:    can be a callable function terminating with StopIteration:
349:        readline = open(myfile).next    # Example of alternate readline
350:
351:    The generator produces 5-tuples with these members: the token type; the
352:    token string; a 2-tuple (srow, scol) of ints specifying the row and
353:    column where the token begins in the source; a 2-tuple (erow, ecol) of
354:    ints specifying the row and column where the token ends in the source;
355:    and the line on which the token was found. The line passed is the
356:    logical line; continuation lines are included.
357:    """
358:    lnum = parenlev = continued = 0
359:    numchars = '0123456789'
360:    contstr, needcont = '', 0
361:    contline = None
362:    indents = [0]
363:
364:    # If we know we're parsing 3.7+, we can unconditionally parse `async` and
365:    # `await` as keywords.
366:    async_is_reserved_keyword = config.async_is_reserved_keyword
367:    # 'stashed' and 'async_*' are used for async/await parsing
368:    stashed = None
369:    async_def = False
370:    async_def_indent = 0
371:    async_def_nl = False
372:
373:    while 1:                                   # loop over lines in stream
374:        try:
375:            line = readline()
376:        except StopIteration:
377:            line = ''
378:        lnum = lnum + 1
379:        pos, max = 0, len(line)
380:
381:        if contstr:                            # continued string
382:            if not line:
383:                raise TokenError("EOF in multi-line string", strstart)
384:            endmatch = endprog.match(line)
385:            if endmatch:
386:                pos = end = endmatch.end(0)
387:                yield (STRING, contstr + line[:end],
388:                       strstart, (lnum, end), contline + line)
389:                contstr, needcont = '', 0
390:                contline = None
391:            elif needcont and line[-2:] != '\\\n' and line[-3:] != '\\\r\n':
392:                yield (ERRORTOKEN, contstr + line,
393:                           strstart, (lnum, len(line)), contline)
394:                contstr = ''
395:                contline = None
396:                continue
397:            else:
398:                contstr = contstr + line
399:                contline = contline + line
400:                continue
401:
402:        elif parenlev == 0 and not continued:  # new statement
403:            if not line: break
404:            column = 0
405:            while pos < max:                   # measure leading whitespace
406:                if line[pos] == ' ': column = column + 1
407:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize
408:                elif line[pos] == '\f': column = 0
409:                else: break

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
1:# Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006 Python Software Foundation.
2:# All rights reserved.
3:
4:"""Tokenization help for Python programs.
5:
6:generate_tokens(readline) is a generator that breaks a stream of
7:text into Python tokens.  It accepts a readline-like method which is called
8:repeatedly to get the next line of input (or "" for EOF).  It generates
9:5-tuples with these members:
10:
11:    the token type (see token.py)
12:    the token (a string)
13:    the starting (row, column) indices of the token (a 2-tuple of ints)
14:    the ending (row, column) indices of the token (a 2-tuple of ints)
15:    the original line (string)
16:
17:It is designed to match the working of the Python tokenizer exactly, except
18:that it produces COMMENT tokens for comments and gives type OP for all
19:operators
20:
21:Older entry points
22:    tokenize_loop(readline, tokeneater)
23:    tokenize(readline, tokeneater=printtoken)
24:are the same, except instead of generating tokens, tokeneater is a callback
25:function to which the 5 fields described above are passed as 5 arguments,
26:each time a new token is found."""
27:
28:__author__ = 'Ka-Ping Yee <ping@lfw.org>'
29:__credits__ = \
30:    'GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro'
31:
32:import re
33:from codecs import BOM_UTF8, lookup
34:from attr import dataclass
35:from blib2to3.pgen2.token import *
36:
37:from . import token
38:__all__ = [x for x in dir(token) if x[0] != '_'] + ["tokenize",
39:           "generate_tokens", "untokenize"]
40:del token
41:
42:try:
43:    bytes
44:except NameError:
45:    # Support bytes type in Python <= 2.5, so 2to3 turns itself into
46:    # valid Python 3 code.
47:    bytes = str
48:
49:def group(*choices): return '(' + '|'.join(choices) + ')'
50:def any(*choices): return group(*choices) + '*'
51:def maybe(*choices): return group(*choices) + '?'
52:def _combinations(*l):
53:    return set(
54:        x + y for x in l for y in l + ("",) if x.casefold() != y.casefold()
55:    )
56:
57:Whitespace = r'[ \f\t]*'
58:Comment = r'#[^\r\n]*'
59:Ignore = Whitespace + any(r'\\\r?\n' + Whitespace) + maybe(Comment)
60:Name = r'\w+'  # this is invalid but it's fine because Name comes after Number in all groups

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
40:from attr import dataclass, evolve, Factory
41:import click
42:import toml
43:from typed_ast import ast3, ast27
44:
45:# lib2to3 fork
46:from blib2to3.pytree import Node, Leaf, type_repr
47:from blib2to3 import pygram, pytree
48:from blib2to3.pgen2 import driver, token
49:from blib2to3.pgen2.grammar import Grammar
50:from blib2to3.pgen2.parse import ParseError
51:from blib2to3.pgen2.tokenize import TokenizerConfig
52:
53:
54:__version__ = "19.3b0"
55:DEFAULT_LINE_LENGTH = 88
56:DEFAULT_EXCLUDES = (
57:    r"/(\.eggs|\.git|\.hg|\.mypy_cache|\.nox|\.tox|\.venv|_build|buck-out|build|dist)/"
58:)
59:DEFAULT_INCLUDES = r"\.pyi?$"
60:CACHE_DIR = Path(user_cache_dir("black", version=__version__))
61:
62:
63:# types
64:FileContent = str
65:Encoding = str
66:NewLine = str
67:Depth = int
68:NodeType = int
69:LeafID = int

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
740:            current_line, line_length=mode.line_length, features=split_line_features
741:        ):
742:            dst_contents += str(line)
743:    return dst_contents
744:
745:
746:def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:
747:    """Return a tuple of (decoded_contents, encoding, newline).
748:
749:    `newline` is either CRLF or LF but `decoded_contents` is decoded with
750:    universal newlines (i.e. only contains LF).
751:    """
752:    srcbuf = io.BytesIO(src)
753:    encoding, lines = tokenize.detect_encoding(srcbuf.readline)
754:    if not lines:
755:        return "", encoding, "\n"
756:
757:    newline = "\r\n" if b"\r\n" == lines[0][-2:] else "\n"
758:    srcbuf.seek(0)
759:    with io.TextIOWrapper(srcbuf, encoding) as tiow:
760:        return tiow.read(), encoding, newline
761:
762:
763:@dataclass(frozen=True)
764:class ParserConfig:
765:    grammar: Grammar
766:    tokenizer_config: TokenizerConfig = TokenizerConfig()
767:
768:
769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:
770:    if not target_versions:
771:        # No target_version specified, so try all grammars.
772:        return [
773:            # Python 3.7+
774:            ParserConfig(
775:                pygram.python_grammar_no_print_statement_no_exec_statement,
776:                TokenizerConfig(async_is_reserved_keyword=True),
777:            ),
778:            # Python 3.0-3.6
779:            ParserConfig(
780:                pygram.python_grammar_no_print_statement_no_exec_statement,
781:                TokenizerConfig(async_is_reserved_keyword=False),
782:            ),
783:            # Python 2.7 with future print_function import
784:            ParserConfig(pygram.python_grammar_no_print_statement),
785:            # Python 2.7
786:            ParserConfig(pygram.python_grammar),
787:        ]
788:    elif all(version.is_python2() for version in target_versions):
789:        # Python 2-only code, so try Python 2 grammars.
790:        return [
791:            # Python 2.7 with future print_function import
792:            ParserConfig(pygram.python_grammar_no_print_statement),
793:            # Python 2.7
794:            ParserConfig(pygram.python_grammar),
795:        ]
796:    else:
797:        # Python 3-compatible code, so only try Python 3 grammar.
798:        configs = []
799:        # If we have to parse both, try to parse async as a keyword first
800:        if not supports_feature(target_versions, Feature.ASYNC_IS_VALID_IDENTIFIER):
801:            # Python 3.7+
802:            configs.append(
803:                ParserConfig(
804:                    pygram.python_grammar_no_print_statement_no_exec_statement,
805:                    TokenizerConfig(async_is_reserved_keyword=True),
806:                )
807:            )
808:        if not supports_feature(target_versions, Feature.ASYNC_IS_RESERVED_KEYWORD):
809:            # Python 3.0-3.6
810:            configs.append(
811:                ParserConfig(
812:                    pygram.python_grammar_no_print_statement_no_exec_statement,
813:                    TokenizerConfig(async_is_reserved_keyword=False),
814:                )
815:            )
816:        # At least one of the above branches must have been taken, because every Python
817:        # version has exactly one of the two 'ASYNC_IS_*' flags
818:        return configs
819:
820:
821:def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:
822:    """Given a string with source, return the lib2to3 Node."""
823:    if src_txt[-1:] != "\n":
824:        src_txt += "\n"
825:
826:    for parser_config in get_parser_configs(set(target_versions)):
827:        drv = driver.Driver(
828:            parser_config.grammar,
829:            pytree.convert,
830:            tokenizer_config=parser_config.tokenizer_config,
831:        )
832:        try:
833:            result = drv.parse_string(src_txt, True)
834:            break
835:
836:        except ParseError as pe:
837:            lineno, column = pe.context[1]
838:            lines = src_txt.splitlines()
839:            try:
840:                faulty_line = lines[lineno - 1]
841:            except IndexError:
842:                faulty_line = "<line number missing in source>"
843:            exc = InvalidInput(f"Cannot parse: {lineno}:{column}: {faulty_line}")
844:    else:
845:        raise exc from None
846:
847:    if isinstance(result, Leaf):
848:        result = Node(syms.file_input, [result])
849:    return result
850:
851:
852:def lib2to3_unparse(node: Node) -> str:
853:    """Given a lib2to3 node, return its string representation."""
854:    code = str(node)
855:    return code
856:
857:
858:T = TypeVar("T")
859:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(
33:        self,
34:        grammar,
35:        convert=None,
36:        logger=None,
37:        tokenizer_config=tokenize.TokenizerConfig(),
38:    ):
39:        self.grammar = grammar
40:        if logger is None:
41:            logger = logging.getLogger(__name__)
42:        self.logger = logger
43:        self.convert = convert
44:        self.tokenizer_config = tokenizer_config
45:
46:    def parse_tokens(self, tokens, debug=False):
47:        """Parse a series of tokens and return the syntax tree."""
48:        # XXX Move the prefix computation into a wrapper around tokenize.
49:        p = parse.Parser(self.grammar, self.convert)
50:        p.setup()
51:        lineno = 1
52:        column = 0
53:        indent_columns = []
54:        type = value = start = end = line_text = None
55:        prefix = ""
56:        for quintuple in tokens:
57:            type, value, start, end, line_text = quintuple
58:            if start != (lineno, column):
59:                assert (lineno, column) <= start, ((lineno, column), start)

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: TokenizerConfig is defined in blib2to3/pgen2/tokenize.py as an attrs dataclass with frozen=True and a single field async_is_reserved_keyword: bool = False, and black.py imports it via `from blib2to3.pgen2.tokenize import TokenizerConfig`. The driver uses it via tokenize.TokenizerConfig(), so it is correctly imported/accessible where used.

Todo item 8 of 19 is correct
description: 'Check `blib2to3/pgen2/tokenize.py` defines `TokenizerConfig` as a frozen attr dataclass with field `async_is_reserved_keyword: bool = False` and is imported where used.'

Validating todo item 9 of 19
description: 'Confirm `generate_tokens` signature in `tokenize.py` is updated to accept `config: TokenizerConfig = TokenizerConfig()` and that it reads `config.async_is_reserved_keyword` into a local flag.'

Calling tool: grepTool
def generate_tokens\(
Matched content:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
330:
331:    Round-trip invariant for limited intput:
332:        # Output text will tokenize the back to the input
333:        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
334:        newcode = untokenize(t1)
335:        readline = iter(newcode.splitlines(1)).next
336:        t2 = [tok[:2] for tokin generate_tokens(readline)]
337:        assert t1 == t2
338:    """
339:    ut = Untokenizer()
340:    return ut.untokenize(iterable)
341:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
343:    """
344:    The generate_tokens() generator requires one argument, readline, which
345:    must be a callable object which provides the same interface as the
346:    readline() method of built-in file objects. Each call to the function
347:    should return one line of input as a string.  Alternately, readline
348:    can be a callable function terminating with StopIteration:
349:        readline = open(myfile).next    # Example of alternate readline
350:
351:    The generator produces 5-tuples with these members: the token type; the
352:    token string; a 2-tuple (srow, scol) of ints specifying the row and
353:    column where the token begins in the source; a 2-tuple (erow, ecol) of
354:    ints specifying the row and column where the token ends in the source;
355:    and the line on which the token was found. The line passed is the
356:    logical line; continuation lines are included.
357:    """
358:    lnum = parenlev = continued = 0
359:    numchars = '0123456789'
360:    contstr, needcont = '', 0
361:    contline = None
362:    indents = [0]
363:
364:    # If we know we're parsing 3.7+, we can unconditionally parse `async` and
365:    # `await` as keywords.
366:    async_is_reserved_keyword = config.async_is_reserved_keyword
367:    # 'stashed' and 'async_*' are used for async/await parsing
368:    stashed = None
369:    async_def = False
370:    async_def_indent = 0
371:    async_def_nl = False
372:
373:    while 1:                                   # loop over lines in stream
374:        try:
375:            line = readline()
376:        except StopIteration:
377:            line = ''
378:        lnum = lnum + 1
379:        pos, max = 0, len(line)
380:
381:        if contstr:                            # continued string
382:            if not line:
383:                raise TokenError("EOF in multi-line string", strstart)
384:            endmatch = endprog.match(line)
385:            if endmatch:
386:                pos = end = endmatch.end(0)
387:                yield (STRING, contstr + line[:end],
388:                       strstart, (lnum, end), contline + line)
389:                contstr, needcont = '', 0
390:                contline = None
391:            elif needcont and line[-2:] != '\\\n' and line[-3:] != '\\\r\n':
392:                yield (ERRORTOKEN, contstr + line,
393:                           strstart, (lnum, len(line)), contline)
394:                contstr = ''
395:                contline = None
396:                continue
397:            else:
398:                contstr = contstr + line
399:                contline = contline + line
400:                continue
401:
402:        elif parenlev == 0 and not continued:  # new statement
403:            if not line: break
404:            column = 0
405:            while pos < max:                   # measure leading whitespace
406:                if line[pos] == ' ': column = column + 1
407:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize
408:                elif line[pos] == '\f': column = 0
409:                else: break

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: In blib2to3/pgen2/tokenize.py, generate_tokens is defined as def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()): (line 342) and it assigns async_is_reserved_keyword = config.async_is_reserved_keyword (lines 364-367), satisfying the requirement.

Todo item 9 of 19 is correct
description: 'Confirm `generate_tokens` signature in `tokenize.py` is updated to accept `config: TokenizerConfig = TokenizerConfig()` and that it reads `config.async_is_reserved_keyword` into a local flag.'

Validating todo item 10 of 19
description: 'Inspect tokenization logic for identifiers in `generate_tokens`: when `token in ('async','await')` it yields `ASYNC`/`AWAIT` tokens if `async_is_reserved_keyword` OR `async_def` is true; otherwise they remain NAME. Ensure existing `async def`/`async for` stashing logic remains intact and resets on dedent/newline as before.'

Calling tool: grepTool
def generate_tokens\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi:32:def generate_tokens(

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
330:
331:    Round-trip invariant for limited intput:
332:        # Output text will tokenize the back to the input
333:        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
334:        newcode = untokenize(t1)
335:        readline = iter(newcode.splitlines(1)).next
336:        t2 = [tok[:2] for tokin generate_tokens(readline)]
337:        assert t1 == t2
338:    """
339:    ut = Untokenizer()
340:    return ut.untokenize(iterable)
341:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
343:    """
344:    The generate_tokens() generator requires one argument, readline, which
345:    must be a callable object which provides the same interface as the
346:    readline() method of built-in file objects. Each call to the function
347:    should return one line of input as a string.  Alternately, readline
348:    can be a callable function terminating with StopIteration:
349:        readline = open(myfile).next    # Example of alternate readline
350:
351:    The generator produces 5-tuples with these members: the token type; the
352:    token string; a 2-tuple (srow, scol) of ints specifying the row and
353:    column where the token begins in the source; a 2-tuple (erow, ecol) of
354:    ints specifying the row and column where the token ends in the source;
355:    and the line on which the token was found. The line passed is the
356:    logical line; continuation lines are included.
357:    """
358:    lnum = parenlev = continued = 0
359:    numchars = '0123456789'
360:    contstr, needcont = '', 0
361:    contline = None
362:    indents = [0]
363:
364:    # If we know we're parsing 3.7+, we can unconditionally parse `async` and
365:    # `await` as keywords.
366:    async_is_reserved_keyword = config.async_is_reserved_keyword
367:    # 'stashed' and 'async_*' are used for async/await parsing
368:    stashed = None
369:    async_def = False
370:    async_def_indent = 0
371:    async_def_nl = False
372:
373:    while 1:                                   # loop over lines in stream
374:        try:
375:            line = readline()
376:        except StopIteration:
377:            line = ''
378:        lnum = lnum + 1
379:        pos, max = 0, len(line)
380:
381:        if contstr:                            # continued string
382:            if not line:
383:                raise TokenError("EOF in multi-line string", strstart)
384:            endmatch = endprog.match(line)
385:            if endmatch:
386:                pos = end = endmatch.end(0)
387:                yield (STRING, contstr + line[:end],
388:                       strstart, (lnum, end), contline + line)
389:                contstr, needcont = '', 0
390:                contline = None
391:            elif needcont and line[-2:] != '\\\n' and line[-3:] != '\\\r\n':
392:                yield (ERRORTOKEN, contstr + line,
393:                           strstart, (lnum, len(line)), contline)
394:                contstr = ''
395:                contline = None
396:                continue
397:            else:
398:                contstr = contstr + line
399:                contline = contline + line
400:                continue
401:
402:        elif parenlev == 0 and not continued:  # new statement
403:            if not line: break
404:            column = 0
405:            while pos < max:                   # measure leading whitespace
406:                if line[pos] == ' ': column = column + 1
407:                elif line[pos] == '\t': column = (column//tabsize + 1)*tabsize
408:                elif line[pos] == '\f': column = 0
409:                else: break

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
410:                pos = pos + 1
411:            if pos == max: break
412:
413:            if stashed:
414:                yield stashed
415:                stashed = None
416:
417:            if line[pos] in '\r\n':            # skip blank lines
418:                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)
419:                continue
420:
421:            if line[pos] == '#':               # skip comments
422:                comment_token = line[pos:].rstrip('\r\n')
423:                nl_pos = pos + len(comment_token)
424:                yield (COMMENT, comment_token,
425:                        (lnum, pos), (lnum, pos + len(comment_token)), line)
426:                yield (NL, line[nl_pos:],
427:                        (lnum, nl_pos), (lnum, len(line)), line)
428:                continue
429:
430:            if column > indents[-1]:           # count indents
431:                indents.append(column)
432:                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)
433:
434:            while column < indents[-1]:        # count dedents
435:                if column not in indents:
436:                    raise IndentationError(
437:                        "unindent does not match any outer indentation level",
438:                        ("<tokenize>", lnum, pos, line))
439:                indents = indents[:-1]
440:
441:                if async_def and async_def_indent >= indents[-1]:
442:                    async_def = False
443:                    async_def_nl = False
444:                    async_def_indent = 0
445:
446:                yield (DEDENT, '', (lnum, pos), (lnum, pos), line)
447:
448:            if async_def and async_def_nl and async_def_indent >= indents[-1]:
449:                async_def = False
450:                async_def_nl = False
451:                async_def_indent = 0
452:
453:        else:                                  # continued statement
454:            if not line:
455:                raise TokenError("EOF in multi-line statement", (lnum, 0))
456:            continued = 0
457:
458:        while pos < max:
459:            pseudomatch = pseudoprog.match(line, pos)
460:            if pseudomatch:                                # scan for tokens
461:                start, end = pseudomatch.span(1)
462:                spos, epos, pos = (lnum, start), (lnum, end), end
463:                token, initial = line[start:end], line[start]
464:
465:                if initial in numchars or \
466:                   (initial == '.' and token != '.'):      # ordinary number
467:                    yield (NUMBER, token, spos, epos, line)
468:                elif initial in '\r\n':
469:                    newline = NEWLINE
470:                    if parenlev > 0:
471:                        newline = NL
472:                    elif async_def:
473:                        async_def_nl = True
474:                    if stashed:
475:                        yield stashed
476:                        stashed = None
477:                    yield (newline, token, spos, epos, line)
478:
479:                elif initial == '#':
480:                    assert not token.endswith("\n")
481:                    if stashed:
482:                        yield stashed
483:                        stashed = None
484:                    yield (COMMENT, token, spos, epos, line)
485:                elif token in triple_quoted:
486:                    endprog = endprogs[token]
487:                    endmatch = endprog.match(line, pos)
488:                    if endmatch:                           # all on one line
489:                        pos = endmatch.end(0)

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
490:                        token = line[start:pos]
491:                        if stashed:
492:                            yield stashed
493:                            stashed = None
494:                        yield (STRING, token, spos, (lnum, pos), line)
495:                    else:
496:                        strstart = (lnum, start)           # multiple lines
497:                        contstr = line[start:]
498:                        contline = line
499:                        break
500:                elif initial in single_quoted or \
501:                    token[:2] in single_quoted or \
502:                    token[:3] in single_quoted:
503:                    if token[-1] == '\n':                  # continued string
504:                        strstart = (lnum, start)
505:                        endprog = (endprogs[initial] or endprogs[token[1]] or
506:                                   endprogs[token[2]])
507:                        contstr, needcont = line[start:], 1
508:                        contline = line
509:                        break
510:                    else:                                  # ordinary string
511:                        if stashed:
512:                            yield stashed
513:                            stashed = None
514:                        yield (STRING, token, spos, epos, line)
515:                elif initial.isidentifier():               # ordinary name
516:                    if token in ('async', 'await'):
517:                        if async_is_reserved_keyword or async_def:
518:                            yield (ASYNC if token == 'async' else AWAIT,
519:                                   token, spos, epos, line)
520:                            continue
521:
522:                    tok = (NAME, token, spos, epos, line)
523:                    if token == 'async' and not stashed:
524:                        stashed = tok
525:                        continue
526:
527:                    if token in ('def', 'for'):
528:                        if (stashed
529:                                and stashed[0] == NAME
530:                                and stashed[1] == 'async'):
531:
532:                            if token == 'def':
533:                                async_def = True
534:                                async_def_indent = indents[-1]
535:
536:                            yield (ASYNC, stashed[1],
537:                                   stashed[2], stashed[3],
538:                                   stashed[4])
539:                            stashed = None
540:
541:                    if stashed:
542:                        yield stashed
543:                        stashed = None
544:
545:                    yield tok
546:                elif initial == '\\':                      # continued stmt
547:                    # This yield is new; needed for better idempotency:
548:                    if stashed:
549:                        yield stashed
550:                        stashed = None
551:                    yield (NL, token, spos, (lnum, pos), line)
552:                    continued = 1
553:                else:
554:                    if initial in '([{': parenlev = parenlev + 1
555:                    elif initial in ')]}': parenlev = parenlev - 1
556:                    if stashed:
557:                        yield stashed
558:                        stashed = None
559:                    yield (OP, token, spos, epos, line)
560:            else:
561:                yield (ERRORTOKEN, line[pos],
562:                           (lnum, pos), (lnum, pos+1), line)
563:                pos = pos + 1
564:
565:    if stashed:
566:        yield stashed
567:        stashed = None
568:
569:    for indent in indents[1:]:                 # pop remaining indent levels

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
570:        yield (DEDENT, '', (lnum, 0), (lnum, 0), '')
571:    yield (ENDMARKER, '', (lnum, 0), (lnum, 0), '')
572:
573:if __name__ == '__main__':                     # testing
574:    import sys
575:    if len(sys.argv) > 1: tokenize(open(sys.argv[1]).readline)
576:    else: tokenize(sys.stdin.readline)

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: In generate_tokens, when token is 'async'/'await' it yields ASYNC/AWAIT if config.async_is_reserved_keyword or async_def is true; otherwise it falls through to NAME. The async stashing logic (stashed, async_def, async_def_indent/nl) remains intact and resets on newline/dedent as before.

Todo item 10 of 19 is correct
description: 'Inspect tokenization logic for identifiers in `generate_tokens`: when `token in ('async','await')` it yields `ASYNC`/`AWAIT` tokens if `async_is_reserved_keyword` OR `async_def` is true; otherwise they remain NAME. Ensure existing `async def`/`async for` stashing logic remains intact and resets on dedent/newline as before.'

Validating todo item 11 of 19
description: 'Check all call sites of `generate_tokens` across the repo: updated calls pass `config` where needed (driver), and legacy calls (e.g., in `pgen.py` and `tokenize_loop`) still compile using the default parameter.'

Calling tool: grepTool
generate_tokens\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:6:generate_tokens(readline) is a generator that breaks a stream of
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:166:    tuples generated by generate_tokens().
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:175:    for token_info in generate_tokens(readline):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:333:        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:336:        t2 = [tok[:2] for tokin generate_tokens(readline)]
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:344:    The generate_tokens() generator requires one argument, readline, which
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py:107:        tokens = tokenize.generate_tokens(stream.readline, config=self.tokenizer_config)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py:121:        tokens = tokenize.generate_tokens(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/pgen.py:19:        self.generator = tokenize.generate_tokens(stream.readline)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi:32:def generate_tokens(

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/pgen.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Pgen imports
5:from . import grammar, token, tokenize
6:
7:class PgenGrammar(grammar.Grammar):
8:    pass
9:
10:class ParserGenerator(object):
11:
12:    def __init__(self, filename, stream=None):
13:        close_stream = None
14:        if stream is None:
15:            stream = open(filename)
16:            close_stream = stream.close
17:        self.filename = filename
18:        self.stream = stream
19:        self.generator = tokenize.generate_tokens(stream.readline)
20:        self.gettoken() # Initialize lookahead
21:        self.dfas, self.startsymbol = self.parse()
22:        if close_stream is not None:
23:            close_stream()
24:        self.first = {} # map from symbol name to set of tokens
25:        self.addfirstsets()
26:
27:    def make_grammar(self):
28:        c = PgenGrammar()
29:        names = list(self.dfas.keys())
30:        names.sort()
31:        names.remove(self.startsymbol)
32:        names.insert(0, self.startsymbol)
33:        for name in names:
34:            i = 256 + len(c.symbol2number)
35:            c.symbol2number[name] = i
36:            c.number2symbol[i] = name
37:        for name in names:
38:            dfa = self.dfas[name]
39:            states = []
40:            for state in dfa:
41:                arcs = []
42:                for label, next in sorted(state.arcs.items()):
43:                    arcs.append((self.make_label(c, label), dfa.index(next)))
44:                if state.isfinal:
45:                    arcs.append((0, dfa.index(state)))
46:                states.append(arcs)
47:            c.states.append(states)
48:            c.dfas[c.symbol2number[name]] = (states, self.make_first(c, name))
49:        c.start = c.symbol2number[self.startsymbol]
50:        return c
51:
52:    def make_first(self, c, name):
53:        rawfirst = self.first[name]
54:        first = {}
55:        for label in sorted(rawfirst):
56:            ilabel = self.make_label(c, label)
57:            ##assert ilabel not in first # XXX failed on <> ... !=
58:            first[ilabel] = 1
59:        return first
60:
61:    def make_label(self, c, label):
62:        # XXX Maybe this should be a method on a subclass of converter?
63:        ilabel = len(c.labels)
64:        if label[0].isalpha():
65:            # Either a symbol name or a named token
66:            if label in c.symbol2number:
67:                # A symbol name (a non-terminal)
68:                if label in c.symbol2label:
69:                    return c.symbol2label[label]
70:                else:
71:                    c.labels.append((c.symbol2number[label], None))
72:                    c.symbol2label[label] = ilabel
73:                    return ilabel
74:            else:
75:                # A named token (NAME, NUMBER, STRING)
76:                itoken = getattr(token, label, None)
77:                assert isinstance(itoken, int), label
78:                assert itoken in token.tok_name, label
79:                if itoken in c.tokens:
80:                    return c.tokens[itoken]

Calling tool: grepTool
tokenize_loop
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:22:    tokenize_loop(readline, tokeneater)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:169:        tokenize_loop(readline, tokeneater)
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py:174:def tokenize_loop(readline, tokeneater):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
1:# Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006 Python Software Foundation.
2:# All rights reserved.
3:
4:"""Tokenization help for Python programs.
5:
6:generate_tokens(readline) is a generator that breaks a stream of
7:text into Python tokens.  It accepts a readline-like method which is called
8:repeatedly to get the next line of input (or "" for EOF).  It generates
9:5-tuples with these members:
10:
11:    the token type (see token.py)
12:    the token (a string)
13:    the starting (row, column) indices of the token (a 2-tuple of ints)
14:    the ending (row, column) indices of the token (a 2-tuple of ints)
15:    the original line (string)
16:
17:It is designed to match the working of the Python tokenizer exactly, except
18:that it produces COMMENT tokens for comments and gives type OP for all
19:operators
20:
21:Older entry points
22:    tokenize_loop(readline, tokeneater)
23:    tokenize(readline, tokeneater=printtoken)
24:are the same, except instead of generating tokens, tokeneater is a callback
25:function to which the 5 fields described above are passed as 5 arguments,
26:each time a new token is found."""
27:
28:__author__ = 'Ka-Ping Yee <ping@lfw.org>'
29:__credits__ = \
30:    'GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro'
31:
32:import re
33:from codecs import BOM_UTF8, lookup
34:from attr import dataclass
35:from blib2to3.pgen2.token import *
36:
37:from . import token
38:__all__ = [x for x in dir(token) if x[0] != '_'] + ["tokenize",
39:           "generate_tokens", "untokenize"]
40:del token
41:
42:try:
43:    bytes
44:except NameError:
45:    # Support bytes type in Python <= 2.5, so 2to3 turns itself into
46:    # valid Python 3 code.
47:    bytes = str
48:
49:def group(*choices): return '(' + '|'.join(choices) + ')'
50:def any(*choices): return group(*choices) + '*'
51:def maybe(*choices): return group(*choices) + '?'
52:def _combinations(*l):
53:    return set(
54:        x + y for x in l for y in l + ("",) if x.casefold() != y.casefold()
55:    )
56:
57:Whitespace = r'[ \f\t]*'
58:Comment = r'#[^\r\n]*'
59:Ignore = Whitespace + any(r'\\\r?\n' + Whitespace) + maybe(Comment)
60:Name = r'\w+'  # this is invalid but it's fine because Name comes after Number in all groups
61:
62:Binnumber = r'0[bB]_?[01]+(?:_[01]+)*'
63:Hexnumber = r'0[xX]_?[\da-fA-F]+(?:_[\da-fA-F]+)*[lL]?'
64:Octnumber = r'0[oO]?_?[0-7]+(?:_[0-7]+)*[lL]?'
65:Decnumber = group(r'[1-9]\d*(?:_\d+)*[lL]?', '0[lL]?')
66:Intnumber = group(Binnumber, Hexnumber, Octnumber, Decnumber)
67:Exponent = r'[eE][-+]?\d+(?:_\d+)*'
68:Pointfloat = group(r'\d+(?:_\d+)*\.(?:\d+(?:_\d+)*)?', r'\.\d+(?:_\d+)*') + maybe(Exponent)
69:Expfloat = r'\d+(?:_\d+)*' + Exponent
70:Floatnumber = group(Pointfloat, Expfloat)
71:Imagnumber = group(r'\d+(?:_\d+)*[jJ]', Floatnumber + r'[jJ]')
72:Number = group(Imagnumber, Floatnumber, Intnumber)
73:
74:# Tail end of ' string.
75:Single = r"[^'\\]*(?:\\.[^'\\]*)*'"
76:# Tail end of " string.
77:Double = r'[^"\\]*(?:\\.[^"\\]*)*"'
78:# Tail end of ''' string.
79:Single3 = r"[^'\\]*(?:(?:\\.|'(?!''))[^'\\]*)*'''"
80:# Tail end of """ string.
81:Double3 = r'[^"\\]*(?:(?:\\.|"(?!""))[^"\\]*)*"""'
82:_litprefix = r"(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?"
83:Triple = group(_litprefix + "'''", _litprefix + '"""')
84:# Single-line ' or " string.
85:String = group(_litprefix + r"'[^\n'\\]*(?:\\.[^\n'\\]*)*'",
86:               _litprefix + r'"[^\n"\\]*(?:\\.[^\n"\\]*)*"')
87:
88:# Because of leftmost-then-longest match semantics, be sure to put the
89:# longest operators first (e.g., if = came before ==, == would get
90:# recognized as two instances of =).
91:Operator = group(r"\*\*=?", r">>=?", r"<<=?", r"<>", r"!=",
92:                 r"//=?", r"->",
93:                 r"[+\-*/%&@|^=<>]=?",
94:                 r"~")
95:
96:Bracket = '[][(){}]'
97:Special = group(r'\r?\n', r'[:;.,`@]')
98:Funny = group(Operator, Bracket, Special)
99:
100:PlainToken = group(Number, Funny, String, Name)
101:Token = Ignore + PlainToken
102:
103:# First (or only) line of ' or " string.
104:ContStr = group(_litprefix + r"'[^\n'\\]*(?:\\.[^\n'\\]*)*" +
105:                group("'", r'\\\r?\n'),
106:                _litprefix + r'"[^\n"\\]*(?:\\.[^\n"\\]*)*' +
107:                group('"', r'\\\r?\n'))
108:PseudoExtras = group(r'\\\r?\n', Comment, Triple)
109:PseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)
110:
111:tokenprog = re.compile(Token, re.UNICODE)
112:pseudoprog = re.compile(PseudoToken, re.UNICODE)
113:single3prog = re.compile(Single3)
114:double3prog = re.compile(Double3)
115:
116:_strprefixes = (
117:    _combinations('r', 'R', 'f', 'F') |
118:    _combinations('r', 'R', 'b', 'B') |
119:    {'u', 'U', 'ur', 'uR', 'Ur', 'UR'}
120:)
121:
122:endprogs = {"'": re.compile(Single), '"': re.compile(Double),
123:            "'''": single3prog, '"""': double3prog,
124:            **{f"{prefix}'''": single3prog for prefix in _strprefixes},
125:            **{f'{prefix}"""': double3prog for prefix in _strprefixes},
126:            **{prefix: None for prefix in _strprefixes}}
127:
128:triple_quoted = (
129:    {"'''", '"""'} |
130:    {f"{prefix}'''" for prefix in _strprefixes} |
131:    {f'{prefix}"""' for prefix in _strprefixes}
132:)
133:single_quoted = (
134:    {"'", '"'} |
135:    {f"{prefix}'" for prefix in _strprefixes} |
136:    {f'{prefix}"' for prefix in _strprefixes}
137:)
138:
139:tabsize = 8
140:
141:@dataclass(frozen=True)
142:class TokenizerConfig:
143:    async_is_reserved_keyword: bool = False
144:
145:class TokenError(Exception): pass
146:
147:class StopTokenizing(Exception): pass
148:
149:def printtoken(type, token, xxx_todo_changeme, xxx_todo_changeme1, line): # for testing
150:    (srow, scol) = xxx_todo_changeme
151:    (erow, ecol) = xxx_todo_changeme1
152:    print("%d,%d-%d,%d:\t%s\t%s" % \
153:        (srow, scol, erow, ecol, tok_name[type], repr(token)))
154:
155:def tokenize(readline, tokeneater=printtoken):
156:    """
157:    The tokenize() function accepts two parameters: one representing the
158:    input stream, and one providing an output mechanism for tokenize().
159:
160:    The first parameter, readline, must be a callable object which provides
161:    the same interface as the readline() method of built-in file objects.
162:    Each call to the function should return one line of input as a string.
163:
164:    The second parameter, tokeneater, must also be a callable object. It is
165:    called once for each token, with five arguments, corresponding to the
166:    tuples generated by generate_tokens().
167:    """
168:    try:
169:        tokenize_loop(readline, tokeneater)
170:    except StopTokenizing:
171:        pass
172:
173:# backwards compatible interface
174:def tokenize_loop(readline, tokeneater):
175:    for token_info in generate_tokens(readline):
176:        tokeneater(*token_info)
177:
178:class Untokenizer:
179:
180:    def __init__(self):
181:        self.tokens = []
182:        self.prev_row = 1
183:        self.prev_col = 0
184:
185:    def add_whitespace(self, start):
186:        row, col = start
187:        assert row <= self.prev_row
188:        col_offset = col - self.prev_col
189:        if col_offset:
190:            self.tokens.append(" " * col_offset)
191:
192:    def untokenize(self, iterable):
193:        for t in iterable:
194:            if len(t) == 2:
195:                self.compat(t, iterable)
196:                break
197:            tok_type, token, start, end, line = t
198:            self.add_whitespace(start)
199:            self.tokens.append(token)
200:            self.prev_row, self.prev_col = end
201:            if tok_type in (NEWLINE, NL):
202:                self.prev_row += 1
203:                self.prev_col = 0
204:        return "".join(self.tokens)
205:
206:    def compat(self, token, iterable):
207:        startline = False
208:        indents = []
209:        toks_append = self.tokens.append
210:        toknum, tokval = token
211:        if toknum in (NAME, NUMBER):
212:            tokval += ' '
213:        if toknum in (NEWLINE, NL):
214:            startline = True
215:        for tok in iterable:
216:            toknum, tokval = tok[:2]
217:
218:            if toknum in (NAME, NUMBER, ASYNC, AWAIT):
219:                tokval += ' '
220:

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.py
320:
321:def untokenize(iterable):
322:    """Transform tokens back into Python source code.
323:
324:    Each element returned by the iterable must be a token sequence
325:    with at least two elements, a token number and token value.  If
326:    only two tokens are passed, the resulting output is poor.
327:
328:    Round-trip invariant for full input:
329:        Untokenized source will match input source exactly
330:
331:    Round-trip invariant for limited intput:
332:        # Output text will tokenize the back to the input
333:        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
334:        newcode = untokenize(t1)
335:        readline = iter(newcode.splitlines(1)).next
336:        t2 = [tok[:2] for tokin generate_tokens(readline)]
337:        assert t1 == t2
338:    """
339:    ut = Untokenizer()
340:    return ut.untokenize(iterable)
341:
342:def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
343:    """
344:    The generate_tokens() generator requires one argument, readline, which
345:    must be a callable object which provides the same interface as the
346:    readline() method of built-in file objects. Each call to the function
347:    should return one line of input as a string.  Alternately, readline
348:    can be a callable function terminating with StopIteration:
349:        readline = open(myfile).next    # Example of alternate readline
350:
351:    The generator produces 5-tuples with these members: the token type; the
352:    token string; a 2-tuple (srow, scol) of ints specifying the row and
353:    column where the token begins in the source; a 2-tuple (erow, ecol) of
354:    ints specifying the row and column where the token ends in the source;
355:    and the line on which the token was found. The line passed is the
356:    logical line; continuation lines are included.
357:    """
358:    lnum = parenlev = continued = 0
359:    numchars = '0123456789'

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi
1:# Stubs for lib2to3.pgen2.tokenize (Python 3.6)
2:# NOTE: Only elements from __all__ are present.
3:
4:from typing import Callable, Iterable, Iterator, List, Text, Tuple
5:from attr import dataclass
6:from blib2to3.pgen2.token import *  # noqa
7:
8:
9:_Coord = Tuple[int, int]
10:_TokenEater = Callable[[int, Text, _Coord, _Coord, Text], None]
11:_TokenInfo = Tuple[int, Text, _Coord, _Coord, Text]
12:
13:@dataclass(frozen=True)
14:class TokenizerConfig:
15:    async_is_reserved_keyword: bool = False
16:
17:class TokenError(Exception): ...
18:class StopTokenizing(Exception): ...
19:
20:def tokenize(readline: Callable[[], Text], tokeneater: _TokenEater = ...) -> None: ...
21:
22:class Untokenizer:
23:    tokens: List[Text]
24:    prev_row: int
25:    prev_col: int
26:    def __init__(self) -> None: ...
27:    def add_whitespace(self, start: _Coord) -> None: ...
28:    def untokenize(self, iterable: Iterable[_TokenInfo]) -> Text: ...
29:    def compat(self, token: Tuple[int, Text], iterable: Iterable[_TokenInfo]) -> None: ...
30:
31:def untokenize(iterable: Iterable[_TokenInfo]) -> Text: ...
32:def generate_tokens(
33:    readline: Callable[[], Text]
34:) -> Iterator[_TokenInfo]: ...

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py
1:# Copyright 2004-2005 Elemental Security, Inc. All Rights Reserved.
2:# Licensed to PSF under a Contributor Agreement.
3:
4:# Modifications:
5:# Copyright 2006 Google, Inc. All Rights Reserved.
6:# Licensed to PSF under a Contributor Agreement.
7:
8:"""Parser driver.
9:
10:This provides a high-level interface to parse a file into a syntax tree.
11:
12:"""
13:
14:__author__ = "Guido van Rossum <guido@python.org>"
15:
16:__all__ = ["Driver", "load_grammar"]
17:
18:# Python imports
19:import codecs
20:import io
21:import os
22:import logging
23:import pkgutil
24:import sys
25:
26:# Pgen imports
27:from . import grammar, parse, token, tokenize, pgen
28:
29:
30:class Driver(object):
31:
32:    def __init__(
33:        self,
34:        grammar,
35:        convert=None,
36:        logger=None,
37:        tokenizer_config=tokenize.TokenizerConfig(),
38:    ):
39:        self.grammar = grammar
40:        if logger is None:
41:            logger = logging.getLogger(__name__)
42:        self.logger = logger
43:        self.convert = convert
44:        self.tokenizer_config = tokenizer_config
45:
46:    def parse_tokens(self, tokens, debug=False):
47:        """Parse a series of tokens and return the syntax tree."""
48:        # XXX Move the prefix computation into a wrapper around tokenize.
49:        p = parse.Parser(self.grammar, self.convert)
50:        p.setup()
51:        lineno = 1
52:        column = 0
53:        indent_columns = []
54:        type = value = start = end = line_text = None
55:        prefix = ""
56:        for quintuple in tokens:
57:            type, value, start, end, line_text = quintuple
58:            if start != (lineno, column):
59:                assert (lineno, column) <= start, ((lineno, column), start)
60:                s_lineno, s_column = start
61:                if lineno < s_lineno:
62:                    prefix += "\n" * (s_lineno - lineno)
63:                    lineno = s_lineno
64:                    column = 0
65:                if column < s_column:
66:                    prefix += line_text[column:s_column]
67:                    column = s_column
68:            if type in (tokenize.COMMENT, tokenize.NL):
69:                prefix += value
70:                lineno, column = end
71:                if value.endswith("\n"):
72:                    lineno += 1
73:                    column = 0
74:                continue
75:            if type == token.OP:
76:                type = grammar.opmap[value]
77:            if debug:
78:                self.logger.debug("%s %r (prefix=%r)",
79:                                  token.tok_name[type], value, prefix)
80:            if type == token.INDENT:
81:                indent_columns.append(len(value))
82:                _prefix = prefix + value
83:                prefix = ""
84:                value = ""
85:            elif type == token.DEDENT:
86:                _indent_col = indent_columns.pop()
87:                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)
88:            if p.addtoken(type, value, (prefix, start)):
89:                if debug:
90:                    self.logger.debug("Stop.")
91:                break
92:            prefix = ""
93:            if type in {token.INDENT, token.DEDENT}:
94:                prefix = _prefix
95:            lineno, column = end
96:            if value.endswith("\n"):
97:                lineno += 1
98:                column = 0
99:        else:
100:            # We never broke out -- EOF is too soon (how can this happen???)
101:            raise parse.ParseError("incomplete input",
102:                                   type, value, (prefix, start))
103:        return p.rootnode
104:
105:    def parse_stream_raw(self, stream, debug=False):
106:        """Parse a stream and return the syntax tree."""
107:        tokens = tokenize.generate_tokens(stream.readline, config=self.tokenizer_config)
108:        return self.parse_tokens(tokens, debug)
109:
110:    def parse_stream(self, stream, debug=False):
111:        """Parse a stream and return the syntax tree."""
112:        return self.parse_stream_raw(stream, debug)
113:
114:    def parse_file(self, filename, encoding=None, debug=False):
115:        """Parse a file and return the syntax tree."""
116:        with io.open(filename, "r", encoding=encoding) as stream:
117:            return self.parse_stream(stream, debug)
118:
119:    def parse_string(self, text, debug=False):
120:        """Parse a string and return the syntax tree."""
121:        tokens = tokenize.generate_tokens(
122:            io.StringIO(text).readline,
123:            config=self.tokenizer_config,
124:        )
125:        return self.parse_tokens(tokens, debug)
126:
127:    def _partially_consume_prefix(self, prefix, column):
128:        lines = []
129:        current_line = ""
130:        current_column = 0
131:        wait_for_nl = False
132:        for char in prefix:
133:            current_line += char
134:            if wait_for_nl:
135:                if char == '\n':
136:                    if current_line.strip() and current_column < column:
137:                        res = ''.join(lines)
138:                        return res, prefix[len(res):]
139:
140:                    lines.append(current_line)
141:                    current_line = ""
142:                    current_column = 0
143:                    wait_for_nl = False
144:            elif char in ' \t':
145:                current_column += 1
146:            elif char == '\n':
147:                # unexpected empty line
148:                current_column = 0
149:            else:
150:                # indent is finished
151:                wait_for_nl = True
152:        return ''.join(lines), current_line
153:
154:
155:def _generate_pickle_name(gt, cache_dir=None):
156:    head, tail = os.path.splitext(gt)
157:    if tail == ".txt":
158:        tail = ""
159:    name = head + tail + ".".join(map(str, sys.version_info)) + ".pickle"
160:    if cache_dir:

Calling tool: grepTool
get_parser_configs\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:826:    for parser_config in get_parser_configs(set(target_versions)):

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py
740:            current_line, line_length=mode.line_length, features=split_line_features
741:        ):
742:            dst_contents += str(line)
743:    return dst_contents
744:
745:
746:def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:
747:    """Return a tuple of (decoded_contents, encoding, newline).
748:
749:    `newline` is either CRLF or LF but `decoded_contents` is decoded with
750:    universal newlines (i.e. only contains LF).
751:    """
752:    srcbuf = io.BytesIO(src)
753:    encoding, lines = tokenize.detect_encoding(srcbuf.readline)
754:    if not lines:
755:        return "", encoding, "\n"
756:
757:    newline = "\r\n" if b"\r\n" == lines[0][-2:] else "\n"
758:    srcbuf.seek(0)
759:    with io.TextIOWrapper(srcbuf, encoding) as tiow:
760:        return tiow.read(), encoding, newline
761:
762:
763:@dataclass(frozen=True)
764:class ParserConfig:
765:    grammar: Grammar
766:    tokenizer_config: TokenizerConfig = TokenizerConfig()
767:
768:
769:def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:
770:    if not target_versions:
771:        # No target_version specified, so try all grammars.
772:        return [
773:            # Python 3.7+
774:            ParserConfig(
775:                pygram.python_grammar_no_print_statement_no_exec_statement,
776:                TokenizerConfig(async_is_reserved_keyword=True),
777:            ),
778:            # Python 3.0-3.6
779:            ParserConfig(
780:                pygram.python_grammar_no_print_statement_no_exec_statement,
781:                TokenizerConfig(async_is_reserved_keyword=False),
782:            ),
783:            # Python 2.7 with future print_function import
784:            ParserConfig(pygram.python_grammar_no_print_statement),
785:            # Python 2.7
786:            ParserConfig(pygram.python_grammar),
787:        ]
788:    elif all(version.is_python2() for version in target_versions):
789:        # Python 2-only code, so try Python 2 grammars.
790:        return [
791:            # Python 2.7 with future print_function import
792:            ParserConfig(pygram.python_grammar_no_print_statement),
793:            # Python 2.7
794:            ParserConfig(pygram.python_grammar),
795:        ]
796:    else:
797:        # Python 3-compatible code, so only try Python 3 grammar.
798:        configs = []
799:        # If we have to parse both, try to parse async as a keyword first
800:        if not supports_feature(target_versions, Feature.ASYNC_IS_VALID_IDENTIFIER):
801:            # Python 3.7+
802:            configs.append(
803:                ParserConfig(
804:                    pygram.python_grammar_no_print_statement_no_exec_statement,
805:                    TokenizerConfig(async_is_reserved_keyword=True),
806:                )
807:            )
808:        if not supports_feature(target_versions, Feature.ASYNC_IS_RESERVED_KEYWORD):
809:            # Python 3.0-3.6
810:            configs.append(
811:                ParserConfig(
812:                    pygram.python_grammar_no_print_statement_no_exec_statement,
813:                    TokenizerConfig(async_is_reserved_keyword=False),
814:                )
815:            )
816:        # At least one of the above branches must have been taken, because every Python
817:        # version has exactly one of the two 'ASYNC_IS_*' flags
818:        return configs
819:
820:
821:def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:
822:    """Given a string with source, return the lib2to3 Node."""
823:    if src_txt[-1:] != "\n":
824:        src_txt += "\n"
825:
826:    for parser_config in get_parser_configs(set(target_versions)):
827:        drv = driver.Driver(
828:            parser_config.grammar,
829:            pytree.convert,
830:            tokenizer_config=parser_config.tokenizer_config,
831:        )
832:        try:
833:            result = drv.parse_string(src_txt, True)
834:            break
835:
836:        except ParseError as pe:
837:            lineno, column = pe.context[1]
838:            lines = src_txt.splitlines()
839:            try:
840:                faulty_line = lines[lineno - 1]
841:            except IndexError:
842:                faulty_line = "<line number missing in source>"
843:            exc = InvalidInput(f"Cannot parse: {lineno}:{column}: {faulty_line}")
844:    else:
845:        raise exc from None
846:
847:    if isinstance(result, Leaf):
848:        result = Node(syms.file_input, [result])
849:    return result
850:
851:
852:def lib2to3_unparse(node: Node) -> str:
853:    """Given a lib2to3 node, return its string representation."""
854:    code = str(node)
855:    return code
856:
857:
858:T = TypeVar("T")
859:

Calling tool: grepTool
Driver\(
Matched content:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/black.py:827:        drv = driver.Driver(
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/driver.py:30:class Driver(object):

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as CORRECT
Reason: Searched for all generate_tokens call sites. Driver now passes tokenizer_config in parse_stream_raw and parse_string. Legacy calls in pgen.py and tokenize_loop still call generate_tokens(readline) and work due to the new default TokenizerConfig() parameter. No other call sites found.

Todo item 11 of 19 is correct
description: 'Check all call sites of `generate_tokens` across the repo: updated calls pass `config` where needed (driver), and legacy calls (e.g., in `pgen.py` and `tokenize_loop`) still compile using the default parameter.'

Validating todo item 12 of 19
description: 'Verify `blib2to3/pgen2/tokenize.pyi` stub reflects the implementation changes: it defines `TokenizerConfig`, but its `generate_tokens` currently lacks the optional `config` parameter; flag this mismatch and ensure the stub's signature is updated to accept `config: TokenizerConfig = ...` to match the implementation.'

Calling tool: globTool
**/blib2to3/pgen2/tokenize.pyi
Matched files:
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi

Calling tool: grepTool
TokenizerConfig|generate_tokens\(
Matched content:
14:class TokenizerConfig:
32:def generate_tokens(

Calling tool: readTool
/Users/marcus/repos/BugsInPy/framework/bin/temp/black-6/good/black/blib2to3/pgen2/tokenize.pyi
1:# Stubs for lib2to3.pgen2.tokenize (Python 3.6)
2:# NOTE: Only elements from __all__ are present.
3:
4:from typing import Callable, Iterable, Iterator, List, Text, Tuple
5:from attr import dataclass
6:from blib2to3.pgen2.token import *  # noqa
7:
8:
9:_Coord = Tuple[int, int]
10:_TokenEater = Callable[[int, Text, _Coord, _Coord, Text], None]
11:_TokenInfo = Tuple[int, Text, _Coord, _Coord, Text]
12:
13:@dataclass(frozen=True)
14:class TokenizerConfig:
15:    async_is_reserved_keyword: bool = False
16:
17:class TokenError(Exception): ...
18:class StopTokenizing(Exception): ...
19:
20:def tokenize(readline: Callable[[], Text], tokeneater: _TokenEater = ...) -> None: ...
21:
22:class Untokenizer:
23:    tokens: List[Text]
24:    prev_row: int
25:    prev_col: int
26:    def __init__(self) -> None: ...
27:    def add_whitespace(self, start: _Coord) -> None: ...
28:    def untokenize(self, iterable: Iterable[_TokenInfo]) -> Text: ...
29:    def compat(self, token: Tuple[int, Text], iterable: Iterable[_TokenInfo]) -> None: ...
30:
31:def untokenize(iterable: Iterable[_TokenInfo]) -> Text: ...
32:def generate_tokens(
33:    readline: Callable[[], Text]
34:) -> Iterator[_TokenInfo]: ...

Calling tool: updateTodo
Todo Item Completed
Todo item was marked as INCORRECT
Reason: In blib2to3/pgen2/tokenize.pyi, TokenizerConfig is defined, but generate_tokens only accepts (readline: Callable[[], Text]) and lacks the optional config parameter. The implementation defines generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()), so the stub signature must be updated to accept config: TokenizerConfig = ... to match.


=== PHASE 2 COMPLETE ===

