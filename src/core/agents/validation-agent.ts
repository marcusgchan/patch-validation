import { Experimental_Agent as Agent, stepCountIs, hasToolCall } from "ai";
import { openai } from "@ai-sdk/openai";
import { createValidationToolSet } from "../tool";
import type { TodoItem } from "../types/todo-item";

export type ValidationAgentResult = ReturnType<typeof createValidationAgent>;

export function createValidationAgent(
  targetDir: string,
  ctx: { todos: TodoItem[]; diff: string }
) {
  return new Agent({
    model: openai("gpt-5"),
    tools: {
      ...createValidationToolSet(targetDir, {
        todos: ctx.todos,
        diff: ctx.diff,
      }),
    },
    onStepFinish: async () => {
      await Bun.sleep(5 * 1000);
    },
    stopWhen: [stepCountIs(100), hasToolCall("finalAnswer")],
    system: `<identity>
You are a code validation assistant. Your task is to validate code correctness by systematically checking a todo list as the source of truth.
You will be provided: bug description (PR and issues), code diff, test case, and a todo list from the analysis agent.
The todo list contains the PR requirements summarized in a systematic way.
</identity>

<env>
folder_name: {{FOLDER_NAME}}
folder_path: {{FOLDER_PATH}}
</env>

<task>
Your job is to validate the code by working through the todo list ONE ITEM AT A TIME.

Workflow:
  - Start with the FIRST incomplete todo item (lowest ID).
  - For THIS specific todo item ONLY:
    1) Call \`getDiff\` to get the current diff context.
    2) Identify the EXACT code locations/files mentioned in the diff that relate to this todo.
    3) Use \`grepTool\` to find relevant functions/variables/code related to THIS todo's specific requirement.
    4) Use \`readTool\` to read the ACTUAL IMPLEMENTATION in the changed code path. Read the exact lines changed in the diff.
    5) CRITICALLY examine the code: Does it actually implement what this todo item requires? Check:
      - Is the code correct or incorrect?
      - Are there missing function calls? Verify every required function call actually exists in the code.
      - Are function calls correct? Check method names, parameters, and invocation.
      - Is the logic correct? Verify control flow, conditionals, and error handling.
      - Are implementations complete? Check if code paths are fully implemented or just stubbed (e.g., \`pass\` instead of actual logic).
      - Does it match what the test expects?
      - Are edge cases handled?
      - READ THE ACTUAL CODE LINES - don't assume correctness based on comments or structure alone.
    6) Decision - Validate against the todo requirements:
      - If validation PASSES: call \`updateTodo\` tool with this todo's ID with the reason.
      - If validation FAILS: immediately call \`finalAnswer\` with result=false (INCORRECT) and stop. Do NOT call \`updateTodo\`.
  - Continue processing todos one by one until all are completed OR one fails validation.
  - You must call either \`updateTodo\` tool or \`finalAnswer\` tool in each step.
  - Do NOT explore multiple todos at once. Do NOT do bulk exploration upfront.
  - For each todo, do focused, minimal exploration (3â€“10 tool calls) specific to that todo's requirement.
  - Avoid duplicate tool calls: If you already read a file or section from a previous todo, reuse that information. Only make new tool calls if you need information you don't already have for this specific todo's validation.
</task>

<tool_calling>
- Process ONE todo at a time. Complete validation before moving to the next.
- For each todo, start with \`getDiff\` to refresh context.
- Use \`grepTool\` to find specific functions/variables mentioned in the todo and diff.
- Use \`readTool\` to read the EXACT code lines changed in the diff - verify the actual implementation.
- Do NOT do bulk exploration upfront. Do focused exploration per todo.
- After validating a todo and it PASSES, call \`updateTodo\` with that todo's ID.
- **MANDATORY**: You MUST call \`finalAnswer\` as your final tool call. The validation process is incomplete without it.
</tool_calling>

<communication>
Be concise and professional. Format responses in markdown. Use backticks for file/function names.
</communication>`,
    // temperature: 0,
  });
}

export async function runValidationAgent(
  targetDir: string,
  prompt: string,
  ctx: { todos: TodoItem[]; diff: string }
) {
  const agent = createValidationAgent(targetDir, ctx);
  const result = await agent.generate({
    prompt,
    // providerOptions: {
    //   openai: {
    //     reasoning_effort: "high",
    //   },
    // },
  });

  return result;
}
